Attend and Diagnose: Clinical Time
Series Analysis Using Attention Models
Huan Song,† Deepta Rajan,‡∗Jayaraman J. Thiagarajan,†† Andreas Spanias†
† SenSIP Center, School of ECEE, Arizona State University, Tempe, AZ
‡ IBM Almaden Research Center, 650 Harry Road, San Jose, CA
†† Lawrence Livermore National Labs, 7000 East Avenue, Livermore, CA
Abstract
With widespread adoption of electronic health records, there
is an increased emphasis for predictive models that can effec-
tively deal with clinical time-series data. Powered by Recur-
rent Neural Network (RNN) architectures with Long Short-
Term Memory (LSTM) units, deep neural networks have
achieved state-of-the-art results in several clinical predic-
tion tasks. Despite the success of RNNs, its sequential na-
ture prohibits parallelized computing, thus making it inefﬁ-
cient particularly when processing long sequences. Recently,
architectures which are based solely on attention mecha-
nisms have shown remarkable success in transduction tasks
in NLP, while being computationally superior. In this pa-
per, for the ﬁrst time, we utilize attention models for clini-
cal time-series modeling, thereby dispensing recurrence en-
tirely. We develop the SAnD (Simply Attend and Diagnose)
architecture, which employs a masked, self-attention mech-
anism, and uses positional encoding and dense interpolation
strategies for incorporating temporal order. Furthermore, we
develop a multi-task variant of SAnD to jointly infer models
with multiple diagnosis tasks. Using the recent MIMIC-III
benchmark datasets, we demonstrate that the proposed ap-
proach achieves state-of-the-art performance in all tasks, out-
performing LSTM models and classical baselines with hand-
engineered features.
Introduction
Healthcare is one of the prominent applications of data min-
ing and machine learning, and it has witnessed tremendous
growth in research interest recently. This can be directly at-
tributed to both the abundance of digital clinical data, pri-
marily due to the widespread adoption of electronic health
records (EHR), and advances in data-driven inferencing
methodologies. Clinical data, for example intensive care unit
(ICU) measurements, is often comprised of multi-variate,
time-series observations corresponding to sensor measure-
ments, test results and subjective assessments. Potential in-
ferencing tasks using such data include classifying diag-
noses accurately, estimating length of stay, and predicting
future illness, or mortality.
The classical approach for healthcare data analysis has
been centered around extracting hand-engineered features
∗The ﬁrst two authors contributed equally.
Copyright c⃝2018, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
and building task-speciﬁc predictive models. Machine learn-
ing models are often challenged by factors such as need for
long-term dependencies, irregular sampling and missing val-
ues. In the recent years, recurrent Neural Networks (RNNs)
based on Long Short-Term Memory (LSTM) (Hochreiter
and Schmidhuber 1997) have become the de facto solution
to deal with clinical time-series data. RNNs are designed
to model varying-length data and have achieved state-of-
the-art results in sequence-to-sequence modeling (Sutskever,
Vinyals, and Le 2014), image captioning (Xu et al. 2015)
and recently in clinical diagnosis (Lipton et al. 2015). Fur-
thermore, LSTMs are effective in exploiting long-range de-
pendencies and handling nonlinear dynamics.
Attention in Clinical Data Analysis:
RNNs perform
computations at each position of the time-series by gener-
ating a sequence of hidden states as a function of the pre-
vious hidden state and the input for current position. This
inherent sequential nature makes parallelization challeng-
ing. Though efforts to improve the computational efﬁciency
of sequential modeling have recently surfaced, some of the
limitations still persist. The recent work of Vaswani et. al.
argues that attention mechanisms, without any recurrence,
can be effective in sequence-to-sequence modeling tasks.
Attention mechanisms are used to model dependencies in
sequences without regard for their actual distances in the se-
quence (Bahdanau, Cho, and Bengio 2014).
In this paper, we develop SAnD (Simply Attend and Di-
agnose), a new approach for clinical time-series analysis,
which is solely based on attention mechanisms. In contrast
to sequence-to-sequence modeling in NLP, we propose to
use self-attention that models dependencies within a single
sequence. In particular, we adopt the multi-head attention
mechanism similar to (Vaswani et al. 2017), with an addi-
tional masking to enable causality. In order to incorporate
temporal order into the representation learning, we propose
to utilize both positional encoding and a dense interpolation
embedding technique.
Evaluation on MIMIC-III Benchmark Dataset:
An-
other important factor that has challenged machine learn-
ing research towards clinical diagnosis is the lack of univer-
sally accepted benchmarks to rigorously evaluate the mod-
eling techniques. Consequently, in an effort to standardize
research in this ﬁeld, in (Harutyunyan et al. 2017), the au-
The Thirty-Second AAAI Conference
on Artificial Intelligence (AAAI-18)
4091
