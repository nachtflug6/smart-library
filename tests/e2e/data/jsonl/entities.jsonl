
{"id":"ZhangZeman2022", "type": "document""pdf_path":"data/pdf/curated/ZhangZeman2022.pdf","page_count":21}
{"id": "doc-001", "type": "document", "data": {"page_count": 3, "title": "Neural Networks and Deep Learning", "author": "Jane Smith"}}
{"id": "doc-001_p0001", "type": "page", "data": {"page_number": 1, "page_text": "Introduction to Neural Networks\n\nNeural networks are computational models inspired by biological neural systems. They consist of interconnected nodes organized in layers."}}
{"id": "doc-001_p0002", "type": "page", "data": {"page_number": 2, "page_text": "Deep Learning Architectures\n\nConvolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are two fundamental architectures. CNNs excel at image processing while RNNs handle sequential data."}}
{"id": "doc-001_p0003", "type": "page", "data": {"page_number": 3, "page_text": "Training and Optimization\n\nBackpropagation algorithm is used to train neural networks by computing gradients. Common optimizers include SGD, Adam, and RMSprop."}}
{"id": "doc-002", "type": "document", "data": {"page_count": 2, "title": "Attention Mechanisms in Transformers", "author": "Bob Johnson"}}
{"id": "doc-002_p0001", "type": "page", "data": {"page_number": 1, "page_text": "Attention Is All You Need\n\nThe transformer architecture revolutionized NLP by introducing self-attention mechanisms that allow models to weigh the importance of different input tokens."}}
{"id": "doc-002_p0002", "type": "page", "data": {"page_number": 2, "page_text": "Multi-Head Attention\n\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions."}}
{"id": "text-001", "type": "text", "data": {"content": "Gradient descent is an optimization algorithm used to minimize the loss function by iteratively moving in the direction of steepest descent."}}
{"id": "text-002", "type": "text", "data": {"content": "Overfitting occurs when a model learns the training data too well, including noise, and fails to generalize to new data."}}
{"id": "term-001", "type": "term", "data": {"term": "backpropagation", "definition": "An algorithm for training neural networks by computing the gradient of the loss function with respect to each weight by the chain rule.", "category": "optimization"}}
{"id": "term-002", "type": "term", "data": {"term": "attention mechanism", "definition": "A technique that allows neural networks to focus on specific parts of the input when producing an output.", "category": "architecture"}}
{"id": "term-003", "type": "term", "data": {"term": "embedding", "definition": "A learned representation that maps discrete objects (like words) to continuous vectors.", "category": "representation"}}