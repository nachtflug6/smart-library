tics. Ct-Echo employs the “Echo” mechanism, where his-
torical information resonates like an echo, inﬂuencing the
current state and diminishing gradually over time. This en-
sures effective capture of the dynamic characteristics, that
is, the temporal dependencies within each sequence. More-
over, Ct-Echo incorporates the Ordinary Differential Equa-
tion (ODE) for continuous-time updates of hidden states at
irregular time intervals, constructing a continuous iteration
process in the hidden layer and effectively modeling irregu-
lar sequences. Fitting with Ct-Echo is accomplished through
ridge regression, eliminating the need for ofﬂine iterative
training such as gradient descent. Consequently, Ct-Echo not
only effectively captures dynamic characteristics in irregular
sequences but also demonstrates high ﬁtting efﬁciency.
Fitting irregular sequences via Ct-Echo, each sequence
is represented with a ﬁtted model respectively1. The dis-
tance metric between models is further introduced, well re-
ﬂecting the difference of dynamic characteristics inherent
in the original sequences. Through the above, the original
sequences are mapped from the data space to the Ct-Echo
model space. The Ct-Echo effectively captures the dynamic
characteristics of irregular sequences into ﬁtted models,
leading to closer models for sequences within the same cate-
gory and more pronounced divergence for sequences across
different categories due to their unique dynamics. Classiﬁ-
cation algorithms are efﬁciently operated on the models, al-
lowing for the efﬁcient anomaly detection of the sequences
represented by these ﬁtted models. The main contributions
of this paper are presented as follows:
• We propose Ct-Echo, effective in capturing the dynamic
characteristics of irregular sequences. It uses the “Echo”
mechanism to capture temporal dependencies and ODE
for continuous-time transitions of hidden states. By rep-
resenting data with ﬁtted models, further processing is
conducted on the models instead of the original data.
• A notable advantage of the proposed method is its com-
putational efﬁciency. Fitting with Ct-Echo is accom-
plished through ridge regression, followed by the read-
ily available distance metric between ﬁtted models. This
efﬁciency is critical in scenarios requiring rapid data pro-
cessing and immediate analysis.
• Focusing and adequately capturing the data-inherent dy-
namic characteristics, our method reduces dependence
on the amount of labeled training data. This attribute is
beneﬁcial in situations where labeled data is limited or
where collecting large datasets is not feasible, broaden-
ing its applicability across various ﬁelds.
Related Work
Analysis of Irregular Sequences
Most studies addressing sequence analysis assume uniform
sampling, but in reality, data could be irregularly sampled
due to inconsistent data collection or missing values. This
1In this paper, “Ct-Echo” refers to the network used to ﬁt ir-
regular sequences, yielding the ﬁtted “Ct-Echo readout model” for
sequence representation, also short-termed as “‘Ct-Echo model”.
Input Layer
Hidden Layer
Output Layer
Reservoir
hx
W
hh
W
yh
W
x
y
Figure 2: Illustration of ESN structure, which consists of an
input layer, a hidden layer, and an output layer.
irregularity poses challenges for methods originally devel-
oped for regularly sampled data (Chowdhury et al. 2023).
Considerable studies have contributed to the analysis of
irregular sequences. Che et al. (2018) proposed GRU-D,
which incorporates missing patterns into Gated Recurrent
Unit (GRU). Besides, transformer-based methods, such as
Multi-Time Attention Networks (mTANs) (Shukla and Mar-
lin 2021) and Warpformer (Zhang et al. 2023), have been
proposed to tackle this. Recent advancements have seen
the integration of ODEs in modeling irregular sequences.
Rubanova et al. (2019) introduced ODE-RNN for capturing
continuous-time hidden dynamics. Furthermore, Kidger et
al. (2020) extended this concept by incorporating CDEs into
the Neural ODE (Chen et al. 2018) framework. Yuan et al.
(2023) further proposed the Ordinary Differential Equation
Recurrent State Space Model (ODE-RSSM). However, the
efﬁcacy of these learning methods diminishes when train-
ing data is scarce, primarily due to their extensive param-
eterization. Furthermore, the signiﬁcant computational re-
quirements associated with these methods result in extended
training times and increased computational resources.
A Brief Introduction to ESN
ESN (Jaeger 2001) (Figure 2) is a form of RNN optimized
for processing sequence, constructed by three parts: input
layer, hidden layer (reservoir), and output layer. The distinc-
tive feature of ESN lies in the reservoir, which is a large,
randomly generated, ﬁxed recurrent network. In MBMs, ﬁt-
ting data with ESN involves sequentially feeding the input
sequence into the reservoir, acquiring the corresponding hid-
den states, and ﬁnally solving only the output weight Wyh
(Chen et al. 2013), while the input weight Whx and the
reservoir weight Whh are randomly generated and ﬁxed un-
der Echo State Property (ESP) (Jaeger 2001). This stream-
lines the ﬁtting process and enhances the efﬁciency.
Model-based Methods
MBMs2 start by ﬁtting each sequence into a suitable
model that captures its dynamic characteristics (i.e. chang-
ing rules). These models then served as more stable and par-
simonious representations of the corresponding sequences.
Thus, learning methods are utilized on the ﬁtted models in-
stead of the original data. In (Chen et al. 2013), temporal
signals are segmented, ﬁtted by Cycle topology with Reg-
ular Jumps (CRJ) (Rodan and Tiˇno 2012), represented by
2Also denoted by “learning in the model space”
15732
