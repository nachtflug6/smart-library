GRU
-∆t
GRU
-Int
Warp
former
MBM
-∆t
MBM
-Int
Proposed
(Decay)
Proposed
(Leaky)
CWRU
76
132
55
2.4
1.1
0.2
13
SU
53
99
67
1.7
0.8
0.1
7.3
WHU
26
52
15
0.8
0.4
<0.1
2
Table 2: Average Training Time(s) of Better-performed
Methods on Multiple Datasets with 50% Missing Rate
Missing
Rate
GRU-
∆t
GRU-
Int
Warp
former
MBM-
Int
MBM-
∆t
Proposed
(Decay)
Proposed
(Leaky)
A
10%
85.03 90.56 92.83 95.89 94.53
98.39
97.67
20%
81.25 81.12 94.50 92.22 90.27
94.96
94.33
30%
77.94 73.11 85.67 85.17 85.40
91.33
92.33
40%
73.62 62.37 82.17 78.67 76.57
82.00
85.78
50%
70.69 33.63 81.94 61.22 66.91
75.22
83.06
B
10%
82.63 75.56 91.06 97.78 97.24
98.33
98.94
20%
79.75 67.75 90.56 93.22 94.50
96.61
96.83
30%
70.63 44.50 84.06 85.28 88.13
93.56
94.06
40%
56.06 49.18 75.28 80.56 78.12
85.17
88.39
50%
63.56 21.31 64.67 67.67 64.83
75.78
82.83
C
10%
80.06 92.13 93.06 98.83 98.92
99.17
99.78
20%
76.94 75.56 87.56 96.11 96.94
97.78
98.56
30%
77.69 70.00 70.78 92.33 92.04
96.00
96.33
40%
65.94 47.56 65.07 85.78 81.02
88.28
90.00
50%
58.85 27.94 67.11 68.89 67.23
80.72
87.67
D
10%
92.00 82.38 96.33 99.78 99.82
99.61
99.94
20%
86.50 83.50 94.44 99.11 99.23
99.28
99.78
30%
73.38 73.69 91.83 96.61 96.37
97.89
98.89
40%
75.00 56.44 84.00 93.22 90.23
93.94
95.83
50%
71.38 43.69 80.22 82.22 81.00
86.67
92.67
E
10%
75.11 85.84 81.97 94.82 94.68
96.23
97.73
20%
65.91 86.80 82.81 92.05 91.22
93.82
95.78
30%
64.92 65.17 69.69 85.32 82.72
90.67
92.83
40%
65.11 56.73 63.28 76.88 74.01
83.14
86.54
50%
55.63 34.94 61.60 65.13 61.61
70.09
78.27
Table 3: Accuracy(%) of Better-performed Methods with
Different Missing Rates on CWRU Datasets
average training times of these methods across different
datasets. GRU-∆t, GRU-Int, and Warpformer show longer
training times, mainly due to the high computing complex-
ity during iterative training, especially when handling long
sequences. Conversely, the proposed methods, especially
Proposed(Decay), achieve signiﬁcantly shorter times. This
stems from the efﬁcient ﬁtting process with Ct-Echo, ac-
complished only through ridge regression instead of itera-
tive training (like gradient descent), as well as the readily
available distance metric between models. The efﬁciency,
combined with the superior classiﬁcation accuracy given in
Table 1, underscores the effectiveness and practicality of the
proposed methods for real-world applications.
Discussion with Different Missing Rates
Given different
missing rates, we evaluated the above ﬁve better-performed
methods and the proposed ones. The experiments are carried
out on ﬁve sub-datasets of CWRU, where we systematically
introduced missing values at rates ranging from 10% to 50%,
with increments of 10%.
Table 3 and Figure 6 reveal the strengths of the pro-
posed methods, particularly Proposed(Leaky), maintaining
higher accuracy across increasing missing rates, even un-
10%
20%
40%
50%
Missing Rate
40%
60%
80%
100%
Accuracy
CWRU-C
10%
20%
40%
50%
Missing Rate
CWRU-E
GRU-
t
GRU-Int
Warpformer
MBM-Int
MBM-
t
Proposed(Decay)
Proposed(Leaky)
0%
0%
Figure 6: Comparison on better-performed methods on
CWRU-C and CWRU-E datasets with different missing
rates, from 10% to 50%. The proposed methods (bold font)
maintain higher accuracy across increasing missing rates.
SVM
KNN
Proposed
(Decay)
Proposed
(Leaky)
Proposed
(Decay)
Proposed
(Leaky)
G20
94.31 ± 0.96
91.25 ± 0.58
87.23 ± 1.81
86.17 ± 2.22
G30
98.15 ± 0.55
97.10 ± 0.58
96.92 ± 0.64
96.54 ± 1.03
B20
99.21 ± 0.28
99.50 ± 0.11
98.47 ± 0.42
98.90 ± 0.44
B30
99.69 ± 0.13
99.69 ± 0.09
99.08 ± 0.26
99.54 ± 0.16
Table 4: Accuracy(%) (mean±std) of Classiﬁcation in Ct-
Echo Model Space on Su Datasets with 50% Missing Rate
der challenging conditions with missing rates ≥30%. The
primary reason is the capture of continuous-time dynamic
characteristics via Ct-Echo as aforementioned, resulting in
a category-discriminative Ct-Echo model space (intuitively
exampled like Figure 5). In this model space, the ﬁtted mod-
els are positioned closer within the same category and more
distinct across different categories. Consequently, as Fig-
ure 6while the difference from some methods is minimal
at lower missing rates, the proposed methods demonstrate
a more pronounced advantage as the missing rate increases.
Classiﬁcation in Ct-Echo Model Space
The comparison
results between classiﬁers SVM and K-Nearest Neighbors
(KNN) (Altman 1992) in the Ct-Echo model space are given
in Table 4. From the results, distance-based classiﬁers like
SVM and KNN are both applicable in the Ct-Echo model
space. SVM exhibits higher and more stable accuracy, pri-
marily due to its ability to ﬁnd an optimal hyperplane that
maximizes the margin between categories. Thus SVM is
chosen as the classiﬁcation method in this paper.
Conclusion
This paper proposes learning in the “Ct-Echo Model Space”
for anomaly detection of irregular sequences. Ct-Echo ﬁts
sequences through continuous-time integration in the hid-
den layer, capturing their dynamic characteristics. Irregu-
lar sequences are represented by the ﬁtted Ct-Echo readout
models. Anomaly detection is then performed on these ﬁtted
models rather than the original data. Experimental results on
several datasets highlight the improved efﬁciency and effec-
tiveness of our method, especially with diverse missing rates
and limited training data. In future work, we plan to extend
its applicability to broader domains, focusing on reﬁning and
conﬁguring Ct-Echo for more complex data analysis.
15737
