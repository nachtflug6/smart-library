3.3. LSTM with imputation unit
For the LSTM-based prediction problem, if the input time series contains missing/null values, the model will fail due to null values
cannot be computed during the training process. If the missing values are set as some pre-defined values, like zeroes, mean of
historical observations, or last observed values, these biased model inputs will result in biased parameter estimation in the training
processing Che et al. (2018). Further, solving the imputation and prediction tasks at the same time often results in separated im-
putation and prediction models.
To fulfill data imputation and traffic prediction in one model, we propose an LSTM-based model with an imputation unit, called
LSTM-I. Unlike the GRU-D Che et al. (2018) targeting on inferring missing values based on the historical mean and the last ob-
servation with a learnable decay rate, the proposed LSTM-I aims to infer missing values at current time step from preceding LSTM cell
states and hidden states. The weight parameters in the imputation unit are learnable. Further, the values inferred from the imputation
values can contribute in the training process. In this way, the LSTM-I can complete the data imputation and prediction tasks at the
same time. Thus, it is particularly suitable for online traffic prediction problems, which may frequently encounter missing values
issues. Please note that the inferred values may not be the “actual“ missing values, since the proposed imputation unit is only
designed for generating appropriate values to help the calculation process in the LSTM structure work properly and generate accurate
predictions.
In LSTM-I, we design an imputation unit
p
_ , which is fed with the preceding cell state C t
_
1 and the preceding output h t
_
1,
to infer the values of the subsequent observation, as shown in Fig. 1(b). The inferred observation x t
_
D is denoted as
=
+
+
x t
g W I C t
U I h t
b I
_
_ (
_ · _
1
_ · _
1
_ )
(10)
where W I
_
and U I
_
are the weights and b I
_
is the bias in the imputation unit. Then, each missing element of the input vector is
updated by the inferred element
+
x t
m t x t
m t
x t
_
_
_
(1
_ ) _
d
d
d
d
d
(11)
where x t
_ d is the d-th element of x t
_ . According to Eq. (11), if x t
_ d is missing, m t
_ d is zero and x t
_ d is imputed by x t
_ d.
Besides, since each masking vector m t
_ contains the positions/indices of missing values at time step t, the masking vector is also
fed into the model and the LSTM-I structure can be characterized as
=
+
x t
m t
x t
m t
x t
_
_
_
(1
_ )
_
(12)
=
+
+
+
f t
g W f x t
U f h t
V f m t
b f
_
_ (
_ · _
_ · _
1
_ ·
_
_ )
(13)
=
+
+
+
i t
g W i x t
U i h t
V i m t
b i
_
_ (
_ · _
_ · _
1
_ ·
_
_ )
(14)
=
+
+
+
o t
g W o x t
U o h t
V o m t
b o
_
_ (
_ · _
_ · _
1
_ ·
_
_ )
(15)
=
+
+
+
C t
W C x t
U C h t
V C m t
b C
_
tanh(
_ · _
_ · _
1
_ ·
_
_ )
(16)
=
+
C t
f t
C t
i t
C t
_
_
_
1
_
_
(17)
=
h t
o t
C t
_
_
tanh( _ )
(18)
where V f V i V o
_ ,
_ ,
_ , and V C
_
are weight parameters for the masking vector m t
_ in different gates. If there are no missing values in
the input data, all elements in m t
_ are zeros and the structure of LSTM-I is identical to that of LSTM.
Furthermore, the imputation unit can contribute to the training process. According to Eq. (10), at each time step t
1, the
imputation unit infers the input x t
_ and generates x t
_ no matter x t
_ contains missing values or not. When x t
_ d is not missing, x t
_ d
can help LSTM-I evaluate the correctness of the inferred value x t
_ d by quantifying the difference between x t
_ d and x t
_ d. Thus, we
can add a regularization term to the model’s total loss (Eq. (9)) at each iteration as follows
=
+
+
=
h T
x T
t
m t
x t
x t
Loss( _
_
1)
_
1
_
_
0
_
_
T
d
d
d
(19)
where
is the penalty term and the regularization term
=
t
m t
x t
x t
_
1
_
_
0
_
_
T
d
d
d measures the total absolute imputation
error during a training iteration. By adding the regularization term to the loss, the imputation performance can be enhanced, and it
has the potential to improve the model’s overall prediction accuracy.
3.4. Bidirectional LSTMs
The idea of the BDLSTM comes from the bidirectional RNN Schuster and Paliwal (1997), which processes sequence data in both
forward and backward directions with two separate LSTM hidden layers. It has been proved that the bidirectional networks are
substantially better than unidirectional ones in many fields, like phoneme classification Graves and Schmidhuber (2005) and speech
recognition Graves et al. (2013). But, based on our review of the literature Ma et al. (2015), Duan et al. (2016), Chen et al. (2016), Wu
and Tan (2016), Yu et al. (2017b), BDLSTMs have not been utilized in traffic prediction problems. Due to the several aforementioned
reasons in the introduction section, BDLSTMs are fit for handling network-wide traffic prediction problems. Thus, we adopt the
BDLSTM as one component of our proposed framework.
Z. Cui, et al.
Transportation Research Part C 118 (2020) 102674
5
