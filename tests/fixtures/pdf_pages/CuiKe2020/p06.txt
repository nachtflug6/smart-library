To let BDLSTMs be able to deal with time series with missing data, we propose a BDLSTM with Imputation unit, called BDLSTM-I,
in which LSTM components are replaced with LSTM-I components, as shown in Fig. 2. The imputation mechanism of the LSTM-I is to
infer the current missing input from the preceding cell state and output. By adopting the BDLSTM-I, missing values can be imputed
from both the forward and the backward LSTM-Is. It is equivalent to infer the missing value at the current time step twice from both
the preceding and the subsequent time steps, respectively. Thus, the advantage of the BDLSTM-I is that missing values are imputed
based on both forward and backward temporal dependencies. Considering the periodic traffic patterns and the interaction between
downstream and upstream in traffic networks, BDLSTM-I has the potentioal to reduce bias in the imputation process and further
enhance traffic prediction.
The structure of an unfolded BDLSTM-I layer contains a forward LSTM-I layer and a backward LSTM-I layer, which is illustrated in
Fig. 2. The forward layer output, h
t
_ , is iteratively calculated based on positive ordered inputs
…
x
x
x T
[ _1,
_2,
, _ ] and masks
…
m
m
m T
[
_1,
_2,
,
_ ]. The backward layer output, h
t
_ , is iteratively calculated using the reversed ordered inputs and masks from time
step T to time step 1. Both forward and backward outputs are calculated based on the LSTM-I model equations (Eqs. (10)–(18)). The
BDLSTM-I layer generates output element y t
_
at each step t based on the combination of h
t
_
and h
t
_
by using the following
equation:
=
y t
h t h
t
_
_ ,
_
(20)
where
is an average function. It should be noted that other functions, such as summation, multiply, or concatenate functions, can
be used instead. Similar to the LSTM-I layer, the final output of a BDLSTM layer can be represented by a vector
=
…
Y
y
y
y T
[ _1,
_2,
, _ ].
If solely using one-layer BDLSTM-I for the prediction task, the loss function of BDLSTM-I should be defined based on that of LSTM-
I. However, due to BDLSTM-I has two LSTM-I arranged in two directions, the regularization term can be slightly adjusted as
=
+
+
=
+
y T
x T
t
m t
x t
x t
x t
x t
Loss
_
_
1
_
1
_
_
01
2
_
_
_
_
T
d
d
d
d
d
(21)
where x t
_ d and x t
_ d denote the inferred values from forward and backward LSTM-Is, respectively. In this way, the imputation
errors of the two LSTM-Is are averaged.
3.5. Stacked bidirectional and unidirectional LSTM network architecture
Existing studies Graves et al. (2013), LeCun et al. (2015) have shown that LSTM architectures with several hidden layers can
progressively build up a higher level of representations of sequence data, and thus, work more effectively. In a stacked multi-layer
LSTM architecture, the output of a hidden layer will be fed as the input into the subsequent hidden layer. This stacking layer
mechanism, which can enhance the power of neural networks, is adopted by our proposed architecture. In this study, we propose a
deep architecture named stacked bidirectional and unidirectional LSTM network architecture (SBULSTM) to predict the network-
wide traffic speed values. The proposed architecture does not have a fixed number of layers or use fixed types of RNNs. Instead, this
architecture, possibly containing multiple layers of LSTM or BDLSTM components, can be flexible for solving different tasks.
As mentioned in previous sections, BDLSTMs can make use of both forward and backward dependencies. When feeding the
Fig. 2. Structure of Bi-Directional LSTM-I.
Z. Cui, et al.
Transportation Research Part C 118 (2020) 102674
6
