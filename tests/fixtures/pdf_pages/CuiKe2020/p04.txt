next subsequent step of traffic state data, which can be described as:
…
…
=
+
F
x
x
x T
m
m
m T
x T
([ _1,
_2,
, _ ];[
_1,
_2,
,
_ ])
[ _
1]
(2)
3.2. Long short-term memory
It has been showed that LSTMs work well on sequence-based tasks with long-term dependencies Duan et al. (2016), Chen et al.
(2016), Zhao et al. (2017), Wu and Tan (2016), Song et al. (2016), Yu et al. (2017a). Although a variety of LSTM variants were
proposed in recent years, a comprehensive analysis of LSTM variants shows that none of the variants can improve upon the standard
LSTM architecture significantly Greff et al. (2017). Thus, we adopt the LSTM as the base model in this study.
The structure of LSTM is shown in Fig. 1(a). At time step t, the LSTM layer maintains a hidden memory cell C t
_ and three gate
units, which are input gate i t
_ , forget gate f t
_ , and output gate o t
_ . The LSTM cell takes the current variable vector x t
_ , the
preceding output h t
_
1, and the preceding cell state C t
_
1 as inputs. With the memory cell and gate units, LSTM can learn long-
term dependencies to allow useful information to pass along the LSTM network. Gate structures, especially the forget gate, help LSTM
to be an effective and scalable model for sequential data learning problems Greff et al. (2017). The input gate, forget gate, output
gate, and memory cell in an LSTM cell are represented by blue boxes in Fig. 1(a). They can be calculated using the following
equations:
=
+
+
f t
g W f x t
U f h t
b f
_
_ (
_ · _
_ · _
1
_ )
(3)
=
+
+
i t
g W i x t
U i h t
b i
_
_ (
_ · _
_ · _
1
_ )
(4)
=
+
+
o t
g W o x t
U o h t
b o
_
_ (
_ · _
_ · _
1
_ )
(5)
=
+
+
C t
W C x t
U C h t
b C
_
tanh(
_ · _
_ · _
1
_ )
(6)
where · is the matrix multiplication operator. W f W i W o
_ ,
_ ,
_ , and W C
_
are the weight matrices mapping the hidden layer input to
the three gate units and the memory cell. U f U i U o
_ ,
_ ,
_ , and U C
_
are the weight matrices connecting the preceding output to the
three gates and the memory cell. b f b i b o
_ ,
_ ,
_ , and b C
_
are four bias vectors.
g
_ (·) is the gate activation function, which is a sigmoid
function here, and tanh(·) is the hyperbolic tangent function. Then, the cell output state C t
_
and the layer output h t
_
can be
calculated as follows:
=
+
C t
f t
C t
i t
C t
_
_
_
1
_
_
(7)
=
h t
o t
C t
_
_
tanh( _ )
(8)
where
is the element-wise vector/matrix multiplication operator.
The output of an LSTM layer can be a set of outputs from all T steps, represented by
=
…
H T
h
h
h T
_
[ _1,
_2,
, _ ]. Here, when taking
the traffic prediction problem (Eq. (2)) as an example, only the last element of the output vector h T
_
is what we want to predict.
Hence, the predicted value for the subsequent time step
+
T
1 is
+
=
x T
h T
_
1
_ . In the training process, the model’s total loss
at
each iteration can be calculated by
=
+
+
=
+
x T
x T
h T
x T
Loss( _
1
_
1)
Loss( _
_
1)
(9)
where Loss(·) is the loss function, which is normally a mean square error function for traffic prediction problems.
Fig. 1. (a) Structure of the vanilla LSTM. (b) Structure of the LSTM-I. The mask gate determines the positions of the missing values. The missing
input values can be imputed via the imputation unit and the inferred/imputed values can assist the training process by adding a regularization term
to the loss function.
Z. Cui, et al.
Transportation Research Part C 118 (2020) 102674
4
