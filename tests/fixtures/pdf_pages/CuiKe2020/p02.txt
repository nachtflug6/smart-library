contributions to address the traffic prediction problem. With the ability to deal with high dimensional data and the capability of
capturing non-linear relationship, CI approaches, especially novel machine learning methods, tend to outperform the statistical
methods with respect to handling complex traffic forecasting problems Karlaftis and Vlahogianni (2011). The representative machine
learning methods include support vector regression Asif et al. (2013), K-nearest neighbor Cai et al. (2016), etc. Besides, nonpara-
metric approaches, such Kalman filter and its variants Chien et al. (2003), Van Lint (2008), and matrix/tensor factorization methods
Tan et al. (2016) are also widely used in traffic prediction problems.
Deep learning models, as a branch of machine learning models, become popular and rapidly be adopted in the traffic forecasting
area. Most of the newly proposed traffic forecasting models Ma et al. (2015), Duan et al. (2016), Chen et al. (2016), Zhao et al.
(2017), Wu and Tan (2016), Song et al. (2016), Yu et al. (2017a) are based on recurrent neural networks (RNNs), which mainly
process sequence data by maintaining a chain-like structure and internal memory with loops Jozefowicz et al. (2015). To address
RNN’s exploding gradient problem, Long Short-Term Memory network (LSTM) Hochreiter and Schmidhuber (1997) and the Gated
Recurrent Unit network (GRU) Cho et al. (2014) were designed to learn long-term dependencies of sequence data via gate and
memory units. Many recent studies Duan et al. (2016), Chen et al. (2016), Zhao et al. (2017) adopted the LSTM as a baseline or
building blocks in their proposed models for traffic forecasting. Although RNN and its variants have been adopted as building blocks
of traffic prediction models, few studies reformulated their model structure to improve traffic prediction accuracy and robustness. In
this study, we focus on RNN-based models and attempt to design a better structure to solve the traffic prediction problem. Three
primary limitations of existing RNN-based models for traffic forecasting can be summarized as follows: (1) Few existing models are
capable of dealing with missing data. (2) Although time series of traffic states are normally processed in a chronological order to
capture the forward dependencies, backward dependencies in traffic state sequences, which can be learned in a reverse-chronological
order, has not been explored. (3) Few studies evaluate the trade-off between model capacity and complexity.
Firstly, missing data is a common problem in the traffic data collection process due to sensor or communication failure. Various
data imputation methods for time series have been developed and applied to estimate missing data. However, solving the imputation
and prediction tasks at the same time often leads to a two-step process where imputation and prediction models are separated Che
et al. (2018). In this way, the missing patterns of the data cannot be effectively explored in prediction models, and thus may result in
biased prediction results Wells et al. (2013). In some real-time traffic forecasting scenarios, the assumption of data imputation
methods may not be satisfied, and thus, missing data cannot be imputed in real-time. Further, it is usually computationally expensive
for training and applying these imputation methods. Due to RNNs for times series with missing values have been explored and applied
Che et al. (2018), Lipton et al. (2016), RNN-based models have the potential to combine imputation methods with prediction models.
Considering the ability of LSTM to capture and maintain long-term dependencies, LSTM is even more suitable for time series im-
putation. From another perspective, the ability to impute missing values in time series can be regarded as a capability of processing
unevenly spaced time series, which is unachievable for most of the LSTM-based traffic prediction models Ma et al. (2015), Duan et al.
(2016), Chen et al. (2016), Song et al. (2016), Yu et al. (2017a). Hence, exploiting the power of customized LSTM to predict traffic
states with missing values, as one of our main motivations, is promising and attainable. In this study, we propose a customized LSTM
structure with an imputation unit (LSTM-I) to fulfill this goal.
The second improvable aspect of previous work is the learning order of the traffic state time series in RNN-based models.
Normally, the dataset fed to an LSTM model is chronologically arranged and the model’s chain-like structure makes use of the
forward dependencies. But in this process, it is possible that useful information does not efficiently pass through the chain-like gated
structure. Therefore, it may be informative to consider backward dependencies into consideration by processing series data in a
negative direction. Another reason for including backward dependencies in our study is the periodicity of the traffic states. Traffic
conditions have strong periodicity and regularity, and even short-term periodicity can be observed Jiang and Adeli (2004). According
to Box et al. (2015), analyzing the periodicity of time series data from both forward and backward temporal perspectives will enhance
the predictive performance. Besides, the impact of upstream and downstream traffic states with respect to a road segment in the
traffic network should not be neglected. Previous studies Chandra and Al-Deek (2009), Kamarianakis et al. (2010) found that past
speed values of upstream and downstream have an influence on the future speed values of a location along a corridor. For com-
plicated traffic networks with intersections and loops, upstream and downstream both refer to relative positions and two arbitrary
locations can be upstream and downstream of each other. Upstream and downstream are defined with respect to space, while forward
and backward dependencies are defined with respect to time. With the help of the forward and backward dependencies of spa-
tial–temporal data, the learned features will be more comprehensive. Based on our review of the literature, few studies on traffic
analysis utilized the backward dependency. To fill this gap, a bidirectional LSTM (BDLSTM) with the ability to deal with both forward
and backward dependencies is adopted as a component of the proposed network framework in this study.
The third limitation of previous work is the lack of trade-off evaluation between model capacity and complexity. Some newly
proposed LSTM-based prediction models, such as Ma et al. (2015), have only one LSTM layer to deal with time series. Existing studies
LeCun et al. (2015) have shown that deep LSTM architectures with several hidden layers can build up progressively higher levels of
representations of sequence data. Although some studies Chen et al. (2016), Wu and Tan (2016), Yu et al. (2017b) utilized more than
one LSTM layers, the influence of the number of LSTM layers needs to be further evaluated. Furthermore, the impact of the number of
other layers, the size of model weights, and the spatial dimension size of the network-wide traffic data should also be evaluated as
influential factors of prediction performance.
In this paper, we focus on RNN-based models and attempt to reformulate the way to incorporate RNNs into traffic prediction
models, even when the input traffic data contains missing values. We propose a stacked bidirectional and unidirectional LSTM
network architecture (SBU-LSTM) for network-wide traffic state prediction to address the aforementioned shortcomings. The eva-
luation of the prediction capability of stacked LSTM- or BDLSTM-based models has the potential to facilitate the further research on
Z. Cui, et al.
Transportation Research Part C 118 (2020) 102674
2
