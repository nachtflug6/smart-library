Published as a conference paper at ICLR 2022
4
EXPERIMENTS
Datasets. Below we brieﬂy overview healthcare and human activity datasets. (1) P19 (Reyna et al.,
2020) includes 38,803 patients that are monitored by 34 sensors. Each patient is associated with a
binary label representing the occurrence of sepsis. (2) P12 (Goldberger et al., 2000) records temporal
measurements of 36 sensors of 11,988 patients in the ﬁrst 48-hour stay in ICU. The samples are
labeled based on hospitalization length. (3) PAM (Reiss & Stricker, 2012) contains 5,333 segments
from 8 activities of daily living that are measured by 17 sensors. Details are in Appendix A.5.
Baselines. We compare RAINDROP with ﬁve state-of-the-art baselines: Transformer (Vaswani et al.,
2017), Trans-mean, GRU-D (Che et al., 2018), SeFT (Horn et al., 2020), and mTAND (Shukla &
Marlin, 2021). The Trans-mean is an imputation method combining transformer architecture with
commonly used average interpolation (i.e., missing values are replaced by average observations in
each sensor). The mTAND (Shukla & Marlin, 2021) method has been shown to outperform numerous
recurrent models including RNN-Impute (Che et al., 2018), RNN-Simple, and Phased-LSTM (Neil
et al., 2016), along with ordinary differential equations (ODE)-based models such as LATENT-ODE
and ODE-RNN (Chen et al., 2018). For this reason, we compare with mTAND and do not report
comparison with those techniques in this paper. Even though, to better show the superiority of
RAINDROP, we provide extensive comparison with popular approaches, such as DGM2-O (Wu et al.,
2021) and MTGNN (Wu et al., 2020c), that are designed for forecasting tasks. Further details are in
Table 1 and Appendix A.11. Details on hyperparameter selection and baselines are in Appendix A.6,
and evaluation metrics are presented in Appendix A.7.
4.1
RESULTS ACROSS DIVERSE EVALUATION SETTINGS
Setting 1: Classic time series classiﬁcation. Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup. We randomly split the dataset into training
(80%), validation (10%), and test (10%) set. The indices of these splits are ﬁxed across all methods.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results. As shown in Table 1, RAINDROP obtains the best performance across three benchmark
datasets, suggesting its strong performance for time series classiﬁcation. In particular, in binary
classiﬁcation (P19 and P12), RAINDROP outperforms the strongest baselines by 5.3% in AUROC
and 4.8% in AUPRC on average. In a more challenging 8-way classiﬁcation on the PAM dataset,
RAINDROP outperforms existing approaches by 5.7% in accuracy and 5.5% in F1 score. Further
exploratory analyses and benchmarking results are shown in Appendix A.9-A.10.
Table 1: Method benchmarking on irregularly sampled time series classiﬁcation (Setting 1).
P19
P12
PAM
Methods
AUROC
AUPRC
AUROC
AUPRC
Accuracy
Precision
Recall
F1 score
Transformer
83.2 ± 1.3
47.6 ± 3.8
65.1 ± 5.6
95.7 ± 1.6
83.5 ± 1.5
84.8 ± 1.5
86.0 ± 1.2
85.0 ± 1.3
Trans-mean
84.1 ± 1.7
47.4 ± 1.4
66.8 ± 4.2
95.9 ± 1.1
83.7 ± 2.3
84.9 ± 2.6
86.4 ± 2.1
85.1 ± 2.4
GRU-D
83.9 ±1.7
46.9 ± 2.1
67.2 ± 3.6
95.9 ± 2.1
83.3 ± 1.6
84.6 ± 1.2
85.2 ± 1.6
84.8 ± 1.2
SeFT
78.7 ± 2.4
31.1 ± 2.8
66.8 ± 0.8
96.2 ± 0.2
67.1 ± 2.2
70.0 ± 2.4
68.2 ± 1.5
68.5 ± 1.8
mTAND
80.4 ± 1.3
32.4 ± 1.8
65.3 ± 1.7
96.5 ± 1.2
74.6 ± 4.3
74.3 ± 4.0
79.5 ± 2.8
76.8 ± 3.4
IP-Net
84.6 ± 1.3
38.1 ± 3.7
72.5 ± 2.4
96.7 ± 0.3
74.3 ± 3.8
75.6 ± 2.1
77.9 ± 2.2
76.6 ± 2.8
DGM2-O
86.7 ± 3.4
44.7 ± 11.7
71.2 ± 2.5
96.9 ± 0.4
82.4 ± 2.3
85.2 ± 1.2
83.9 ± 2.3
84.3 ± 1.8
MTGNN
81.9 ± 6.2
39.9 ± 8.9
67.5 ± 3.1
96.4 ± 0.7
83.4 ± 1.9
85.2 ± 1.7
86.1 ± 1.9
85.9 ± 2.4
RAINDROP
87.0 ± 2.3
51.8 ± 5.5
72.1 ± 1.3
97.0 ± 0.4
88.5 ± 1.5
89.9 ± 1.5
89.9 ± 0.6
89.8 ± 1.0
Setting 2: Leave-ﬁxed-sensors-out. Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup.
Setup. RAINDROP can compensate for missing sensor obser-
vations by exploiting dependencies between sensors. To this end, we test whether RAINDROP can
achieve good performance when a subset of sensors are completely missing. This setting is practically
relevant in situations when, for example, sensors fail or are unavailable. We select a fraction of
sensors and hide all their observations in both validation and test sets (training samples are not
changed). In particular, we leave out the most informative sensors as deﬁned by information gain
analysis (Appendix A.8). The left-out sensors are ﬁxed across samples and models. Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results.
Results. We
report results taking PAM as an example. In Table 2 (left block), we observe that RAINDROP achieves
top performance in 18 out of 20 settings when the number of left-out sensors goes from 10% to 50%.
With the increased amount of missing data, RAINDROP yield greater performance improvements.
RAINDROP outperforms baselines by up to 24.9% in accuracy, 50.3% in precision, 29.3% in recall,
and 42.8% in F1 score.
8
