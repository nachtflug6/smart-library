Published as a conference paper at ICLR 2022
as in Che et al. (2018). Taking the architecture of RAINDROP, we concatenate the observation xt
i,u
with a binary mask indicator bt
i,u as input. The indicator bt
i,u is set as 1 when there is an observation
of sensor i at time t and set as 0 otherwise. All the experimental settings and hyperparameters are
the same as in RAINDROP (P19; Setting 1). The experimental results show that taking advantage of
missing pattern can slightly boost the AUROC by 1.2% and AUPRC by 0.9% in P19. This empirically
shed the light for future research on integrating multiple characteristics in representation of irregularly
time series.
A.10
COMPARISON BETWEEN TEMPORAL ATTENTION AND LSTM
We conduct extensive experiments to compare the effectiveness of temporal attention and LSTM. To
this end, we replace the temporal attention in sensor embedding generation (Eq 4-5) in RAINDROP by
LSTM layer which processes all observation embeddings sequentially. We use zero padding to convert
the irregular observations into ﬁxed-length time series so the data can be fed into LSTM architecture.
We regard the last output of LSTM as generated sensor embedding. The number of LSTM cells
equal to the dimension of observation embedding. All the model structures are identical except in
the part of temporal attention and LSTM. We keep all experimental settings (P19; Setting 1) and
hyperparameter selections the same. The experimental results show that the temporal self-attention
outperform LSTM by 1.8% (AUROC) and additionally saved 49% of the training time. One potential
reason is that the self-attention mechanism avoids recursion and allows parallel computation and also
reduces performance degradation caused by long-term dependencies (Ganesh et al., 2021; Vaswani
et al., 2017).
A.11
ADDITIONAL INFORMATION ON METHOD BENCHMARKING
Taking experimental Setting 1 (i.e., classic time series classiﬁcation) as an example, we conduct
extensive experiments to compare Raindrop with ODE-RNN (Chen et al., 2020), DGM2-O (Wu et al.,
2021), EvoNet (Hu et al., 2021), and MTGNN (Wu et al., 2020c). As IP-Net (Shukla & Marlin, 2018)
and mTAND (Shukla & Marlin, 2021) are from the same authors, we only compare with mTAND
which is the latest model. For the baselines, we follow the settings as provided in their public codes.
For methods, which cannot deal with irregular data (e.g., EvoNet and MTGNN), we ﬁrst impute the
missing data using mean imputation and then feed data into the model. For forecasting models (e.g.,
MTGNN) which are strictly not comparable with the proposed classiﬁcation model, we formulate the
task as a single-step forecasting, concatenate the learned representations from all sensors and feed
into a fully-connected layer (work as classiﬁer) to make prediction, and use cross-entropy to quantify
the loss.
A.12
RESULTS FOR P19 (SETTINGS 2-3)
Here we report the experimental results for P19 in Setting 2 (Table 4) and Setting 3 (Table 5).
Table 4: Classiﬁcation on samples with ﬁxed missing sensors (P19; Setting 2)
Models
Missing ratio
0%
10%
20%
30%
40%
50%
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
Transformer
83.2 ± 1.3
47.6 ± 3.8
77.4 ± 3.5
38.2 ± 4.2
75.7 ± 3.4
35.2 ± 5.4
75.1 ± 3.5
35.5 ± 4.4
75.3 ± 3.5
36.2 ± 4.2
74.9 ± 3.1
35.5 ± 5.0
Trans-mean
84.1 ± 1.7
47.4 ± 1.4
79.2 ± 2.7
40.6 ± 5.7
79.8 ± 2.5
38.3 ± 2.8
76.9 ± 2.4
37.5 ± 5.9
76.4 ± 2.0
36.3 ± 5.8
74.1 ± 2.3
41.3 ± 4.7
GRU-D
83.9 ± 1.7
46.9 ± 2.1
79.6 ± 2.2
37.4 ± 2.5
77.5 ± 3.1
36.5 ± 4.6
76.6 ± 2.9
35.1 ± 2.4
74.6 ± 2.7
35.9± 2.7
74.1 ± 2.9
33.2 ± 3.8
SeFT
78.7 ± 2.4
31.1 ± 2.8
77.3 ± 2.4
25.5 ± 2.3
63.5 ± 2.0
14.0 ± 1.1
62.3 ± 2.1
12.9 ± 1.2
57.8 ± 1.7
9.8 ± 1.1
56.0 ± 3.1
7.8 ± 1.3
mTAND
80.4 ± 1.3
32.4 ± 1.8
79.7 ± 2.2
29.0 ± 4.3
77.8 ± 1.9
25.3 ± 2.4
77.7 ± 1.9
27.8 ± 2.6
79.4 ± 2.0
32.1 ± 2.1
77.3 ± 2.1
27.0 ± 2.5
RAINDROP
87.0 ± 2.3
51.8 ± 5.5
84.3 ± 2.5
46.1 ± 3.5
81.9 ± 2.1
45.2 ± 6.4
81.4 ± 2.1
43.7 ± 7.2
81.8 ± 2.2
44.9 ± 6.6
79.7 ± 1.9
43.8 ± 5.6
Table 5: Classiﬁcation on samples with random missing sensors (P19; Setting 3)
Models
Missing ratio
0%
10%
20%
30%
40%
50%
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
AUROC
AUPRC
Transformer
83.2 ± 1.3
47.6 ± 3.8
82.2 ± 2.7
46.8 ± 3.5
81.6 ± 3.5
42.5 ± 8.5
81.3 ± 3.1
42.1 ± 4.5
80.2 ± 2.9
41.9 ± 6.8
79.2 ± 1.9
43.7 ± 3.7
Trans-mean
84.1 ± 1.7
47.4 ± 1.4
82.5 ± 3.7
44.7 ± 6.8
81.7 ± 2.0
45.9 ± 3.6
81.2 ± 2.2
43.2 ± 6.3
80.2 ± 1.7
41.5 ± 4.8
79.8 ± 3.1
39.3 ± 5.1
GRU-D
83.9 ± 1.7
46.9 ± 2.1
81.2 ± 3.4
46.4 ± 2.7
78.6 ± 4.1
43.3 ± 2.4
76.3 ± 2.5
28.5 ± 2.1
74.2 ± 2.7
29.6 ± 3.1
74.6 ± 3.5
26.5 ± 4.2
SeFT
78.7 ± 2.4
31.1 ± 2.8
76.8 ± 2.2
28.3 ± 2.5
77.0 ± 2.2
24.1 ± 2.4
75.2 ± 2.2
22.5 ± 3.0
73.6 ± 2.7
18.3 ± 3.2
72.6 ± 2.5
15.7 ± 1.9
mTAND
80.4 ± 1.3
32.4 ± 1.8
75.2 ± 2.5
24.5 ± 2.4
74.4 ± 3.5
24.6 ± 3.5
74.2 ± 3.2
22.6 ± 2.3
74.1 ± 2.6
23.1 ± 3.6
73.9 ± 3.7
24.6 ± 3.7
RAINDROP
87.0 ± 2.3
51.8 ± 5.5
85.5 ± 2.1
50.2 ± 5.5
83.5 ± 3.2
47.4 ± 7.0
83.1 ± 1.5
48.2 ± 4.7
82.6 ± 1.7
48.0 ± 5.5
80.9 ± 2.4
45.2 ± 6.9
18
