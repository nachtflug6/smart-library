Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction
KDD â€™22, August 14â€“18, 2022, Washington DC, USA
3
PRELIMINARIES
We focus on solving the non-stationary time encoding problem
in the attention manner. In this section, we first present the main
notations used in this problem. Then, the basic attention module,
positional and temporal encoding, and kernel functions will be
introduced briefly.
3.1
Notations
Let S = {(ğ‘¡1,ğ‘’1), ..., (ğ‘¡ğ‘›,ğ‘’ğ‘›)} denotes the sequence that contains a
series of event ğ‘’ğ‘–happened at time ğ‘¡ğ‘–, and ğœ™ğ‘¡(ğ‘¡),ğœ™ğ‘’(ğ‘’) represents
learnable mapping functions that encode ğ‘¡,ğ‘’into Rğ‘‘. Accordingly,
we use matrices ğ‘‰ğ‘’,ğ‘‰ğ‘¡âˆˆRğ‘›Ã—ğ‘‘to denote the encoded events and
temporal embeddings of the sequence S.
In practice, the temporal embedding ğ‘‰ğ‘¡is usually added or con-
catenated with the event embedding ğ‘‰ğ‘’. Without loss of generality,
we use ğ‘‰to denote the combination of ğ‘‰ğ‘¡and ğ‘‰ğ‘’. On the other
hand, regarding the input to attention module, we use ğ¾, ğ‘„, ğ‘‰to
denote the key, query, and value matrices, respectively. Note that,
in our self-attention setting, the key and query matrices equal to
the value matrix, i.e., ğ¾= ğ‘„= ğ‘‰.
3.2
Attention Module
The attention mechanism in the original Transformer [27] is formed
in the scaled dot-product manner:
ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾,ğ‘‰) = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥( (ğ‘„ğ‘Šğ‘„)(ğ¾ğ‘Šğ¾)ğ‘‡
âˆšï¸
ğ‘‘ğ‘˜
)(ğ‘‰ğ‘Šğ‘‰),
(1)
where ğ‘‘ğ‘˜is the dimension of the hidden representation of the key.
ğ‘Šğ¾,ğ‘Šğ‘„,ğ‘Šğ‘‰are the projection matrices for any specific head of Q,
K, V, respectively. To simplify notations in ğ‘˜-head attention module
that has ğ‘˜sets of projection matrices, in the following sections, we
omit them in equations.
To be more specific, let qi, ki, vi represent the ğ‘–âˆ’th component
of Q, K, V, which are vectors representing the input embedding at
ğ‘–âˆ’th position of the query, key, value sequences. z = (z1, ..., zi, ...zn)
denotes the output of attention module. Then, the attention formula
can be rewritten as:
zi =
ğ‘›
âˆ‘ï¸
ğ‘—=1
ğ‘˜ğ‘’ğ‘¥ğ‘(qi, kj)
Ãğ‘˜ğ‘’ğ‘¥ğ‘(qm, kl) vj,
ğ‘˜ğ‘’ğ‘¥ğ‘(qi, kj) = ğ‘’ğ‘¥ğ‘(
qikT
j
âˆšï¸
ğ‘‘ğ‘˜
).
(2)
As we can see from Eq.2, the relative contribution of each key-
value pair to each query is determined by the attention weight
ğ‘˜ğ‘’ğ‘¥ğ‘(qi,kj)
Ãğ‘˜ğ‘’ğ‘¥ğ‘(qm,kl) times the value vector vj.
3.3
Positional and Temporal Embedding
To count for the order information of a sequence, [27] added a
non-learnable positional embedding, i.e., ğ‘‰ğ‘¡, as the input into the
attention module. Each entry in ğ‘‰ğ‘¡is calculated by the following
sin and cos functions, respectively:
ğ‘ƒğ¸(ğ‘ğ‘œğ‘ , 2ğ‘–) = ğ‘ ğ‘–ğ‘›(
ğ‘ğ‘œğ‘ 
100002ğ‘–/ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™),
ğ‘ƒğ¸(ğ‘ğ‘œğ‘ , 2ğ‘–+ 1) = ğ‘ğ‘œğ‘ (
ğ‘ğ‘œğ‘ 
100002ğ‘–/ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™),
(3)
where ğ‘ğ‘œğ‘ is the position of the token/event in the sequence, ğ‘–
corresponds to 2 consecutive position embedding dimensions, and
ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™is the dimension of the token/event embedding. In practice,
the dimension of the positional encoding is usually the same as
ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™that the positional embedding and event embedding can be
summed as the attention inputs.
Recently, in [30], the authors further proposed a functional time
encoding method which maps the continuous-time values ğ‘¡to high
dimensional space by Mercerâ€™s feature map:
ğ‘¡â†¦â†’Î¦ = [Î¦ğœ”1 (ğ‘¡), ..., Î¦ğœ”ğ‘‘(ğ‘¡)]ğ‘‡,
(4)
where
Î¦ğœ”ğ‘–(ğ‘¡) =
ğ‘
âˆ‘ï¸
ğ‘—=1
âˆšï¸ƒ
ğ‘2ğ‘—(ğœ”ğ‘–)ğ‘ğ‘œğ‘ ğ‘—ğœ‹ğ‘¡
ğœ”ğ‘–
+
âˆšï¸ƒ
ğ‘2ğ‘—+1(ğœ”ğ‘–)ğ‘ ğ‘–ğ‘›ğ‘—ğœ‹ğ‘¡
ğœ”ğ‘–
.
(5)
[
âˆšï¸
ğ‘1(ğœ”), ...,
âˆšï¸
ğ‘2ğ‘—(ğœ”)ğ‘ğ‘œğ‘ ğ‘—ğœ‹ğ‘¡
ğœ”,
âˆšï¸
ğ‘2ğ‘—+1(ğœ”)ğ‘ ğ‘–ğ‘›ğ‘—ğœ‹ğ‘¡
ğœ”, ...] is the truncated
Fourier basis under certain frequencies of a class of continuous,
positive semi-definite (PSD) and translation-invariant periodical
kernels. The advantage of this Mercerâ€™s time encoding is that it can
take the continuous time-span information instead of the discrete
position index. And the frequencies ğœ”ğ‘˜and corresponding coeffi-
cients âˆšğ‘ğ‘˜ğ‘—are learnable parameters that can be jointly optimized
to cover a broad range of bandwidths in order to capture various
temporal patterns of the signal.
Limitation discussion. These related methods cannot encode
a non-stationary signal well. Specifically, by seeing Eq.3, the ampli-
tude (i.e., 1) and frequencies (i.e.,
1
100002ğ‘–/ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™) are predefined and
then kept constant during the learning process, which limits its
representation capability. Furthermore, since it takes the discrete
positional index as input, it cannot encode irregular continuous
time-interval information into the representation. In Eq.5, although
the amplitude (i.e.,
âˆšï¸
ğ‘âˆ—(ğœ”âˆ—)) and frequencies (i.e., ğ‘—ğœ‹
ğœ”âˆ—) are learnable,
they are independent with the absolute time ğ‘¡. As mentioned above,
the Mercerâ€™s time embedding is essentially a dual representation
of a temporal translation-invariant kernel. Taking the Mercerâ€™s
embedding into the context of expressing temporal correlations
between events < Î¦ğ‘€
ğ‘‘(ğ‘¡ğ‘–), Î¦ğ‘€
ğ‘‘(ğ‘¡ğ‘—) >, the correlation would natu-
rally be temporal translation-invariant thus limiting its capability
of encoding non-stationary signals.
3.4
Kernel Functions
A real kernel function [28] ğ‘˜: X Ã— X â†¦â†’R is a symmetric, PSD
function in its arguments for which the following property holds
ğ‘˜(ğ‘¥,ğ‘¥â€²) = ğœ™(ğ‘¥) Â· ğœ™(ğ‘¥â€²) for some feature map ğœ™. Kernel functions
usually serve as a similarity measure between two inputs in the
machine learning filed [14].
A stationary kernel is a function of ğœ= ğ‘¥âˆ’ğ‘¥â€², i.e., it is invariant
to translation of the inputs. Standard kernels such as Gaussian
kernel and MatÃ©rn kernels are both stationary. In [29], the authors
proposed a Spectral Mixture (SM) kernel which supports a broad
1226
