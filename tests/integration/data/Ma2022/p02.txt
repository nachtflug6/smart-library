KDD ‚Äô22, August 14‚Äì18, 2022, Washington DC, USA
Yu Ma et al.
Utility bills
(stationary)
one month
predict
Inter-city travel
(stationary)
one month
one
week
predict
Fund
Ôºànon-stationaryÔºâ
one
day
two weeks
predict
time axis
Figure 1: Illustration of a discrete mixed periodic signal, in-
cluding two stationary cases and one non-stationary case.
Most of the aforementioned methodology is under the implicit
assumption that the dataset is stationary, i.e., the underlying dis-
tribution does not change over time. However, that is not the case
in most real-world scenarios. In recommendation systems, user
behaviors and preferences usually change over time [7, 13, 21]. Sim-
ilar time-varying issues can be found in financial time series [6],
medical data analysis [17, 19], etc. Figure 1 illustrates a discrete
mixed periodical signal, which records a user‚Äôs usage of different
online services. Among them, utility bills and inter-city travel ser-
vices present stationary periodicity, while the purchase of fund
is non-stationary. Unlike the above mentioned related work that
mainly focuses on stationary signals, since the periodicity of a
non-stationary single would change over time, a powerful time
encoding method should jointly consider both of the time intervals
(marked by the red rectangle in Figure 1) and the absolute time
(marked by the red circle in Figure 1). Therefore, extending the
time representation learning ability in neural network structures
to non-stationary signals is new and of significant value in many
applications.
In this paper, we propose a non-stationary time-aware kernelized
attention (NsTKA) method to address the aforementioned time-
varying issues. Specifically, the NsTKA directly models the tem-
poral relations between events utilizing a non-stationary kernel
without introducing explicit time mapping functions. Unlike the
conventional attention mechanism [27] that makes event input and
time input as a whole (e.g., by embedding addition or concatena-
tion), our proposed method handles them separately. Hence, we
first present the mathematical relationship between related work
and our method. Then, a new attention neural network structure is
devised, which strictly corresponds to our proposed NsTKA. Finally,
the experimental results on both real-world and synthetic datasets
demonstrate the superiority of the proposed method.
To summarize, the main contributions of this paper can be de-
scribed as follows:
‚Ä¢ Mathematically, we present the proposed methodology and
its relationship with other related works.
‚Ä¢ We design a new non-stationary time-aware kernelized at-
tention structure to address the time-varying properties of
sequential data. The structure as a plugin, can also be ex-
tended to a variety of neural network structures or other
temporal process methodologies.
‚Ä¢ Our proposed method obtains competitive performance on
the synthetic dataset and extensive real-world datasets.
2
RELATED WORK
2.1
Sequential Encodings
In this paper, we are interested in incorporating non-stationary or-
der and temporal patterns into the attention mechanism. Although
the classic attention calculation is order-independent (namely, it is
an operation on sets), a wealth of related work has been dedicated
to adding explicit positional or temporal encodings.
Positional Encoding. While the original Transformer uses a
sinusoidal position signal (see Eq.3 for details) or learnable po-
sition embeddings [27], instead of encoding absolute position, it
has recently obtained significant improvement by incorporating
relative position embeddings [5, 24]. Although extending the at-
tention mechanism to efficiently consider representations of the
relative positions between sequence elements allows the model
to better recognize ordering information, it suffers from modeling
continuous-time event sequences, in which the time span between
events often matters.
Temporal Encoding. To overcome the shortcoming above, sev-
eral temporal encoding methods are proposed. In [30], the authors
proposed a method that embeds time span into high-dimensional
spaces using a set of stationary kernel basis functions in the form
of sine and cosine. In [25], the authors explicitly defined a learn-
able embedding method that captured both periodic (i.e., the sine
term) and non-periodic (i.e., the linear term) patterns. In sequential
recommendation, [15] considered both time intervals between two
items and the absolute positions of them. Nevertheless, few works
have been conducted on non-stationary time series.
Recurrence and Convolution. Through sequential structure,
conventional RNNs (e.g., LSTM [11] and GRU [3]) model the order
and temporal information implicitly by ‚Ñéùë°= ùëÄ(‚Ñéùë°‚àí1) + ùë•ùë°[2, 31],
where ùëÄis a nonlinear map, and ‚Ñéùë°and ùë•ùë°are ùëõ-vectors represent-
ing respectively the hidden state and the external input at time
ùë°. To deal with continuous time series, time gate [34] and classic
temporal point process [20, 32] are also introduced to couple with
RNNs. Although less common, by stacking several layers, CNNs,
like [8], inherently capture relative positions within the kernel size
of each convolution. We argue that these work are in general or-
thogonal to ours, and to some extent, our kernelized method can be
plugged into any methods aiming to capture the pairwise temporal
relations (e.g., time intervals in the time gate).
2.2
Kernelized Attention
With the tremendous success of Transformers, kernelized attention
is raised as a new research direction. In [26], the authors presented
a new formulation of attention from a view of kernel. To deal with
quadratic complexity of attention calculation, [18] proposed a ker-
nelized attention for acceleration. Regarding kernels as a form of
approximation of the attention matrix, they can be also viewed
as a form of low-rank method [4]. Unlike most of the kernelized
methods that focus on reducing the computational complexity, by
introducing a non-stationary kernel, our work aims to improve
the time representation ability of the attention mechanism. Fur-
thermore, using the mathematical relationship between kernel and
attention, in this paper, we organically couple the proposed non-
stationary temporal encoding kernel with the original attention
mechanism.
1225
