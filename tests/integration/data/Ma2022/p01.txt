Non-stationary Time-aware Kernelized Attention for Temporal
Event Prediction
Yu Ma‚àó
Zhining Liu‚àó
haishan.my@antgroup.com
eason.lzn@antgroup.com
Ant Group
Hangzhou, Zhejiang, China
Chenyi Zhuang
chenyi.zcy@antgroup.com
Ant Group
Hangzhou, Zhejiang, China
Yize Tan
yize.tyz@antgroup.com
Ant Group
Hangzhou, Zhejiang, China
Yi Dong
dongyi.dy@antgroup.com
Ant Group
Hangzhou, Zhejiang, China
Wenliang Zhong
yice.zwl@antgroup.com
Ant Group
Hangzhou, Zhejiang, China
Jinjie Gu
jinjie.gujj@antgroup.com
Ant Group
Hangzhou, Zhejiang, China
ABSTRACT
Modeling sequential data is essential to many applications such as
natural language processing, recommendation systems, time series
predictions, anomaly detection, etc. When processing sequential
data, one of the critical issues is how to capture the temporal-
correlation among events. Though prevalent and effective in many
applications, conventional approaches such as RNNs and Trans-
formers, struggle with handling the non-stationary characteristics
(i.e., such temporal-correlation among events would change over
time), which is indeed encountered in many real-world scenarios.
In this paper, we present a non-stationary time-aware kernelized
attention approach for input sequences of neural networks. By
constructing the Generalized Spectral Mixture Kernel (GSMK), and
integrating it to the attention mechanism, we mathematically re-
veal its representation capability in terms of the time-dependent
temporal-correlation. Following that, a novel neural network struc-
ture is proposed, which would enable us to encode both stationary
and non-stationary time event series. Finally, we demonstrate the
performance of the proposed method on both synthetic data which
presents the theoretical insights, and a variety of real-world datasets
which shows its competitive performance against related work.
CCS CONCEPTS
‚Ä¢ Computing methodologies ‚ÜíNeural networks; Kernel meth-
ods.
KEYWORDS
temporal event prediction; kernelized attention; non-stationarity
‚àóBoth authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD ‚Äô22, August 14‚Äì18, 2022, Washington DC, USA
¬© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9385-0/22/08...$15.00
https://doi.org/10.1145/3534678.3539470
ACM Reference Format:
Yu Ma, Zhining Liu, Chenyi Zhuang, Yize Tan, Yi Dong, Wenliang Zhong,
and Jinjie Gu. 2022. Non-stationary Time-aware Kernelized Attention for
Temporal Event Prediction. In Proceedings of the 28th ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining (KDD ‚Äô22), August 14‚Äì
18, 2022, Washington DC, USA. ACM, New York, NY, USA, 9 pages. https:
//doi.org/10.1145/3534678.3539470
1
INTRODUCTION
Modeling sequential data is essential to many applications such
as recommendation systems, time series predictions, anomaly de-
tection, etc. Unlike isolated event datasets, which only depend on
features and contexts at one point, sequential data requires catching
the relation among events. And in most real-world scenarios, such
relations not only depend on the characteristics of the event itself
but also the ordering of temporal or spatial information at which
the events occur. For example, in continuous-time event sequences,
the time span between the event occurrences has significant im-
plications on predicting the next occurring event. A more specific
example would be its periodical pattern of a re-occurring event.
Therefore, classical sequence modeling introduces mechanisms
like positional encoding and functional time representations to
count for order and temporal patterns. RNN models the order and
temporal information implicitly by assuming ùë¶ùë°= ùëÄ(ùë¶ùë°‚àí1) + ùúñùë°
[2, 31], where ùëÄ(¬∑) is the mapping function and ùúñùë°represents the
noise term. To improve the modeling of long-range dependency,
the gated mechanism has been introduced into RNN structures
which leads to GRU and LSTM [3, 11]. Some recent researches also
further improve GRU or LSTM units by adding specifically designed
time gates to capture the temporal information of the sequence
[1]. The attention mechanism [27] itself contains no recurrence
structure to count for the order and temporal information, thus it
introduces positional encoding explicitly to model the relative or
absolute position of the tokens in the sequence. Since the positional
encoding can only incorporate the ordering information, recent re-
searches have extended the positional encoding to several different
approaches of time representation learning to further address the
temporal patterns [15, 30].
1224
