KDD â€™22, August 14â€“18, 2022, Washington DC, USA
Yu Ma et al.
0 and 2ğœ‹generated by uniform distribution, and ğœ–ğ‘–is the noise
term following normal distribution. It is well known that when the
amplitudes and frequencies ğ´ğ‘–,ğœ”ğ‘–are both constants with regard
to time ğ‘¡, the signal is stationary, otherwise the signal would be
non-stationary.
To illustrate the capability of dealing with the non-stationary
signal of our proposed method, the amplitudes are designed to be
linearly increasing or decreasing with respect to time:
ğ´ğ‘–(ğ‘¡) = ğ´ğ‘–,0(1 + ğ›¼ğ‘–ğ‘¡/ğ‘‡),
(14)
where ğ´ğ‘–,0 is the initial amplitude when ğ‘¡= 0, ğ›¼ğ‘–is the increasing or
decreasing coefficient corresponding to positive or negative values,
and T is the length of the overall time. Similarly, the frequencies are
designed to be exponentially increasing or decreasing with time:
ğœ”ğ‘–(ğ‘¡) = ğœ”ğ‘–,0 âˆ—ğ‘’ğ‘¥ğ‘(ğ›½ğ‘–ğ‘¡/ğ‘‡),
(15)
where ğœ”ğ‘–,0 is the initial frequency when ğ‘¡= 0 and ğ›½ğ‘–is the increas-
ing or decreasing coefficient corresponding to positive or negative
values. In the following experiments, we use 5 sinusoidal compo-
nents with different frequencies and amplitudes and varying factors
to represent our SynD dataset.
In addition, to observe how different types of non-stationarities
affect the performance of different methods, we combine time-
varying amplitude function and time-varying frequency function,
which leads to three datasets, i.e., SynD-A (only time-varying am-
plitude function), SynD-F (only time-varying frequency function),
SynD-AF (both time-varying amplitude and frequency function are
included). Here the task is designed as one-step prediction given a
fixed length of the input sequence. Due to that the synthetic dataset
is controllable, it is of significant importance for presenting the
insights into our method.
5.1.2
Real-world Datasets. To further demonstrate the effective-
ness of the proposed method, we compare different methods on
three real-world datasets from various domains:
â€¢ ETT (Electricity Transformer Temperature)2 is a crucial in-
dicator in the electric power long-term deployment. [33] col-
lected and published 2-year data from two separated counties
in China. In this paper, we use the 1-hour level data collection
in the first site (noted as ğ¸ğ‘‡ğ‘‡â„1). The data point consists of
the target value "oil temperature" and 6 power load features.
The task is to predict the oil temperature in the next few
hours.
â€¢ PM2.5 dataset [16] records hourly PM2.5 value and the asso-
ciated meteorological measurements in Beijing City of China
from the year 2010 to 2014. The PM2.5 values are categorized
into six levels according to the United States Environmental
Protection Agency standard. Our task is to predict the class
of the PM2.5 level on the next day at 8 am using the collected
data in the previous day, when is the commuter peak period
in Beijing [12].
â€¢ An industrial dataset (IndD).3 We further provide an in-
dustrial dataset that records the behavioral data of millions of
2https://github.com/zhouhaoyi/ETDataset
3Note: The data set does not contain any Personal Identifiable Information (PII); The
data set is desensitized and encrypted; Adequate data protection was carried out during
the experiment to prevent the risk of data copy leakage, and the data set was destroyed
after the experiment; The data set is only used for academic research, it does not
represent any real business situation.
users. We collected interaction data with timestamps among
3072058 users and 200 services. Our task is to predict the
next service the user would visit on a platform.
Considering consistency and comparability, for the public datasets
ETT and PM2.5, we follow the same prepossessing steps mentioned
in [33] and [12].
For the ETT dataset, we use the first 12-month data as the train-
ing set and use the following 4-month and the next following 4-
month data as the corresponding validation and test set. We use
the sequence length of 96 data points to predict the next 24 steps.
For the PM2.5 dataset, we use 70% of the samples as the training set
and 15% of the samples for validation and the remaining 15 % of the
samples as the test dataset. We use the previous 24-hour data (i.e.,
sequence length 24) to predict the class of PM2.5 value at 8 am on
the next day. For the IndD, since it is a large dataset, we use 1% of
the samples as the validation set and 1% of the samples as the test
set. The overall statistics of the datasets are summarized in Table 1.
Name
Label type
Sequence
length
Prediction
length
Sample
numbers
SynD
Continuous
value
100
1
2,924
ETT
Continuous
value
96
24
8,640
PM2.5
Multi-class
24
1
1,215
IndD
Multi-class
300
1
14,049,979
Table 1: Statistics of datasets.
5.2
Comparison Methods
The baseline methods consist of two parts: (1) a typical RNN related
method; and (2) five Transformer and its various variants that are
introduced in a progressive manner.
â€¢ TimeLSTM. A conventional LSTM method equipped with
time gates for modeling time intervals. [34]
â€¢ RNN+Attention. A multi-time attention network that learns
embeddings of irregularly sampled time series. [25]
â€¢ TiSASRec. A state-of-the-art time interval aware self-attention
method for sequential recommendation. [15]
â€¢ Transformer w/o PE. The classic Transformer without
positional encoding.
â€¢ Transformer+PE. The classic Transformer with sinusoidal
positional encoding [27].
â€¢ Transformer+ME. The Transformer with mercer time em-
bedding [30].
â€¢ Transformer+NsTKA. Our proposed method.
We implemented our proposed method based on Keras4 with
the Adam optimizer. For the proposed NsTKA method, we treat the
number of mixture Gaussian components ğ‘šas a hyper-parameter
and select from {3, 4, 5, 6, 7, 8}. For all compared methods includ-
ing ours, we consider latent dimensions {8, 16, 32, 64, 128} and ğ‘™2
regularizer {0.0001, 0.001, 0.01, 0.1, 1}. All other hyperparameters
and initialization strategies (e.g., learning rate, optimizer, dropout,
maximum sequence length, maximum time interval, etc.) are similar
to the suggested optimal configurations by the methodsâ€™ authors.
4https://github.com/alipay/nstka-kdd22
1229
