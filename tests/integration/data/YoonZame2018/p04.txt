1480
IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 66, NO. 5, MAY 2019
Note that the function f we seek depends on the particular d and
t, and on the entire array of time stamps and measurements –
but not on labels (which may not be observed). Also note that
the formal problem asks to ﬁnd an f that minimizes the loss with
respect to the true distribution. Of course we do not observe the
true distribution and cannot compute the true loss, so we will
minimize the empirical loss.
IV. MULTI-DIRECTIONAL RECURRENT NEURAL
NETWORKS (M-RNN)
Suppose that stream d was not measured at time stamp t,
so that xd
t = ∗. We would like to form an estimate ˆxd
t of what
the actual measurement would have been. As we have noted,
familiar interpolation methods use only the measurements xd
t′
of the ﬁxed data stream d for other time stamps t′ ̸= t (perhaps
both before and after t) – but ignore the information contained in
other data streams d′ ̸= d; familiar imputation methods use only
the measurements xd′
t at the ﬁxed time t for other data streams
d′ ̸= d – but ignores the information contained at other times
t′ ̸= t. Because information is often correlated both within and
across data streams, each of these familiar approaches throws
away potentially useful information. Our approach forms an
estimate ˆxd
t using measurements both within the given data
stream and across other data streams. In principle, we could
try to form the estimate ˆxd
t by using all the information in D.
However, this would be impractical because it would require
learning a number of parameters that is on the order of the
square of the number of data streams, and also because it would
create a serious danger of over-ﬁtting. Instead, we propose an
efﬁcient hierarchical learning framework using a novel RNN
architecture that effectively allows us to capture the correlations
both within streams and across streams. Our approach limits the
number of parameters to be learned to be of the linear order of
the number data streams and avoids over-ﬁtting. See Fig. 1.
Our basic single-imputation M-RNN consists of 2 blocks: an
Interpolation block and an Imputation block; see Fig. 2. (Our
construction puts the Imputation block after the Interpolation
block in order to use the outputs of the Interpolation block to
improve the accuracy of the Imputation block; as we discuss
later, it would not be useful to put the Interpolation block after
the Imputation block.) To produce multiple imputations, we
adjoin an additional dropout layer to the basic single-imputation
M-RNN. (We defer the details until Section IV-D.) The entire
source codes of M-RNN implementation are publicly available
in the following link: http://github.com/jsyoon0823/MRNN/.
A. Error/Loss
As formalized above in Equation (1), our overall objective is
to minimize the error that would be made in estimating missing
measurements. Evidently, we cannot estimate the error of a
measurement that was not made and hence is truly missing in
the dataset. Instead we ﬁx a measurement xd
t that was made and
is present in the dataset, form an estimate ˆxd
t for xd
t using only
the dataset with xd
t removed (which we denote by D −xd
t ), and
then compute the error between the estimate ˆxd
t and the actual
measurement xd
t . As above, we use the squared error (ˆxd
t −xd
t )2
as the loss for this particular estimate; as the total loss/error for
the entire dataset D we use the mean squared error (MSE):
L(ˆx, x) =
N

n=1
Tn
t=1
D
d=1 md
t (n) × (ˆxd
t (n) −xd
t (n))2
Tn
t=1
D
d=1 md
t (n)

Note that this is the empirical error, which only utilized actually
achievable variables.
B. Interpolation Block
The Interpolation block constructs an interpolation function
Φd that operates within the d-th stream. To emphasize that the
output ˜xd
t of the interpolation block depends only on the d-th
data stream with xd
t removed, we write ˜xd
t = Φd(Dd −xd
t ),
where Dd is the d-th stream of the entire dataset D, and the
notation Dd −xd
t emphasizes that we have removed xd
t . It is
important to keep in mind that the construction uses only the
data from stream d, not the data from other streams. We construct
Φd using a bi-directional recurrent neural network (Bi-RNN).
However, unlike a conventional Bi-RNN [12], the timing of
inputs into the hidden layer is lagged in the forward direction
and advanced in the backward direction: at t, inputs of forward
hidden states come from t −1 and inputs of backward hidden
states come from t + 1. (This procedure ensures that the actual
value xd
t is not used in the estimation of ˜xd
t .) Note that each data
stream uses its own Bi-RNN architecture (Φd). The inputs of
the Interpolation block consist of the feature vector x, the mask
vector m, and the elapsed time vector δ (deﬁned in Section III,
and extracted from the original data streams). If we write zd
t =
[xd
t , md
t , δd
t ] (note that we explicitly include δd
t as the additional
input to deal with the irregular sampling procedures) then a
more mathematical description is:
˜xd
t = g(U d[−→
h d
t ; ←−
h d
t ] + cd
o) = g(−→
U d−→
h d
t + ←−
U d←−
h d
t + cd
o)
−→
h d
t = (1 −−→
u d
t ) ◦−→
h d
t−1
+ −→
u d
t ◦q(−→
W d
h(−→r d
t ◦−→
h d
t−1) + −→
V d
hzd
t−1 + −→c d
h)
−→
u d
t = γ(−→
W d
u
−→
h d
t−1 + −→
V d
uzd
t−1 + −→c d
u)
−→r d
t = γ(−→
W d
r
−→
h d
t−1 + −→
V d
r zd
t−1 + −→c d
r )
←−
h d
t = (1 −←−
u d
t ) ◦←−
h d
t+1
+ ←−
u d
t ◦q(←−
W d
h(←−r d
t ◦←−
h d
t+1) + ←−
V d
hzd
t+1 + ←−c d
h)
←−
u d
t = γ(←−
W d
u
←−
h d
t+1 + ←−
V d
uzd
t+1 + ←−c d
u)
←−r d
t = γ(←−
W d
r
←−
h d
t+1 + ←−
V d
r zd
t+1 + ←−c d
r )
(As can be seen from these equations, we are using a bidirec-
tional GRU.) Here, g, q, γ are activation functions. (In principle,
any activation functions, such as Rectiﬁed Linear Unit (ReLU),
tanh, etc., could be used; here we use ReLU.) The arrows indi-
cate forward/backward direction and ◦indicates element-wise
multiplication. As we have emphasized, in this interpolation
block, we are only using/capturing the temporal correlation
within each data stream. In particular, the parameters for each
Authorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on October 28,2024 at 18:11:05 UTC from IEEE Xplore.  Restrictions apply. 
