the manually selected interval is long, it may cause the loss
of temporal information; Conversely, it may increase the
missing data rate when the interval is short. Thus, a learning-
based method is introduced to obtain an optimal interval in
InterpNet (Shukla and Marlin 2019). However, when Interp-
Net ﬁnally speciﬁes the interval, it still unavoidably intro-
duces additional noise or causes information loss, because
different patients could have very different numbers of vis-
its. Moreover, these methods usually assume that there is an
expected ﬁxed time interval. This assumption may not valid
in practice due to the dynamics of diseases.
A better way to handle IMTS data is to directly model
the unequally spaced data. Time-aware LSTM (T-LSTM) in-
corporates irregular time intervals to adjust the hidden sta-
tus in the memory cell (Baytas et al. 2017). However, T-
LSTM is designed for ICD-9 codes, which cannot address
the missing data problem in real-valued variables. The re-
cently proposed GRU-D tries to handle both problems (Che
et al. 2018). GRU-D introduces observed records and corre-
sponding timestamps into standard GRU to impute missing
values as the decay of previous input values toward the over-
all mean over time. However, GRU-D only combines the
empirical mean value and the previous observation to im-
pute missing values. This strategy cannot capture the global
structure information of sequence data. Furthermore, GRU-
D ignores the diversity in the reliability of different data
points, especially the relatively larger unreliability of im-
puted records compared with actual records. As a result, it
assigns equal weights to actually observed data and imputed
data, which seriously damages its performance.
To address the aforementioned challenges, this paper
presents a novel end-to-end Dual-Attention Time-Aware
Gated Recurrent Unit (DATA-GRU) for IMTS to improve
the mortality risk prediction performance. To preserve in-
formative varying intervals, which reﬂect dynamics in the
conditions of patients, we introduce a time-aware structure
to handle irregular time intervals. This strategy avoids pro-
cessing IMTS into equally spaced, thus protecting tempo-
ral information in dense records and avoiding introducing
extra noise to sparse records. Since missing values cause
data misalignment, they need to be imputed so as to com-
pose tensor (Comon 2014). However, the imputation pro-
cess would impair medical information contained in missing
values. Therefore, we propose a novel dual-attention struc-
ture with two new attention mechanisms to simultaneously
focus on the data-quality view and the medical-knowledge
view. For the data-quality view, a novel unreliability-aware
attention mechanism is proposed to estimate diversity in the
unreliability of different data and accordingly assign them
learnable attention weights to ensure high-quality data play
more important roles. Our main ideas are that imputed data
normally are less reliable than actual records and different
imputed data could have different degrees of unreliability,
e.g., data inferred from sparse observations are less reliable
than from dense observations. For the medical-knowledge
view, a novel symptom-aware attention mechanism is pro-
posed to directly extract medical information from original
clinical records. Different from other domains, missing val-
ues in EHR data possess important medical considerations.
It should be noted that DATA-GRU is designed in an end-to-
end architecture to ensure the parameters of different parts
are trained jointly to achieve global optimal.
The main contributions of this paper are listed as follows:
• We propose a new end-to-end DATA-GRU network with
two novel structures to handle the two key challenges in
medical IMTS data analysis.
• We introduce a time-aware structure into deep learning
architecture to directly incorporate irregular time inter-
vals to adjust the inﬂuence of the previous status. This
strategy preserves the contained useful information on dy-
namic changes in the health status of patients.
• We design a new dual-attention structure to handle miss-
ing values from both data-quality and medical-knowledge
views. Novel unreliability-aware attention is proposed to
assign learnable weights to different data in coordination
with their reliabilities, while new symptom-aware atten-
tion is designed to learn medical information from the
sampling characteristics of original EHR data.
• We empirically show that DATA-GRU outperforms state-
of-the-art methods on two real-world datasets. The case
study indicates that the learned attention weights can pro-
vide meaningful clinical interpretation.
Related Work
Attention methods have been successfully applied in many
tasks, e.g., machine translation (Shankar and Sarawagi
2019) and computer vision (Fu et al. 2019). However, data
in their domains usually have regular time intervals or have
no time attribute, which is unsuitable for the EHR data.
Several works have investigated the attention mechanism
for EHR data. To increase the interpretability of networks,
RETAIN introduces an attention network to detect inﬂuen-
tial visits and key variables (Choi et al. 2016). A graph-
based attention model is proposed to learn robust repre-
sentations of EHR data (Choi et al. 2017). Similarly, (Ma
et al. 2018) introduce a knowledge-based attention mecha-
nism to embed nodes in the knowledge graph. Three atten-
tion mechanisms are introduced to measure relationships be-
tween current status and past state in RNN (Ma et al. 2017)
and monitor health conditions (Suo et al. 2017). The atten-
tion mechanism is also used to handle the low measuring
quality problem (Heo et al. 2018). (Song et al. 2018) use
masked self-attention to dispense the recurrence in the net-
work. The attention mechanisms used in these methods can
promote the performance and interpretability of models at
some extents. However, these attention-based methods are
usually designed for regular time-series data (or generated
regular data), thus cannot be applied to the irregularly sam-
pled EHR data, which is a key problem for health data.
Proposed Method
In this section, we present the proposed Dual-Attention
Time-Aware Gated Recurrent Unit (DATA-GRU). We ﬁrstly
introduce the notations used in this paper.
931
