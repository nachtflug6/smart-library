<?xml version="1.0" encoding="UTF-8"?>
<TEI
    xmlns="http://www.tei-c.org/ns/1.0"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
    <teiHeader xml:lang="en">
        <fileDesc>
            <titleStmt>
                <title level="a" type="main" coords="1,60.69,84.23,490.61,15.44;1,240.75,104.15,130.51,15.44">Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction</title>
            </titleStmt>
            <publicationStmt>
                <publisher>ACM</publisher>
                <availability status="unknown">
                    <licence/>
                </availability>
                <date type="published" when="2022-08-14">2022-08-14</date>
            </publicationStmt>
            <sourceDesc>
                <biblStruct>
                    <analytic>
                        <author>
                            <persName coords="1,124.81,127.58,31.24,10.59;1,156.05,125.33,1.00,7.77">
                                <forename type="first">Yu</forename>
                                <surname>Ma</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,111.63,141.53,57.96,10.59;1,169.58,139.27,1.00,7.77">
                                <forename type="first">Zhining</forename>
                                <surname>Liu</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,268.36,127.58,76.28,10.59">
                                <forename type="first">Chenyi</forename>
                                <surname>Zhuang</surname>
                            </persName>
                            <email>chenyi.zcy@antgroup.com</email>
                        </author>
                        <author>
                            <persName>
                                <forename type="first">Yize</forename>
                                <surname>Tan</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,121.69,211.87,39.96,10.59">
                                <forename type="first">Yi</forename>
                                <surname>Dong</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,264.87,211.87,81.68,10.59">
                                <forename type="first">Wenliang</forename>
                                <surname>Zhong</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,448.59,211.87,42.93,10.59">
                                <forename type="first">Jinjie</forename>
                                <surname>Gu</surname>
                            </persName>
                            <email>jinjie.gujj@antgroup.com</email>
                        </author>
                        <author>
                            <affiliation key="aff0" coords="1,120.23,178.70,43.90,8.83;1,86.30,190.65,112.09,8.83">
                                <orgName type="institution">Ant Group Hangzhou</orgName>
                                <address>
                                    <settlement>Zhejiang</settlement>
                                    <country key="CN">China</country>
                                </address>
                            </affiliation>
                        </author>
                        <author>
                            <affiliation key="aff1" coords="1,284.38,152.80,43.90,8.83;1,250.45,164.75,112.09,8.83">
                                <orgName type="institution">Ant Group Hangzhou</orgName>
                                <address>
                                    <settlement>Zhejiang</settlement>
                                    <country key="CN">China</country>
                                </address>
                            </affiliation>
                        </author>
                        <author>
                            <affiliation key="aff2" coords="1,448.53,152.80,43.90,8.83;1,414.60,164.75,112.09,8.83">
                                <orgName type="institution">Ant Group Hangzhou</orgName>
                                <address>
                                    <settlement>Zhejiang</settlement>
                                    <country key="CN">China</country>
                                </address>
                            </affiliation>
                        </author>
                        <author>
                            <affiliation key="aff3" coords="1,119.73,237.08,43.90,8.83;1,85.81,249.04,112.09,8.83">
                                <orgName type="institution">Ant Group Hangzhou</orgName>
                                <address>
                                    <settlement>Zhejiang</settlement>
                                    <country key="CN">China</country>
                                </address>
                            </affiliation>
                        </author>
                        <author>
                            <affiliation key="aff4" coords="1,283.88,237.08,43.90,8.83;1,249.96,249.04,112.09,8.83">
                                <orgName type="institution">Ant Group Hangzhou</orgName>
                                <address>
                                    <settlement>Zhejiang</settlement>
                                    <country key="CN">China</country>
                                </address>
                            </affiliation>
                        </author>
                        <author>
                            <affiliation key="aff5" coords="1,448.03,237.08,43.90,8.83;1,414.11,249.04,112.09,8.83">
                                <orgName type="institution">Ant Group Hangzhou</orgName>
                                <address>
                                    <settlement>Zhejiang</settlement>
                                    <country key="CN">China</country>
                                </address>
                            </affiliation>
                        </author>
                        <title level="a" type="main" coords="1,60.69,84.23,490.61,15.44;1,240.75,104.15,130.51,15.44">Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction</title>
                    </analytic>
                    <monogr>
                        <title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
                        <meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining						</meeting>
                        <imprint>
                            <publisher>ACM</publisher>
                            <biblScope unit="page" from="1224" to="1232"/>
                            <date type="published" when="2022-08-14"/>
                        </imprint>
                    </monogr>
                    <idno type="MD5">DF154ACB25D33E5C27C318D09198C895</idno>
                    <idno type="DOI">10.1145/3534678.3539470</idno>
                </biblStruct>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application version="0.8.2" ident="GROBID" when="2025-12-08T16:23+0000">
                    <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
                    <label type="revision">a91ee48</label>
                    <label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[ref, p, biblStruct, persName, figure, formula, head, note, title, affiliation], sentenceSegmentation=false, flavor=null</label>
                    <ref target="https://github.com/kermitt2/grobid"/>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords>
                    <term>Computing methodologies â†’ Neural networks</term>
                    <term>Kernel methods temporal event prediction</term>
                    <term>kernelized attention</term>
                    <term>non-stationarity</term>
                </keywords>
            </textClass>
            <abstract>
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <p coords="1,53.80,283.37,240.23,7.94;1,53.80,294.33,240.24,7.94;1,53.80,305.29,240.24,7.94;1,53.80,316.25,241.75,7.94;1,53.80,327.21,240.47,7.94;1,53.80,338.17,241.75,7.94;1,53.80,349.13,240.24,7.94;1,53.53,360.09,240.67,7.94;1,53.80,371.04,241.62,7.94;1,53.80,382.00,240.23,7.94;1,53.80,392.96,240.47,7.94;1,53.80,403.92,240.23,7.94;1,53.80,414.88,241.75,7.94;1,53.57,425.84,240.46,7.94;1,53.80,436.80,241.75,7.94;1,53.80,447.76,240.47,7.94;1,53.80,458.72,240.24,7.94;1,53.80,469.67,240.24,7.94;1,53.80,480.63,240.24,7.94;1,53.47,491.59,232.21,7.94">Modeling sequential data is essential to many applications such as natural language processing, recommendation systems, time series predictions, anomaly detection, etc. When processing sequential data, one of the critical issues is how to capture the temporalcorrelation among events. Though prevalent and effective in many applications, conventional approaches such as RNNs and Transformers, struggle with handling the non-stationary characteristics (i.e., such temporal-correlation among events would change over time), which is indeed encountered in many real-world scenarios. In this paper, we present a non-stationary time-aware kernelized attention approach for input sequences of neural networks. By constructing the Generalized Spectral Mixture Kernel (GSMK), and integrating it to the attention mechanism, we mathematically reveal its representation capability in terms of the time-dependent temporal-correlation. Following that, a novel neural network structure is proposed, which would enable us to encode both stationary and non-stationary time event series. Finally, we demonstrate the performance of the proposed method on both synthetic data which presents the theoretical insights, and a variety of real-world datasets which shows its competitive performance against related work.</p>
                </div>
            </abstract>
        </profileDesc>
    </teiHeader>
    <facsimile>
        <surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
        <surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
    </facsimile>
    <text xml:lang="en">
        <body>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="1" coords="1,317.96,379.95,102.73,9.37">INTRODUCTION</head>
                <p coords="1,317.96,394.64,240.24,7.94;1,317.96,405.60,241.75,7.94;1,317.96,416.56,240.24,7.94;1,317.96,427.52,240.24,7.94;1,317.96,438.48,240.23,7.94;1,317.96,449.44,240.23,7.94;1,317.96,460.40,240.23,7.94;1,317.96,471.36,241.22,7.94;1,317.96,482.31,241.75,7.94;1,317.96,493.27,240.23,7.94;1,317.96,504.23,232.93,7.94">Modeling sequential data is essential to many applications such as recommendation systems, time series predictions, anomaly detection, etc. Unlike isolated event datasets, which only depend on features and contexts at one point, sequential data requires catching the relation among events. And in most real-world scenarios, such relations not only depend on the characteristics of the event itself but also the ordering of temporal or spatial information at which the events occur. For example, in continuous-time event sequences, the time span between the event occurrences has significant implications on predicting the next occurring event. A more specific example would be its periodical pattern of a re-occurring event.</p>
                <p coords="1,327.92,515.19,230.27,7.94;1,317.96,526.15,240.24,7.94;1,317.96,537.11,240.24,7.94;1,317.96,548.07,178.70,7.94;1,501.42,547.58,24.50,7.93;1,525.89,547.58,31.08,8.79;1,339.73,558.54,162.79,8.43;1,506.00,559.03,52.20,7.94;1,317.96,569.99,241.23,7.94;1,317.96,580.95,240.24,7.94;1,317.62,591.90,110.61,7.94;1,452.01,591.90,106.19,7.94;1,317.96,602.86,240.24,7.94;1,317.96,613.82,240.24,7.94;1,327.47,624.78,102.55,7.94;1,450.43,624.78,107.76,7.94;1,317.96,635.74,240.23,7.94;1,317.96,646.70,240.41,7.94;1,317.96,657.66,240.24,7.94;1,317.96,668.62,241.75,7.94;1,317.96,679.58,240.24,7.94;1,317.96,690.53,240.23,7.94;1,317.96,701.49,65.32,7.94;1,411.39,701.49,3.38,7.94;2,63.76,228.98,230.27,7.94;2,53.80,239.94,241.75,7.94;2,53.80,250.90,240.24,7.94;2,53.80,261.86,240.40,7.94;2,53.80,272.82,184.55,7.94;2,273.72,272.82,21.85,7.94;2,53.80,283.78,226.13,7.94;2,291.86,283.78,3.18,7.94;2,53.80,294.74,80.37,7.94;2,163.44,294.74,45.52,7.94;2,218.68,294.74,75.36,7.94;2,53.80,305.69,240.23,7.94;2,53.80,316.58,241.76,8.03;2,53.57,327.54,240.47,8.03;2,53.80,338.57,240.24,7.94;2,53.80,349.53,240.24,7.94;2,53.80,360.49,240.24,7.94;2,53.80,371.45,240.24,7.94;2,53.53,382.41,146.36,7.94;2,206.12,382.41,87.91,7.94;2,53.53,393.37,132.54,7.94;2,191.89,393.37,102.14,7.94;2,53.80,404.32,240.23,7.94;2,53.80,415.28,240.47,7.94;2,53.80,426.24,46.25,7.94">Therefore, classical sequence modeling introduces mechanisms like positional encoding and functional time representations to count for order and temporal patterns. RNN models the order and temporal information implicitly by assuming ğ‘¦ ğ‘¡ = ğ‘€ (ğ‘¦ ğ‘¡ -1 ) + ğœ– ğ‘¡ 
                    <ref type="bibr" coords="1,317.96,559.03,9.38,7.94" target="#b1">[2,</ref>
                    <ref type="bibr" coords="1,329.58,559.03,10.16,7.94" target="#b30">31]</ref>, where ğ‘€ (â€¢) is the mapping function and ğœ– ğ‘¡ represents the noise term. To improve the modeling of long-range dependency, the gated mechanism has been introduced into RNN structures which leads to GRU and LSTM 
                    <ref type="bibr" coords="1,430.47,591.90,9.24,7.94" target="#b2">[3,</ref>
                    <ref type="bibr" coords="1,441.95,591.90,10.06,7.94" target="#b10">11]</ref>. Some recent researches also further improve GRU or LSTM units by adding specifically designed time gates to capture the temporal information of the sequence 
                    <ref type="bibr" coords="1,317.96,624.78,9.52,7.94" target="#b0">[1]</ref>. The attention mechanism 
                    <ref type="bibr" coords="1,432.80,624.78,14.85,7.94" target="#b26">[27]</ref> itself contains no recurrence structure to count for the order and temporal information, thus it introduces positional encoding explicitly to model the relative or absolute position of the tokens in the sequence. Since the positional encoding can only incorporate the ordering information, recent researches have extended the positional encoding to several different approaches of time representation learning to further address the temporal patterns 
                    <ref type="bibr" coords="1,385.51,701.49,13.50,7.94" target="#b14">[15,</ref>
                    <ref type="bibr" coords="1,401.26,701.49,10.13,7.94" target="#b29">30]</ref>. Most of the aforementioned methodology is under the implicit assumption that the dataset is stationary, i.e., the underlying distribution does not change over time. However, that is not the case in most real-world scenarios. In recommendation systems, user behaviors and preferences usually change over time 
                    <ref type="bibr" coords="2,240.29,272.82,9.24,7.94" target="#b6">[7,</ref>
                    <ref type="bibr" coords="2,251.46,272.82,10.27,7.94" target="#b12">13,</ref>
                    <ref type="bibr" coords="2,263.67,272.82,10.05,7.94" target="#b20">21]</ref>. Similar time-varying issues can be found in financial time series 
                    <ref type="bibr" coords="2,282.33,283.78,9.53,7.94" target="#b5">[6]</ref>, medical data analysis 
                    <ref type="bibr" coords="2,136.90,294.74,13.61,7.94" target="#b16">[17,</ref>
                    <ref type="bibr" coords="2,153.24,294.74,10.21,7.94" target="#b18">19]</ref>, etc. Figure 
                    <ref type="figure" coords="2,211.69,294.74,4.25,7.94" target="#fig_0">1</ref> illustrates a discrete mixed periodical signal, which records a user's usage of different online services. Among them, utility bills and inter-city travel services present stationary periodicity, while the purchase of fund is non-stationary. Unlike the above mentioned related work that mainly focuses on stationary signals, since the periodicity of a non-stationary single would change over time, a powerful time encoding method should jointly consider both of the time intervals (marked by the red rectangle in Figure 
                    <ref type="figure" coords="2,202.63,382.41,3.49,7.94" target="#fig_0">1</ref>) and the absolute time (marked by the red circle in Figure 
                    <ref type="figure" coords="2,188.90,393.37,3.00,7.94" target="#fig_0">1</ref>). Therefore, extending the time representation learning ability in neural network structures to non-stationary signals is new and of significant value in many applications.
                </p>
                <p coords="2,63.76,437.20,230.28,7.94;2,53.80,448.16,241.75,7.94;2,53.57,459.12,241.98,7.94;2,53.80,470.08,240.24,7.94;2,53.47,481.04,240.57,7.94;2,53.80,492.00,124.27,7.94;2,196.76,492.00,97.29,7.94;2,53.80,502.95,241.75,7.94;2,53.80,513.91,240.24,7.94;2,53.80,524.87,240.47,7.94;2,53.80,535.83,240.24,7.94;2,53.80,546.79,241.22,7.94;2,53.80,557.75,240.23,7.94;2,53.80,568.71,193.78,7.94">In this paper, we propose a non-stationary time-aware kernelized attention (NsTKA) method to address the aforementioned timevarying issues. Specifically, the NsTKA directly models the temporal relations between events utilizing a non-stationary kernel without introducing explicit time mapping functions. Unlike the conventional attention mechanism 
                    <ref type="bibr" coords="2,180.11,492.00,14.60,7.94" target="#b26">[27]</ref> that makes event input and time input as a whole (e.g., by embedding addition or concatenation), our proposed method handles them separately. Hence, we first present the mathematical relationship between related work and our method. Then, a new attention neural network structure is devised, which strictly corresponds to our proposed NsTKA. Finally, the experimental results on both real-world and synthetic datasets demonstrate the superiority of the proposed method.
                </p>
                <p coords="2,63.76,579.67,231.79,7.94;2,53.80,590.63,67.09,7.94">To summarize, the main contributions of this paper can be described as follows:</p>
                <p coords="2,69.77,613.34,224.27,8.43;2,78.21,624.78,149.77,7.94;2,69.77,635.26,225.78,8.43;2,78.21,646.70,215.83,7.94;2,78.21,657.66,217.35,7.94;2,78.21,668.62,215.99,7.94;2,78.21,679.58,120.44,7.94;2,69.77,690.05,224.27,8.43;2,78.21,701.49,202.13,7.94">â€¢ Mathematically, we present the proposed methodology and its relationship with other related works. â€¢ We design a new non-stationary time-aware kernelized attention structure to address the time-varying properties of sequential data. The structure as a plugin, can also be extended to a variety of neural network structures or other temporal process methodologies. â€¢ Our proposed method obtains competitive performance on the synthetic dataset and extensive real-world datasets.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="2" coords="2,317.96,86.80,103.79,9.37;2,317.96,102.49,130.85,9.37">RELATED WORK 2.1 Sequential Encodings</head>
                <p coords="2,317.96,117.18,241.75,7.94;2,317.96,128.14,240.23,7.94;2,317.96,139.10,240.24,7.94;2,317.96,150.06,240.23,7.94;2,317.96,161.02,188.78,7.94;2,327.92,171.98,230.28,7.94;2,317.96,182.94,241.75,7.94;2,317.96,193.90,68.39,7.94;2,402.35,193.90,155.84,7.94;2,317.96,204.86,240.24,7.94;2,317.96,215.81,108.43,7.94;2,451.23,215.81,108.48,7.94;2,317.96,226.77,240.24,7.94;2,317.96,237.73,240.24,7.94;2,317.96,248.69,240.24,7.94;2,317.96,259.65,240.23,7.94;2,317.96,270.61,76.67,7.94">In this paper, we are interested in incorporating non-stationary order and temporal patterns into the attention mechanism. Although the classic attention calculation is order-independent (namely, it is an operation on sets), a wealth of related work has been dedicated to adding explicit positional or temporal encodings. Positional Encoding. While the original Transformer uses a sinusoidal position signal (see Eq.3 for details) or learnable position embeddings 
                    <ref type="bibr" coords="2,388.86,193.90,13.49,7.94" target="#b26">[27]</ref>, instead of encoding absolute position, it has recently obtained significant improvement by incorporating relative position embeddings 
                    <ref type="bibr" coords="2,428.99,215.81,9.44,7.94" target="#b4">[5,</ref>
                    <ref type="bibr" coords="2,441.03,215.81,10.21,7.94" target="#b23">24]</ref>. Although extending the attention mechanism to efficiently consider representations of the relative positions between sequence elements allows the model to better recognize ordering information, it suffers from modeling continuous-time event sequences, in which the time span between events often matters.
                </p>
                <p coords="2,327.92,281.57,231.80,7.94;2,317.96,292.53,177.85,7.94;2,511.33,292.53,46.87,7.94;2,317.96,303.49,240.23,7.94;2,317.96,314.44,240.23,7.94;2,317.96,325.40,79.19,7.94;2,413.06,325.40,146.65,7.94;2,317.96,336.36,240.24,7.94;2,317.96,347.32,240.24,7.94;2,317.96,358.28,62.88,7.94;2,399.97,358.28,158.23,7.94;2,317.96,369.24,240.23,7.94;2,317.96,380.20,189.00,7.94">Temporal Encoding. To overcome the shortcoming above, several temporal encoding methods are proposed. In 
                    <ref type="bibr" coords="2,498.05,292.53,13.28,7.94" target="#b29">[30]</ref>, the authors proposed a method that embeds time span into high-dimensional spaces using a set of stationary kernel basis functions in the form of sine and cosine. In 
                    <ref type="bibr" coords="2,399.57,325.40,13.49,7.94" target="#b24">[25]</ref>, the authors explicitly defined a learnable embedding method that captured both periodic (i.e., the sine term) and non-periodic (i.e., the linear term) patterns. In sequential recommendation, 
                    <ref type="bibr" coords="2,383.10,358.28,14.61,7.94" target="#b14">[15]</ref> considered both time intervals between two items and the absolute positions of them. Nevertheless, few works have been conducted on non-stationary time series.
                </p>
                <p coords="2,327.92,391.16,231.26,7.94;2,317.96,402.12,113.18,7.94;2,450.27,402.12,32.72,7.94;2,495.07,402.12,63.30,7.94;2,317.96,413.07,154.11,7.94;2,475.79,412.59,23.68,7.93;2,499.53,412.59,30.97,8.79;2,555.80,413.07,3.39,7.94;2,317.62,424.03,127.42,7.94;2,448.47,424.03,22.11,7.94;2,474.00,424.03,85.71,7.94;2,317.96,434.99,240.24,7.94;2,317.64,445.95,180.71,7.94;2,518.00,445.95,40.19,7.94;2,317.96,456.91,83.84,7.94;2,433.55,456.91,124.65,7.94;2,317.96,467.87,241.23,7.94;2,317.96,478.83,13.13,7.94;2,342.56,478.83,215.63,7.94;2,317.96,489.79,241.75,7.94;2,317.96,500.75,240.24,7.94;2,317.96,511.70,240.24,7.94;2,317.96,522.66,168.15,7.94">Recurrence and Convolution. Through sequential structure, conventional RNNs (e.g., LSTM 
                    <ref type="bibr" coords="2,433.37,402.12,14.66,7.94" target="#b10">[11]</ref> and GRU 
                    <ref type="bibr" coords="2,485.22,402.12,9.85,7.94" target="#b2">[3]</ref>) model the order and temporal information implicitly by â„ ğ‘¡ = ğ‘€ (â„ ğ‘¡ -1 ) + ğ‘¥ ğ‘¡ 
                    <ref type="bibr" coords="2,533.97,413.07,9.40,7.94" target="#b1">[2,</ref>
                    <ref type="bibr" coords="2,545.62,413.07,10.18,7.94" target="#b30">31]</ref>, where ğ‘€ is a nonlinear map, and â„ ğ‘¡ and ğ‘¥ ğ‘¡ are ğ‘›-vectors representing respectively the hidden state and the external input at time ğ‘¡. To deal with continuous time series, time gate 
                    <ref type="bibr" coords="2,500.75,445.95,14.86,7.94" target="#b33">[34]</ref> and classic temporal point process 
                    <ref type="bibr" coords="2,404.04,456.91,13.49,7.94" target="#b19">[20,</ref>
                    <ref type="bibr" coords="2,419.78,456.91,11.52,7.94" target="#b31">32]</ref> are also introduced to couple with RNNs. Although less common, by stacking several layers, CNNs, like 
                    <ref type="bibr" coords="2,333.29,478.83,9.27,7.94" target="#b7">[8]</ref>, inherently capture relative positions within the kernel size of each convolution. We argue that these work are in general orthogonal to ours, and to some extent, our kernelized method can be plugged into any methods aiming to capture the pairwise temporal relations (e.g., time intervals in the time gate).
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="2.2" coords="2,317.96,544.33,128.99,9.37">Kernelized Attention</head>
                <p coords="2,317.53,559.03,240.67,7.94;2,317.96,569.99,141.10,7.94;2,474.49,569.99,83.70,7.94;2,317.96,580.95,240.23,7.94;2,317.96,591.90,165.63,7.94;2,502.74,591.90,56.98,7.94;2,317.96,602.86,240.23,7.94;2,317.96,613.82,240.24,7.94;2,317.96,624.78,112.02,7.94;2,441.91,624.78,116.29,7.94;2,317.96,635.74,240.47,7.94;2,317.96,646.70,240.24,7.94;2,317.96,657.66,241.75,7.94;2,317.96,668.62,240.24,7.94;2,317.96,679.58,241.75,7.94;2,317.96,690.53,240.24,7.94;2,317.96,701.49,43.75,7.94">With the tremendous success of Transformers, kernelized attention is raised as a new research direction. In 
                    <ref type="bibr" coords="2,461.27,569.99,13.23,7.94" target="#b25">[26]</ref>, the authors presented a new formulation of attention from a view of kernel. To deal with quadratic complexity of attention calculation, 
                    <ref type="bibr" coords="2,485.82,591.90,14.69,7.94" target="#b17">[18]</ref> proposed a kernelized attention for acceleration. Regarding kernels as a form of approximation of the attention matrix, they can be also viewed as a form of low-rank method 
                    <ref type="bibr" coords="2,432.38,624.78,9.53,7.94" target="#b3">[4]</ref>. Unlike most of the kernelized methods that focus on reducing the computational complexity, by introducing a non-stationary kernel, our work aims to improve the time representation ability of the attention mechanism. Furthermore, using the mathematical relationship between kernel and attention, in this paper, we organically couple the proposed nonstationary temporal encoding kernel with the original attention mechanism.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3" coords="3,53.80,86.80,101.88,9.37">PRELIMINARIES</head>
                <p coords="3,53.37,101.49,240.67,7.94;3,53.80,112.45,240.23,7.94;3,53.80,123.41,241.23,7.94;3,53.80,134.37,240.24,7.94;3,53.80,145.33,67.05,7.94">We focus on solving the non-stationary time encoding problem in the attention manner. In this section, we first present the main notations used in this problem. Then, the basic attention module, positional and temporal encoding, and kernel functions will be introduced briefly.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.1" coords="3,53.80,170.48,73.10,9.37">Notations</head>
                <p coords="3,53.80,184.69,79.04,9.38;3,132.86,187.11,15.69,5.34;3,149.31,184.69,144.73,8.43;3,53.80,196.13,61.20,7.94;3,118.26,196.13,71.49,7.94;3,190.76,196.13,27.43,7.94;3,219.87,195.65,21.23,7.76;3,242.57,195.65,51.47,8.43;3,53.80,207.62,186.40,8.10;3,239.84,207.06,3.91,3.24;3,244.83,207.62,50.20,7.94;3,53.47,219.12,80.97,7.94;3,138.75,218.63,13.87,8.58;3,152.33,218.55,12.60,3.24;3,168.45,219.12,125.59,7.94;3,53.80,229.59,149.17,8.43">Let S = {(ğ‘¡ 1 , ğ‘’ 1 ), ..., (ğ‘¡ ğ‘› , ğ‘’ ğ‘› )} denotes the sequence that contains a series of event ğ‘’ ğ‘– happened at time ğ‘¡ ğ‘– , and ğœ™ ğ‘¡ (ğ‘¡), ğœ™ ğ‘’ (ğ‘’) represents learnable mapping functions that encode ğ‘¡, ğ‘’ into R ğ‘‘ . Accordingly, we use matrices ğ‘‰ ğ‘’ , ğ‘‰ ğ‘¡ âˆˆ R ğ‘›Ã—ğ‘‘ to denote the encoded events and temporal embeddings of the sequence S.</p>
                <p coords="3,63.76,241.04,141.27,7.94;3,208.52,241.04,87.04,7.94;3,53.80,251.99,140.58,7.94;3,195.40,251.99,99.62,7.94;3,53.47,262.95,157.89,7.94;3,215.43,262.95,24.07,7.94;3,240.52,262.95,53.70,7.94;3,53.80,273.91,240.25,7.94;3,53.80,284.87,241.22,7.94;3,53.80,295.83,240.24,7.94;3,53.80,306.55,120.14,8.19">In practice, the temporal embedding ğ‘‰ ğ‘¡ is usually added or concatenated with the event embedding ğ‘‰ ğ‘’ . Without loss of generality, we use ğ‘‰ to denote the combination of ğ‘‰ ğ‘¡ and ğ‘‰ ğ‘’ . On the other hand, regarding the input to attention module, we use ğ¾, ğ‘„, ğ‘‰ to denote the key, query, and value matrices, respectively. Note that, in our self-attention setting, the key and query matrices equal to the value matrix, i.e., ğ¾ = ğ‘„ = ğ‘‰ .</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.2" coords="3,53.80,331.94,113.02,9.37">Attention Module</head>
                <p coords="3,53.53,346.63,189.23,7.94;3,260.75,346.63,33.29,7.94;3,53.80,357.59,124.07,7.94">The attention mechanism in the original Transformer 
                    <ref type="bibr" coords="3,244.45,346.63,14.61,7.94" target="#b26">[27]</ref> is formed in the scaled dot-product manner:
                </p>
                <formula xml:id="formula_0" coords="3,63.15,381.58,230.89,22.63">ğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„, ğ¾, ğ‘‰ ) = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ ( (ğ‘„ğ‘Š ğ‘„ ) (ğ¾ğ‘Š ğ¾ ) ğ‘‡ âˆšï¸ ğ‘‘ ğ‘˜ ) (ğ‘‰ğ‘Š ğ‘‰ ),
                    <label>(1)</label>
                </formula>
                <p coords="3,53.47,419.72,32.67,7.94;3,89.45,419.72,205.96,7.94;3,52.99,430.30,49.17,6.52;3,105.73,430.86,189.29,7.94;3,53.80,441.82,240.24,7.94;3,53.80,452.78,240.24,7.94;3,53.80,463.74,87.18,7.94">where ğ‘‘ ğ‘˜ is the dimension of the hidden representation of the key. ğ‘Š ğ¾ ,ğ‘Š ğ‘„ ,ğ‘Š ğ‘‰ are the projection matrices for any specific head of Q, K, V, respectively. To simplify notations in ğ‘˜-head attention module that has ğ‘˜ sets of projection matrices, in the following sections, we omit them in equations.</p>
                <p coords="3,63.76,474.70,230.28,8.92;3,53.80,485.66,240.23,7.94;3,53.44,496.13,240.16,9.41;3,53.80,507.58,240.24,7.94;3,53.80,518.53,71.95,7.94">To be more specific, let q i , k i , v i represent the ğ‘–-th component of Q, K, V, which are vectors representing the input embedding at ğ‘–-th position of the query, key, value sequences. z = (z 1 , ..., z i , ...z n ) denotes the output of attention module. Then, the attention formula can be rewritten as:</p>
                <formula xml:id="formula_1" coords="3,125.64,539.79,96.12,58.14">z i = ğ‘› âˆ‘ï¸ ğ‘—=1 ğ‘˜ ğ‘’ğ‘¥ğ‘ (q i , k j ) ğ‘˜ ğ‘’ğ‘¥ğ‘ (q m , k l ) v j , ğ‘˜ ğ‘’ğ‘¥ğ‘ (q i , k j ) = ğ‘’ğ‘¥ğ‘ ( q i k T j âˆšï¸ ğ‘‘ ğ‘˜</formula>
                <p coords="3,214.53,581.46,5.55,7.70">).</p>
                <p coords="3,284.53,565.19,3.17,7.94">(</p>
                <formula xml:id="formula_2" coords="3,287.70,565.19,6.34,7.94">)
                    <label>2</label>
                </formula>
                <p coords="3,63.76,613.29,231.79,7.94;3,53.57,624.25,240.46,7.94">As we can see from Eq.2, the relative contribution of each keyvalue pair to each query is determined by the attention weight</p>
                <formula xml:id="formula_3" coords="3,59.66,633.22,33.85,7.49">ğ‘˜ ğ‘’ğ‘¥ğ‘ (q i ,k j )</formula>
                <p coords="3,61.79,642.24,36.67,7.69;3,102.48,637.94,92.45,8.92">ğ‘˜ ğ‘’ğ‘¥ğ‘ (q m ,k l ) times the value vector v j .</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.3" coords="3,53.80,664.88,205.32,9.37">Positional and Temporal Embedding</head>
                <p coords="3,53.53,679.58,190.17,7.94;3,264.62,679.58,29.43,7.94;3,53.80,690.53,159.74,7.94;3,214.77,690.53,79.27,7.94;3,53.80,701.49,127.46,7.94;3,184.73,701.49,109.31,7.94;3,317.96,87.79,127.59,7.94">To count for the order information of a sequence, 
                    <ref type="bibr" coords="3,246.73,679.58,14.86,7.94" target="#b26">[27]</ref> added a non-learnable positional embedding, i.e., ğ‘‰ ğ‘¡ , as the input into the attention module. Each entry in ğ‘‰ ğ‘¡ is calculated by the following sin and cos functions, respectively:
                </p>
                <formula xml:id="formula_4" coords="3,368.53,102.94,189.67,40.88">ğ‘ƒğ¸ (ğ‘ğ‘œğ‘ , 2ğ‘–) = ğ‘ ğ‘–ğ‘›( ğ‘ğ‘œğ‘  10000 2ğ‘–/ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ), ğ‘ƒğ¸ (ğ‘ğ‘œğ‘ , 2ğ‘– + 1) = ğ‘ğ‘œğ‘  ( ğ‘ğ‘œğ‘  10000 2ğ‘–/ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ),
                    <label>(3)</label>
                </formula>
                <p coords="3,317.62,148.89,239.96,7.94;3,317.96,159.85,240.23,7.94;3,317.51,172.74,24.02,5.96;3,344.97,170.80,214.21,7.94;3,317.96,181.76,240.24,7.94;3,317.51,194.66,24.02,5.96;3,344.97,192.72,213.22,7.94;3,317.96,203.68,117.54,7.94;3,327.92,214.64,42.01,7.94;3,385.38,214.64,172.81,7.94;3,317.96,225.60,240.25,7.94;3,317.96,236.56,160.60,7.94">where ğ‘ğ‘œğ‘  is the position of the token/event in the sequence, ğ‘– corresponds to 2 consecutive position embedding dimensions, and ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ is the dimension of the token/event embedding. In practice, the dimension of the positional encoding is usually the same as ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ that the positional embedding and event embedding can be summed as the attention inputs. Recently, in 
                    <ref type="bibr" coords="3,372.16,214.64,13.23,7.94" target="#b29">[30]</ref>, the authors further proposed a functional time encoding method which maps the continuous-time values ğ‘¡ to high dimensional space by Mercer's feature map:
                </p>
                <formula xml:id="formula_5" coords="3,381.61,252.89,176.59,10.58">ğ‘¡ â†¦ â†’ Î¦ = [Î¦ ğœ” 1 (ğ‘¡), ..., Î¦ ğœ” ğ‘‘ (ğ‘¡)] ğ‘‡ ,
                    <label>(4)</label>
                </formula>
                <p coords="3,317.62,270.61,22.80,7.94">where</p>
                <formula xml:id="formula_6" coords="3,344.18,285.82,210.85,24.75">Î¦ ğœ” ğ‘– (ğ‘¡) = ğ‘ âˆ‘ï¸ ğ‘—=1 âˆšï¸ƒ ğ‘ 2ğ‘— (ğœ” ğ‘– )ğ‘ğ‘œğ‘  ğ‘—ğœ‹ğ‘¡ ğœ” ğ‘– + âˆšï¸ƒ ğ‘ 2ğ‘—+1 (ğœ” ğ‘– )ğ‘ ğ‘–ğ‘› ğ‘—ğœ‹ğ‘¡ ğœ” ğ‘– . (
                    <label>5</label>
                </formula>
                <formula xml:id="formula_7" coords="3,328.46,294.10,229.74,37.59">) [ âˆšï¸ ğ‘ 1 (ğœ”), ..., âˆšï¸ ğ‘ 2ğ‘— (ğœ”)ğ‘ğ‘œğ‘  ğ‘—ğœ‹ğ‘¡ ğœ” , âˆšï¸ ğ‘ 2ğ‘—+1 (ğœ”)ğ‘ ğ‘–ğ‘› ğ‘—ğœ‹ğ‘¡ ğœ” , .</formula>
                <p coords="3,499.98,322.66,63.95,7.94;3,317.96,333.62,241.23,7.94;3,317.96,344.58,240.24,7.94;3,317.96,355.53,240.24,7.94;3,317.96,366.49,240.24,7.94;3,317.96,377.45,143.47,7.94;3,464.74,377.45,94.98,7.94;3,317.96,388.41,21.31,7.94;3,341.51,383.13,6.30,7.70;3,347.41,390.35,10.72,5.96;3,361.58,388.41,196.61,7.94;3,317.96,399.37,240.23,7.94;3,317.96,410.33,114.71,7.94">..] is the truncated Fourier basis under certain frequencies of a class of continuous, positive semi-definite (PSD) and translation-invariant periodical kernels. The advantage of this Mercer's time encoding is that it can take the continuous time-span information instead of the discrete position index. And the frequencies ğœ” ğ‘˜ and corresponding coefficients âˆš ğ‘ ğ‘˜ ğ‘— are learnable parameters that can be jointly optimized to cover a broad range of bandwidths in order to capture various temporal patterns of the signal.</p>
                <p coords="3,327.92,421.29,230.28,7.94;3,317.96,432.25,241.75,7.94;3,317.96,443.37,117.69,7.94;3,460.29,440.94,3.38,6.44;3,439.08,447.97,44.25,7.93;3,486.08,443.37,72.12,7.94;3,317.96,455.59,240.24,7.94;3,317.96,466.55,240.23,7.94;3,317.96,477.51,240.24,7.94;3,317.96,488.47,240.24,7.94;3,317.96,500.52,65.93,7.94;3,385.81,498.11,6.56,5.99;3,391.97,500.04,102.29,8.79;3,498.01,498.73,6.88,3.24;3,497.17,506.31,8.00,5.09;3,506.86,500.52,52.32,7.94;3,317.96,511.86,241.23,7.94;3,317.96,522.82,240.24,7.94;3,317.96,533.78,240.24,7.94;3,317.96,544.74,240.24,7.94;3,317.96,555.01,74.99,8.97;3,392.87,555.32,6.17,3.24;3,392.58,562.49,3.91,3.24;3,400.60,555.40,5.78,7.70;3,406.41,559.92,2.23,3.24;3,409.65,555.01,12.49,8.97;3,422.07,555.32,6.17,3.24;3,421.78,562.49,3.91,3.24;3,429.80,555.40,5.78,7.70;3,436.48,559.92,2.09,3.24;3,439.79,555.40,119.92,8.43;3,317.96,567.00,240.47,7.94;3,317.96,577.96,129.34,7.94">Limitation discussion. These related methods cannot encode a non-stationary signal well. Specifically, by seeing Eq.3, the amplitude (i.e., 1) and frequencies (i.e., 1 10000 2ğ‘–/ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ) are predefined and then kept constant during the learning process, which limits its representation capability. Furthermore, since it takes the discrete positional index as input, it cannot encode irregular continuous time-interval information into the representation. In Eq.5, although the amplitude (i.e., âˆšï¸ ğ‘ * (ğœ” * )) and frequencies (i.e., ğ‘—ğœ‹ ğœ” * ) are learnable, they are independent with the absolute time ğ‘¡. As mentioned above, the Mercer's time embedding is essentially a dual representation of a temporal translation-invariant kernel. Taking the Mercer's embedding into the context of expressing temporal correlations between events &lt; Î¦ ğ‘€ ğ‘‘ (ğ‘¡ ğ‘– ), Î¦ ğ‘€ ğ‘‘ (ğ‘¡ ğ‘— ) &gt;, the correlation would naturally be temporal translation-invariant thus limiting its capability of encoding non-stationary signals.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="3.4" coords="3,317.96,599.13,109.90,9.37">Kernel Functions</head>
                <p coords="3,317.64,613.82,83.26,7.94;3,420.57,613.34,137.63,8.58;3,317.96,624.78,240.23,7.94;3,317.60,635.26,20.89,7.70;3,339.59,633.20,218.61,10.49;3,317.96,646.70,240.24,7.94;3,317.96,657.66,82.06,7.94;3,415.61,657.66,3.34,7.94">A real kernel function 
                    <ref type="bibr" coords="3,403.49,613.82,14.85,7.94" target="#b27">[28]</ref> ğ‘˜ : X Ã— X â†¦ â†’ R is a symmetric, PSD function in its arguments for which the following property holds ğ‘˜ (ğ‘¥, ğ‘¥ â€² ) = ğœ™ (ğ‘¥) â€¢ ğœ™ (ğ‘¥ â€² ) for some feature map ğœ™. Kernel functions usually serve as a similarity measure between two inputs in the machine learning filed 
                    <ref type="bibr" coords="3,402.25,657.66,13.36,7.94" target="#b13">[14]</ref>.
                </p>
                <p coords="3,327.92,668.13,160.09,8.43;3,489.10,666.07,2.52,6.25;3,491.91,668.62,66.30,7.94;3,317.96,679.58,240.24,7.94;3,317.96,690.53,177.87,7.94;3,511.36,690.53,46.84,7.94;3,317.96,701.49,240.23,7.94;4,53.80,87.79,113.60,7.94">A stationary kernel is a function of ğœ = ğ‘¥ -ğ‘¥ â€² , i.e., it is invariant to translation of the inputs. Standard kernels such as Gaussian kernel and MatÃ©rn kernels are both stationary. In 
                    <ref type="bibr" coords="3,498.09,690.53,13.28,7.94" target="#b28">[29]</ref>, the authors proposed a Spectral Mixture (SM) kernel which supports a broad class of stationary covariances:
                </p>
                <formula xml:id="formula_8" coords="4,102.53,100.50,191.51,20.28">ğ‘˜ (ğœ) = âˆ‘ï¸ ğ‘– ğ›¼ ğ‘– ğ‘’ğ‘¥ğ‘ (-2ğœ‹ 2 ğœ 2 ğ‘– ğœ)ğ‘ğ‘œğ‘  (2ğœ‹ ğœ‡ ğ‘– ğœ),
                    <label>(6)</label>
                </formula>
                <p coords="4,53.47,127.23,32.05,7.94;4,88.77,127.23,67.48,7.94;4,157.26,127.23,26.48,7.94;4,186.99,127.23,107.05,7.94;4,53.80,138.19,113.34,7.94">where ğ›¼ ğ‘– are the weights, ğœ ğ‘– , and ğœ‡ ğ‘– are the variance and mean of mixture Gaussian components.</p>
                <p coords="4,63.76,149.14,231.79,7.94;4,53.80,160.10,219.01,7.94;4,273.91,157.56,2.52,6.25;4,276.71,160.10,17.34,7.94;4,53.40,170.76,37.01,7.93;4,91.50,168.70,5.79,9.76;4,97.16,170.68,4.20,3.24;4,102.94,171.24,192.08,7.94;4,53.80,182.20,240.23,7.94;4,53.80,193.16,9.41,7.94">A non-stationary kernel, on the other hand, can infer the relations in a input-dependent manner, i.e., it is a function of ğ‘¥, ğ‘¥ â€² , and ğœ = (ğ‘¥ -ğ‘¥ â€² ) ğ‘‡ . A classic extension of the stationary SM kernel (i.e., Eq.6) to a non-stationary Spectral Mixture kernel is in the format of:</p>
                <formula xml:id="formula_9" coords="4,66.18,203.90,205.53,20.28">ğ‘˜ (ğ‘¥, ğ‘¥ â€² , ğœ) = âˆ‘ï¸ ğ‘– ğ›¼ 2 ğ‘– ğ‘’ğ‘¥ğ‘ (-2ğœ‹ 2 ğœ ğ‘‡ Î£ ğ‘– ğœ)Î¨ ğœ‡ ğ‘– ,ğœ‡ â€² ğ‘– (ğ‘¥) ğ‘‡ Î¨ ğœ‡ ğ‘– ,ğœ‡ â€² ğ‘– (ğ‘¥ â€² ),</formula>
                <p coords="4,66.53,229.10,22.80,7.94">where</p>
                <formula xml:id="formula_10" coords="4,66.67,241.75,135.40,22.65">Î¨ ğœ‡ ğ‘– ğœ‡ â€² ğ‘– (ğ‘¥) = ğ‘ğ‘œğ‘  (2ğœ‹ ğœ‡ ğ‘– ğ‘¥) + ğ‘ğ‘œğ‘  (2ğœ‹ ğœ‡ â€² ğ‘– ğ‘¥) ğ‘ ğ‘–ğ‘›(2ğœ‹ ğœ‡ ğ‘– ğ‘¥) + ğ‘ ğ‘–ğ‘›(2ğœ‹ ğœ‡ â€² ğ‘– ğ‘¥)</formula>
                <p coords="4,207.95,251.82,1.97,4.02">,</p>
                <formula xml:id="formula_11" coords="4,66.71,243.74,224.16,46.46">Î£ ğ‘– = ğœ 2 ğ‘– ğœŒ ğ‘– ğœ ğ‘– ğœ â€² ğ‘– ğœŒ ğ‘– ğœ ğ‘– ğœ â€² ğ‘– ğœ â€²2 ğ‘– . (
                    <label>7</label>
                </formula>
                <formula xml:id="formula_12" coords="4,290.87,243.74,3.17,7.94">)</formula>
                <p coords="4,53.57,298.10,7.03,5.34;4,63.85,296.16,166.44,7.94;4,231.31,295.68,12.21,8.43;4,243.49,298.10,10.71,5.34;4,254.98,293.62,2.52,6.25;4,254.18,302.06,2.23,3.24;4,257.78,295.68,18.15,8.43;4,275.82,298.10,10.98,5.34;4,287.81,293.62,2.52,6.25;4,286.69,302.06,2.23,3.24;4,290.61,295.68,2.99,7.70;4,53.80,307.12,240.24,7.94;4,53.80,318.08,94.42,7.94">ğ›¼ ğ‘– is the mixture weight for each component. ğœŒ ğ‘– , (ğœ‡ ğ‘– , ğœ‡ â€² ğ‘– ) , (ğœ ğ‘– , ğœ â€² ğ‘– ) are the correlation, means and variances of the bivariate Gaussian components, respectively.</p>
                <p coords="4,63.76,329.04,230.48,7.94;4,53.80,340.00,240.40,7.94;4,53.80,350.96,67.22,7.94">By using a mixture of bi-variate Gaussian components, Eq.7 intrinsically is a closed-form solution to the generalized Fourier inverse transform:</p>
                <formula xml:id="formula_13" coords="4,94.42,361.70,199.62,20.74">ğ‘˜ (ğ‘¥, ğ‘¥ â€² ) = âˆ« ğ‘… âˆ« ğ‘… ğ‘’ 2ğœ‹ğ‘– (ğ‘¥ğœ”-ğ‘¥ â€² ğœ” â€² ) ğœ‡ ğœ” (ğ‘‘ğœ”, ğ‘‘ğœ” â€² ),
                    <label>(8)</label>
                </formula>
                <p coords="4,53.47,388.15,34.23,7.94;4,91.03,388.15,150.69,7.94;4,256.71,388.15,37.33,7.94;4,53.80,398.62,165.92,8.43;4,220.73,396.57,73.32,10.49;4,53.80,410.07,241.75,7.94;4,53.80,420.78,141.91,8.19;4,196.72,418.48,2.52,6.25;4,199.52,421.03,62.14,7.94;4,53.47,431.98,242.08,7.94;4,53.80,442.94,70.71,7.94;4,140.11,442.94,3.34,7.94">where ğœ‡ ğœ” is a positive bounded symmetric measure 
                    <ref type="bibr" coords="4,243.95,388.15,10.53,7.94" target="#b8">[9]</ref> associated to some PSD spectral density function ğ‘† (ğœ”, ğœ” â€² ), which is denoted as the spectral surface. When the spectral measure mass is concentrated only on the diagonal (i.e., ğœ” = ğœ” â€² ), its closed-form would reduce to a stationary one, e.g., Eq.6. For the detailed description, please refer to 
                    <ref type="bibr" coords="4,126.75,442.94,13.36,7.94" target="#b22">[23]</ref>.
                </p>
                <p coords="4,63.76,453.90,231.79,7.94;4,53.80,464.86,241.75,7.94;4,53.80,475.82,240.24,7.94;4,53.80,486.78,241.75,7.94;4,53.80,497.74,240.24,7.94;4,53.80,508.70,241.75,7.94;4,53.80,519.66,240.24,7.94;4,53.80,530.61,241.62,7.94">Our proposed method is on the basis of the non-stationary Spectral Mixture kernel. In the next section, we first present the mathematical rationale of introducing such a non-stationary kernel into the attention mechanism. Then, in the context of machine learning, by decomposing the kernel function, we explain why it can count for the time-dependent periodical and long-range dependency. Finally, a new attention neural structure is devised. In its implementation, several constraints would be added for robustness.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4" coords="4,53.80,551.03,101.98,9.37;4,53.80,566.72,220.56,9.37;4,78.58,579.67,92.74,9.37">METHODOLOGY 4.1 Attention with Temporal Embedding in Kernel Perspective</head>
                <p coords="4,53.80,593.88,133.17,9.38;4,186.99,596.30,15.69,5.34;4,203.43,593.88,92.13,8.43;4,53.80,605.86,27.34,7.94;4,82.82,605.37,35.73,8.43;4,120.03,605.37,96.14,8.58;4,215.81,605.29,3.91,3.24;4,220.81,605.86,74.75,7.94;4,53.80,616.82,241.75,7.94;4,53.80,627.77,26.15,7.94;4,81.42,627.29,126.07,8.43;4,209.18,627.29,84.86,8.43;4,53.80,638.73,40.72,7.94;4,94.22,642.77,2.23,3.24;4,99.96,638.49,19.27,7.68;4,121.77,638.25,18.52,7.76;4,141.30,638.73,152.74,7.94;4,53.80,647.65,240.24,10.09;4,53.80,660.76,79.00,7.94;4,136.05,660.76,30.64,7.94;4,168.38,660.27,35.05,8.43;4,204.90,660.27,11.17,7.70;4,53.80,593.88,133.17,9.38;4,186.99,596.30,15.69,5.34;4,203.43,593.88,92.13,8.43;4,53.80,605.86,27.34,7.94;4,82.82,605.37,35.73,8.43;4,120.03,605.37,96.14,8.58;4,215.81,605.29,3.91,3.24;4,220.81,605.86,74.75,7.94;4,53.80,616.82,241.75,7.94;4,53.80,627.77,26.15,7.94;4,81.42,627.29,126.07,8.43;4,209.18,627.29,84.86,8.43;4,53.80,638.73,40.72,7.94;4,94.22,642.77,2.23,3.24;4,99.96,638.49,19.27,7.68;4,121.77,638.25,18.52,7.76;4,141.30,638.73,152.74,7.94;4,53.80,649.80,136.89,7.94;4,196.81,649.80,97.23,7.94;4,53.80,660.76,79.00,7.94;4,136.05,660.76,30.64,7.94;4,168.38,660.27,35.05,8.43;4,204.90,660.27,11.17,7.70">Given a sequence S = {(ğ‘¡ 1 , ğ‘’ 1 ), ..., (ğ‘¡ ğ‘› , ğ‘’ ğ‘› )} and its mapping functions ğœ™ ğ‘¡ (ğ‘¡) and ğœ™ ğ‘’ (ğ‘’) that encodes ğ‘¡, ğ‘’ into R ğ‘‘ , conventional attention related methods usually add or concatenate the event embedding ğœ™ ğ‘’ (ğ‘’) and the temporal embedding ğœ™ ğ‘¡ (ğ‘¡). We use the additive form (i.e., x ğ‘– = ğœ™ ğ‘¡,ğ‘– + ğœ™ ğ‘’,ğ‘– ) as the case to illustrate the attention with temporal embedding in a kernel view
                    <ref type="foot" coords="4,192.93,647.65,3.38,6.44" target="#foot_0">foot_0</ref> . To simplify notations, we omit the variable ğ‘¡ ğ‘– , ğ‘’ ğ‘– in the ğœ™ ğ‘¡ (â€¢) and ğœ™ ğ‘’ (â€¢).
                </p>
                <p coords="4,327.92,87.79,230.27,7.94;4,317.73,98.75,241.97,7.94;4,317.73,109.71,240.46,7.94;4,317.96,120.67,240.24,7.94;4,317.96,131.63,240.23,7.94;4,317.96,142.59,241.22,7.94;4,317.96,153.55,240.24,7.94;4,317.96,164.51,240.24,7.94;4,317.96,174.98,48.06,9.38;4,365.72,179.50,4.17,3.24;4,370.87,174.98,5.88,7.70">As stated in Eq.2, the output of the attention module can be viewed as a superposition of relative contributions from each keyvalue pair to each query. To be more specific, we would consider the attention weights which depend on the similarity measure between the query and key, and the value vector separately. The following mathematical illustration applies to general attention mechanisms, but out of simplicity, we use the self-attention case to demonstrate the derivation where the query, key and value are all the same as X = {x 1 , ..., x ğ‘› }.</p>
                <p coords="4,327.92,186.42,230.44,7.94;4,317.96,195.89,6.56,5.99;4,324.07,200.11,8.18,5.96;4,335.57,198.18,222.85,7.94;4,317.96,209.14,240.23,7.94;4,317.96,220.10,11.95,7.94">Following Eq.2 and omitting the constant normalizing factor âˆšï¸ ğ‘‘ ğ‘˜ for notation simplicity, the attention weights, with explicitly expressing the temporal and event embedding, can be reformatted as :</p>
                <formula xml:id="formula_14" coords="4,329.43,246.89,228.77,67.94">ğ‘˜ ğ‘’ğ‘¥ğ‘ (x ğ‘– , x ğ‘— ) = ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– + ğœ™ ğ‘’,ğ‘– , ğœ™ ğ‘¡,ğ‘— + ğœ™ ğ‘’,ğ‘— ) = ğ‘’ğ‘¥ğ‘ ((ğœ™ ğ‘¡,ğ‘– + ğœ™ ğ‘’,ğ‘– )(ğœ™ ğ‘¡,ğ‘— + ğœ™ ğ‘’,ğ‘— ) ğ‘‡ ) = ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– ğœ™ ğ‘‡ ğ‘¡,ğ‘— + ğœ™ ğ‘’,ğ‘– ğœ™ ğ‘‡ ğ‘’,ğ‘— + ğœ™ ğ‘¡,ğ‘– ğœ™ ğ‘‡ ğ‘’,ğ‘— + ğœ™ ğ‘’,ğ‘– ğœ™ ğ‘‡ ğ‘¡,ğ‘— ) = ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– , ğœ™ ğ‘¡,ğ‘— ) â€¢ ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘’,ğ‘– , ğœ™ ğ‘’,ğ‘— ) â€¢ ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– , ğœ™ ğ‘’,ğ‘— ) â€¢ ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘— , ğœ™ ğ‘’,ğ‘– ),
                    <label>(9)</label>
                </formula>
                <p coords="4,317.62,338.51,107.41,7.94;4,426.50,338.03,7.84,7.70;4,434.50,340.45,23.33,5.34;4,459.04,338.03,35.39,8.43;4,495.90,338.03,7.85,7.70;4,503.90,340.45,24.24,5.34;4,529.35,338.03,29.08,8.43;4,317.96,348.99,207.80,8.43;4,525.78,351.41,12.07,5.34;4,538.86,348.99,19.34,8.43;4,318.40,359.95,5.78,7.70;4,325.08,362.37,12.87,5.34;4,339.17,359.95,164.12,8.43;4,504.76,359.95,7.84,7.70;4,512.75,362.37,23.79,5.34;4,537.75,359.95,20.45,8.43;4,317.60,373.32,16.07,5.34;4,335.14,370.90,7.85,7.70;4,343.13,373.32,23.99,5.34;4,368.13,370.90,190.07,8.43;4,317.96,382.35,82.39,7.94">where the first two terms, ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– , ğœ™ ğ‘¡,ğ‘— ) and ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘’,ğ‘– , ğœ™ ğ‘’,ğ‘— ) clearly represent the temporal and event correlation between (ğ‘¡ ğ‘– , ğ‘’ ğ‘– ) and (ğ‘¡ ğ‘— , ğ‘’ ğ‘— ). With regard to the two cross terms ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– , ğœ™ ğ‘’,ğ‘— ) and ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘— , ğœ™ ğ‘’,ğ‘– ), they represent the correlation between time ğ‘– and event ğ‘— and vice versa.</p>
                <p coords="4,327.92,393.53,230.27,7.70;4,317.96,404.26,241.76,7.94;4,317.96,415.22,154.92,7.94;4,473.90,415.22,84.30,7.94;4,317.96,426.18,71.56,7.94;4,417.79,426.18,140.41,7.94;4,317.96,437.14,240.24,7.94;4,317.96,448.10,127.78,7.94">The first two terms indicate that the event and temporal correlation are calculated separately. From Eq.9, since the exponential of a dot-product operation (i.e., ğ‘˜ ğ‘’ğ‘¥ğ‘ ) is noted as an instance of kernel operation 
                    <ref type="bibr" coords="4,391.75,426.18,13.61,7.94" target="#b13">[14,</ref>
                    <ref type="bibr" coords="4,407.59,426.18,10.21,7.94" target="#b27">28]</ref>, the attention weights with temporal embedding can be further considered as the product of temporal kernel and event kernel separately.
                </p>
                <p coords="4,327.92,459.28,230.27,7.70;4,317.96,470.02,240.24,7.94;4,317.96,480.98,240.23,7.94;4,317.96,491.94,240.24,7.94;4,317.96,502.90,240.46,7.94;4,342.10,513.85,216.10,7.94;4,317.95,524.81,240.24,7.94;4,317.96,535.77,213.63,7.94">The two cross terms are a byproduct of summing up the temporal and event embedding. Since the time and event are projected into two separate vector spaces, intuitively the two cross terms have no valid physical meaning. Even worse, they introduce a kind of noise in the attention weights. Several recent related work 
                    <ref type="bibr" coords="4,317.96,513.85,9.23,7.94" target="#b4">[5,</ref>
                    <ref type="bibr" coords="4,328.91,513.85,11.47,7.94" target="#b25">26]</ref> also discussed this issue. They claimed that using the product of separate temporal and non-temporal kernels is better than using the direct sum of temporal and non-temporal embeddings.
                </p>
                <p coords="4,327.92,546.73,230.51,7.94;4,317.96,557.69,240.24,7.94;4,317.96,568.65,240.24,7.94;4,317.96,579.61,165.58,7.94;4,485.01,579.12,7.84,7.70;4,493.01,581.54,23.33,5.34;4,517.55,579.12,5.41,7.70">Since we focus on improving the representation learning ability of the non-stationary temporal correlation, in accordance with Eq.9, in the next subsection, we introduce a non-stationary kernel function to replace the exponential term ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– , ğœ™ ğ‘¡,ğ‘— ).</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.2" coords="4,317.96,608.09,196.80,9.37;4,342.74,621.04,82.64,9.37">Encode Temporal Correlation with Non-stationarity</head>
                <p coords="4,317.96,635.74,241.75,7.94;4,317.96,646.70,240.24,7.94;4,317.60,659.59,16.07,5.34;4,335.14,657.17,7.85,7.70;4,343.13,659.59,23.33,5.34;4,367.67,657.17,192.04,8.43;4,317.95,668.62,240.24,7.94;4,317.96,679.58,240.40,7.94;4,317.96,690.53,241.75,7.94;4,317.96,701.49,240.24,7.94;5,53.53,87.79,94.22,7.94;5,163.46,87.79,3.37,7.94">In this section, we introduce a kernel function to encode temporal correlation with non-stationarity. Note that, the original ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– , ğœ™ ğ‘¡,ğ‘— ) function can be regarded as exponential of a linear kernel. As has been discussed in section 3.3, it suffers from encoding the non-stationary temporal correlation. In order to infer non-stationary long-range and periodic correlations in an inputdependent manner, we utilize the Generalized Spectral Mixture (GSM) kernel proposed in 
                    <ref type="bibr" coords="5,149.99,87.79,13.47,7.94" target="#b21">[22]</ref>:
                </p>
                <formula xml:id="formula_15" coords="5,61.66,98.76,232.39,36.14">ğ‘˜ ğºğ‘†ğ‘€ (ğ‘¥, ğ‘¥ â€² ) = âˆ‘ï¸ ğ‘– ğ›¼ ğ‘– (ğ‘¥)ğ›¼ ğ‘– (ğ‘¥ â€² )ğ‘˜ ğºğ‘–ğ‘ğ‘ğ‘ ,ğ‘– (ğ‘¥, ğ‘¥ â€² )ğ‘ğ‘œğ‘  (2ğœ‹ (ğœ‡ ğ‘– (ğ‘¥)ğ‘¥ -ğœ‡ ğ‘– (ğ‘¥ â€² )ğ‘¥ â€² )),
                    <label>(10)</label>
                </formula>
                <p coords="5,53.47,140.90,22.80,7.94">where</p>
                <formula xml:id="formula_16" coords="5,60.92,152.19,194.11,24.49">ğ‘˜ ğºğ‘–ğ‘ğ‘ğ‘ ,ğ‘– (ğ‘¥, ğ‘¥ â€² ) = âˆšï¸„ 2ğ‘™ ğ‘– (ğ‘¥)ğ‘™ ğ‘– (ğ‘¥ â€² ) ğ‘™ ğ‘– (ğ‘¥) 2 + ğ‘™ ğ‘– (ğ‘¥ â€² ) 2 ğ‘’ğ‘¥ğ‘ (- (ğ‘¥ -ğ‘¥ â€² ) 2</formula>
                <p coords="5,211.16,171.34,4.64,5.34;5,217.26,167.92,28.72,8.77;5,247.44,167.53,18.05,9.10;5,267.18,162.40,26.86,8.43;5,53.80,184.48,28.63,7.94;5,83.89,184.00,62.41,8.43;5,147.76,184.00,62.78,8.43;5,212.00,184.00,82.05,8.43;5,53.80,195.44,240.47,7.94;5,53.80,206.40,240.24,7.94;5,53.80,217.36,104.33,7.94">ğ‘™ ğ‘– (ğ‘¥) 2 + ğ‘™ ğ‘– (ğ‘¥ â€² ) 2 ). (11) Mean ğœ‡ ğ‘– (ğ‘¥), lengthscale ğ‘™ ğ‘– (ğ‘¥) and weight ğ›¼ ğ‘– (ğ‘¥) of each component are input-dependent learnable functions. The kernel is essentially a product of three kernels, and its PSD is guaranteed since all of the product kernels are PSD.</p>
                <p coords="5,63.76,228.32,230.27,7.94;5,53.80,239.28,240.24,7.94;5,53.80,249.75,54.14,8.43;5,109.03,247.70,185.01,10.49;5,53.80,261.20,199.47,7.94">Eq.10 is derived from Eq.7, which originally aims to overcome the limit that the non-stationary kernel in Eq.7 vanishes rapidly outside the origin (ğ‘¥, ğ‘¥ â€² ) = (0, 0). When encoding temporal correlation in our problem, this kernel has the following advantages:</p>
                <p coords="5,69.77,273.59,206.98,8.43">â€¢ the cosine term can represent the periodic correlation.</p>
                <p coords="5,69.77,284.55,225.78,8.43;5,78.21,295.99,108.74,7.94;5,206.10,295.99,87.95,7.94;5,78.21,306.95,215.83,7.94;5,78.21,317.91,80.63,7.94;5,69.77,328.38,92.70,8.43;5,163.92,328.38,130.12,8.43;5,78.21,339.83,204.68,7.94;5,53.80,352.70,240.24,7.94;5,53.80,363.66,241.22,7.94;5,53.47,374.62,241.95,7.94">â€¢ the Gibbs' kernel (Eq.11), which is a non-stationary generalization of the Gaussian kernel 
                    <ref type="bibr" coords="5,189.18,295.99,14.69,7.94" target="#b9">[10]</ref> , can encode both global and local correlations, namely the short-and long-range temporal dependency. â€¢ the value of weight ğ›¼ ğ‘– (ğ‘¥) also changes over time, which dynamically adjusts the importance of each component. Since all the three terms in Eq.10 is input-dependent, it can represent non-stationary temporal correlation well. On the basis of this kernel, we next introduce the implementation of a new attention structure.
                </p>
                <p coords="5,102.54,536.20,19.82,5.61;5,105.10,514.60,15.12,5.61;5,97.33,492.58,30.24,5.61;5,101.36,451.98,22.18,5.61;5,80.51,408.79,19.82,5.61;5,174.23,536.20,19.82,5.61;5,176.80,514.60,15.12,5.61;5,169.02,492.58,30.24,5.61;5,224.60,536.38,28.23,4.81;5,235.10,514.60,15.12,5.61;5,227.33,492.58,30.24,5.61;5,224.15,466.85,49.55,4.81;5,237.93,408.79,19.82,5.61;5,100.64,579.57,6.33,4.81;5,226.32,579.57,6.33,4.81">MatMul Scale Mask (opt.) SoftMax MatMul MatMul Scale Mask (opt.) GSMKernel, Scale Mask (opt.) See Eq. 12 for details MatMul (a) (b) </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="4.3" coords="5,53.80,623.84,175.78,9.37">Implementation of the NsTKA</head>
                <p coords="5,53.80,638.53,240.24,7.94;5,53.80,649.49,70.99,7.94;5,129.96,649.49,128.72,7.94;5,260.16,649.01,33.89,9.41;5,53.80,660.45,96.54,7.94;5,151.11,660.45,2.12,7.94">Following the above sections, the framework of NsTKA method is illustrated as Figure 
                    <ref type="figure" coords="5,126.96,649.49,3.01,7.94" target="#fig_2">2</ref>. In the NsTKA, we replace the ğ‘˜ ğ‘’ğ‘¥ğ‘ (q i , k j ) in Eq.2 with a new kernel ğ‘˜ ğ‘š :
                </p>
                <formula xml:id="formula_17" coords="5,106.30,673.56,187.75,9.63">ğ‘˜ ğ‘š (q i , k j ) = ğ‘˜ ğ‘¡ (ğ‘¡ ğ‘ ğ‘– , ğ‘¡ ğ‘˜ ğ‘— ) â€¢ ğ‘˜ ğ‘’ (e ğ‘ ğ‘– , e ğ‘˜ ğ‘— ),
                    <label>(12)</label>
                </formula>
                <p coords="5,53.47,690.21,33.18,7.94;5,88.34,689.72,5.78,7.70;5,94.11,692.14,19.38,7.03;5,115.05,689.72,178.98,8.43;5,53.80,701.49,240.24,7.94;5,317.95,87.79,205.70,7.94;5,525.12,87.31,7.56,7.70;5,532.36,87.79,14.84,8.35;5,546.91,92.45,6.29,4.32;5,554.77,87.31,2.99,7.70;5,317.96,99.08,240.23,7.94;5,317.96,110.04,229.57,7.94;5,549.00,109.56,8.75,7.70;5,317.96,121.00,27.15,7.94">where ğ‘˜ ğ‘¡ (ğ‘¡ ğ‘ ğ‘– , ğ‘¡ ğ‘˜ ğ‘— ) denotes the temporal kernel which takes the continuous time scalar value of the query and key as input and follows the GSM kernel (i.e., Eq.10) expression. And ğ‘˜ ğ‘’ (e ğ‘ ğ‘– , e ğ‘˜ ğ‘— ) denotes the event kernel which takes the event embedding of the query and key as input and remains the same expression as ğ‘˜ ğ‘’ğ‘¥ğ‘ (â€¢) in Eq.2.</p>
                <p coords="5,327.92,131.96,131.50,7.94;5,461.11,131.47,40.40,8.43;5,502.97,131.47,19.09,7.76;5,523.51,131.47,21.66,7.76;5,546.63,131.47,11.12,7.70;5,317.96,142.92,240.24,7.94;5,317.96,153.88,240.23,7.94;5,317.96,164.84,240.23,7.94;5,317.96,175.79,240.24,7.94;5,317.96,186.27,240.25,8.43;5,317.96,197.71,97.42,7.94;5,416.83,197.23,117.01,8.43">For the temporal GSM kernel part ğ‘˜ ğ‘¡ (â€¢), since ğœ‡ ğ‘– (ğ‘¥), ğ‘™ ğ‘– (ğ‘¥), ğ›¼ ğ‘– (ğ‘¥) in Eq.10 are all input-dependent learnable parameters, we use a two-layer fully-connected neural network to jointly optimize them along the training process. Since all the learnable parameters have clear physical meanings, the ReLU activation functions are used to make them non-negative. In addition, a small constant ğœ– = 1ğ‘’ -6 is added to the length scale ğ‘™ ğ‘– (ğ‘¥) layer to ensure it is positive.</p>
                <p coords="5,327.92,208.67,231.80,7.94;5,317.96,219.63,240.24,7.94;5,321.04,230.59,237.15,7.94;5,317.96,241.55,240.24,7.94;5,317.96,252.51,163.59,7.94;5,482.58,252.51,75.62,7.94;5,317.96,263.47,240.24,7.94;5,330.76,274.42,227.43,7.94;5,317.96,285.38,36.39,7.94;5,370.48,285.38,189.23,7.94;5,317.96,296.34,240.24,7.94;5,317.96,307.30,240.57,7.94;5,317.96,318.26,240.23,7.94;5,317.62,329.22,174.58,7.94;5,508.64,329.22,49.56,7.94;5,317.96,340.18,240.23,7.94;5,317.96,351.14,240.23,7.94;5,317.96,362.10,241.75,7.94;5,317.96,373.05,240.24,7.94;5,317.62,384.01,240.57,7.94;5,317.95,394.97,241.62,7.94;5,317.95,405.93,240.24,7.94;5,317.96,416.89,103.05,7.94">Differences between our method and related work. Compared with the conventional attention structure shown in Figure 
                    <ref type="figure" coords="5,317.96,230.59,3.09,7.94" target="#fig_2">2</ref>(a), the first difference is that we use the GSM kernel to calculate the temporal attention weights. The second one is that only the non-temporal input are used as values, i.e., ğ‘‰ ğ‘’ . Although viewed in a kernel way, the implementation of the attention mechanism in 
                    <ref type="bibr" coords="5,317.96,274.42,10.55,7.94" target="#b4">[5]</ref> is still through the mapping to temporal and event embedding spaces. In 
                    <ref type="bibr" coords="5,356.99,285.38,13.49,7.94" target="#b25">[26]</ref>, a similar method is proposed, which uses the direct product of two separate kernels to represent the temporal and non-temporal correlations. The authors experimented with a few standard kernels such as linear, polynomial and Gaussian kernels without further discussing their properties. In 
                    <ref type="bibr" coords="5,495.14,329.22,13.50,7.94" target="#b29">[30]</ref>, the authors discussed the characteristics of temporal correlation starting in a kernel view but transferred that into a series of basis functions to express stationary temporal embedding. Although the aforementioned work has discussed the kernel view of expressing attention weights in a temporal and non-temporal correlation way, there is limited discussion in diving into the design choice of kernels. In the next section, by comparison, we present the competitive performance of our method.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5" coords="5,317.96,437.41,93.69,9.37">EXPERIMENTS</head>
                <p coords="5,317.96,452.10,241.75,7.94;5,317.96,463.06,240.24,7.94;5,317.62,474.02,242.08,7.94;5,317.96,484.98,240.24,7.94;5,317.96,495.94,240.24,7.94;5,317.96,506.90,74.23,7.94">In this section, we evaluate the proposed method on both a synthetic dataset and 3 real-world datasets. A quantitative comparison with baseline models is first presented, followed by a comprehensive ablation experiment that shows the theoretical insights of the proposed method. Our experiments are intended to answer the following questions:</p>
                <p coords="5,333.92,519.62,225.79,8.43;5,342.36,531.06,144.92,7.94;5,333.92,541.53,225.79,8.43;5,342.36,552.98,216.21,7.94;5,333.92,563.45,225.79,8.43;5,342.36,574.89,46.28,7.94">â€¢ Q1: Does the proposed method outperform the state-of-theart temporal event prediction methods? â€¢ Q2: By adjusting the mean, length-scale, and weight functions of the GSM kernel, how do variants of NsTKA perform? â€¢ Q3: How does NsTKA perform under different levels of nonstationarity?</p>
                <p coords="5,317.96,595.41,66.29,9.37">5.1 Datasets</p>
                <p coords="5,317.96,610.04,240.24,8.04;5,317.96,621.06,240.24,7.94;5,317.96,632.02,240.24,7.94;5,317.96,642.98,60.04,7.94">5.1.1 Synthetic Dataset. A synthetic dataset (SynD) with a mixture of multiple non-stationary signals is introduced to study how the proposed method captures the non-stationary pattern. The dataset is generated by :</p>
                <formula xml:id="formula_18" coords="5,369.15,658.95,189.06,24.75">ğ‘¥ (ğ‘¡) = ğ‘ âˆ‘ï¸ ğ‘–=1 ğ´ ğ‘– (ğ‘¡)ğ‘ğ‘œğ‘  (ğœ” ğ‘– (ğ‘¡)ğ‘¡) + ğœ™ ğ‘– ) + ğœ– ğ‘– ,
                    <label>(13)</label>
                </formula>
                <p coords="5,317.62,690.53,33.42,7.94;5,352.50,690.05,21.13,7.76;5,375.09,690.05,184.62,8.43;5,317.96,701.49,140.16,7.94;5,461.21,701.49,96.99,7.94;6,53.80,87.79,192.82,7.94;6,250.55,87.79,43.50,7.94;6,53.80,98.75,240.24,7.94;6,53.80,109.71,123.50,7.94;6,180.54,109.71,113.50,7.94;6,53.80,120.67,240.24,7.94;6,53.80,131.63,55.97,7.94">where ğ´ ğ‘– (ğ‘¡), ğœ” ğ‘– (ğ‘¡) are the amplitude and frequency of each composed sinusoidal signal, respectively, ğœ™ ğ‘– is a random phase between 0 and 2ğœ‹ generated by uniform distribution, and ğœ– ğ‘– is the noise term following normal distribution. It is well known that when the amplitudes and frequencies ğ´ ğ‘– , ğœ” ğ‘– are both constants with regard to time ğ‘¡, the signal is stationary, otherwise the signal would be non-stationary.</p>
                <p coords="6,63.76,142.59,230.51,7.94;6,53.80,153.55,240.23,7.94;6,53.80,164.51,197.31,7.94">To illustrate the capability of dealing with the non-stationary signal of our proposed method, the amplitudes are designed to be linearly increasing or decreasing with respect to time:</p>
                <formula xml:id="formula_19" coords="6,130.53,179.22,163.52,8.43">ğ´ ğ‘– (ğ‘¡) = ğ´ ğ‘–,0 (1 + ğ›¼ ğ‘– ğ‘¡/ğ‘‡ ),
                    <label>(14)</label>
                </formula>
                <p coords="6,53.47,192.06,37.36,7.94;6,93.08,191.82,131.18,8.19;6,227.02,192.06,67.19,7.94;6,53.80,203.02,241.22,7.94;6,53.80,213.98,240.24,7.94;6,53.80,224.94,236.19,7.94">where ğ´ ğ‘–,0 is the initial amplitude when ğ‘¡ = 0, ğ›¼ ğ‘– is the increasing or decreasing coefficient corresponding to positive or negative values, and T is the length of the overall time. Similarly, the frequencies are designed to be exponentially increasing or decreasing with time:</p>
                <formula xml:id="formula_20" coords="6,126.39,239.65,167.66,8.43">ğœ” ğ‘– (ğ‘¡) = ğœ” ğ‘–,0 * ğ‘’ğ‘¥ğ‘ (ğ›½ ğ‘– ğ‘¡/ğ‘‡ ),
                    <label>(15)</label>
                </formula>
                <p coords="6,53.47,255.33,37.93,7.94;6,94.14,255.09,147.82,8.19;6,245.21,255.33,50.35,7.94;6,53.80,266.29,240.24,7.94;6,53.57,277.25,241.97,7.94;6,53.80,288.21,240.24,7.94;6,53.80,299.17,111.31,7.94">where ğœ” ğ‘–,0 is the initial frequency when ğ‘¡ = 0 and ğ›½ ğ‘– is the increasing or decreasing coefficient corresponding to positive or negative values. In the following experiments, we use 5 sinusoidal components with different frequencies and amplitudes and varying factors to represent our SynD dataset.</p>
                <p coords="6,63.76,310.13,230.27,7.94;6,53.80,321.09,241.75,7.94;6,53.57,332.05,241.44,7.94;6,53.47,343.01,242.08,7.94;6,53.80,353.96,241.22,7.94;6,53.80,364.92,240.24,7.94;6,53.80,375.88,240.23,7.94;6,53.80,386.84,240.24,7.94;6,53.80,397.80,240.24,7.94;6,53.80,408.76,92.67,7.94">In addition, to observe how different types of non-stationarities affect the performance of different methods, we combine timevarying amplitude function and time-varying frequency function, which leads to three datasets, i.e., SynD-A (only time-varying amplitude function), SynD-F (only time-varying frequency function), SynD-AF (both time-varying amplitude and frequency function are included). Here the task is designed as one-step prediction given a fixed length of the input sequence. Due to that the synthetic dataset is controllable, it is of significant importance for presenting the insights into our method.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.1.2" coords="6,53.80,425.34,100.09,8.04">Real-world Datasets.</head>
                <p coords="6,157.37,425.41,138.19,7.94;6,53.80,436.37,240.24,7.94;6,53.80,447.32,175.63,7.94">To further demonstrate the effectiveness of the proposed method, we compare different methods on three real-world datasets from various domains:</p>
                <p coords="6,69.77,458.25,225.79,10.09;6,78.21,471.36,185.15,7.94;6,69.77,459.92,168.08,8.43;6,243.98,460.40,51.58,7.94;6,78.21,471.36,185.15,7.94;6,282.08,471.36,13.47,7.94;6,78.21,482.32,215.83,7.94;6,78.21,493.28,215.83,7.94;6,78.21,504.24,215.84,8.89;6,78.21,515.20,217.21,7.94;6,77.94,526.16,216.43,7.94;6,78.21,537.11,22.91,7.94;6,69.77,547.59,60.30,8.43;6,148.68,548.07,146.88,7.94;6,78.21,559.03,215.83,7.94;6,78.21,569.99,215.83,7.94;6,78.21,580.95,215.83,7.94;6,78.21,591.91,215.83,7.94;6,78.21,602.87,215.83,7.94;6,78.21,613.83,215.83,7.94;6,78.21,624.79,35.45,7.94;6,129.26,624.79,3.34,7.94;6,69.77,635.26,125.20,8.41;6,201.08,635.74,94.47,7.94;6,78.21,646.70,215.83,7.94;6,342.36,87.79,215.83,7.94;6,342.36,98.75,215.83,7.94;6,342.36,109.71,172.51,7.94;6,327.92,122.78,233.27,7.94;6,317.96,133.74,240.24,7.94;6,317.96,144.70,7.29,7.94;6,344.45,144.70,13.49,7.94;6,373.54,144.70,3.34,7.94">â€¢ ETT (Electricity Transformer Temperature)
                    <ref type="foot" coords="6,237.85,458.25,3.38,6.44" target="#foot_1">foot_1</ref> is a crucial indicator in the electric power long-term deployment. 
                    <ref type="bibr" coords="6,265.42,471.36,14.60,7.94" target="#b32">[33]</ref> collected and published 2-year data from two separated counties in China. In this paper, we use the 1-hour level data collection in the first site (noted as ğ¸ğ‘‡ğ‘‡â„ 1 ). The data point consists of the target value "oil temperature" and 6 power load features. The task is to predict the oil temperature in the next few hours. â€¢ PM2.5 dataset 
                    <ref type="bibr" coords="6,132.07,548.07,14.60,7.94" target="#b15">[16]</ref> records hourly PM2.5 value and the associated meteorological measurements in Beijing City of China from the year 2010 to 2014. The PM2.5 values are categorized into six levels according to the United States Environmental Protection Agency standard. Our task is to predict the class of the PM2.5 level on the next day at 8 am using the collected data in the previous day, when is the commuter peak period in Beijing 
                    <ref type="bibr" coords="6,115.90,624.79,13.36,7.94" target="#b11">[12]</ref>. â€¢ An industrial dataset (IndD). 
                    <ref type="foot" coords="6,194.97,633.59,3.38,6.44" target="#foot_2">3</ref> We further provide an industrial dataset that records the behavioral data of millions of users. We collected interaction data with timestamps among 3072058 users and 200 services. Our task is to predict the next service the user would visit on a platform. Considering consistency and comparability, for the public datasets ETT and PM2.5, we follow the same prepossessing steps mentioned in 
                    <ref type="bibr" coords="6,327.49,144.70,14.72,7.94" target="#b32">[33]</ref> and 
                    <ref type="bibr" coords="6,360.18,144.70,13.36,7.94" target="#b11">[12]</ref>.
                </p>
                <p coords="6,327.92,155.66,231.79,7.94;6,317.96,166.61,241.75,7.94;6,317.96,177.57,240.24,7.94;6,317.96,188.53,241.62,7.94;6,317.96,199.49,240.24,7.94;6,317.96,210.45,240.24,7.94;6,317.96,221.41,241.22,7.94;6,317.96,232.37,240.24,7.94;6,317.96,243.33,240.23,7.94;6,317.96,254.29,240.24,7.94;6,317.96,265.24,233.35,7.94;6,556.56,265.24,3.02,7.94">For the ETT dataset, we use the first 12-month data as the training set and use the following 4-month and the next following 4month data as the corresponding validation and test set. We use the sequence length of 96 data points to predict the next 24 steps. For the PM2.5 dataset, we use 70% of the samples as the training set and 15% of the samples for validation and the remaining 15 % of the samples as the test dataset. We use the previous 24-hour data (i.e., sequence length 24) to predict the class of PM2.5 value at 8 am on the next day. For the IndD, since it is a large dataset, we use 1% of the samples as the validation set and 1% of the samples as the test set. The overall statistics of the datasets are summarized in Table 
                    <ref type="table" coords="6,553.54,265.24,3.02,7.94" target="#tab_1">1</ref>.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="6,324.01,294.29,21.46,7.94">Name</head>
                <p coords="6,369.83,294.29,19.72,7.94">Label </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.2" coords="6,317.96,403.37,131.19,9.37">Comparison Methods</head>
                <p coords="6,317.69,418.07,240.51,7.94;6,317.96,429.03,240.24,7.94;6,317.96,439.98,131.75,7.94">The baseline methods consist of two parts: (1) a typical RNN related method; and (2) five Transformer and its various variants that are introduced in a progressive manner.</p>
                <p coords="6,333.92,452.57,224.27,8.43;6,342.36,464.01,141.81,7.94;6,327.92,597.63,206.64,7.94;6,541.08,597.63,17.12,7.94;6,317.96,608.58,240.24,7.94;6,317.96,619.54,240.42,7.94;6,317.96,630.50,241.75,7.94;6,317.96,641.46,239.75,8.89;6,317.96,651.94,240.24,8.43;6,317.96,663.38,241.22,7.94;6,317.96,674.34,240.40,7.94;6,317.96,685.30,241.62,7.94">â€¢ TimeLSTM. A conventional LSTM method equipped with time gates for modeling time intervals. We implemented our proposed method based on Keras
                    <ref type="foot" coords="6,534.56,595.48,3.38,6.44" target="#foot_3">foot_3</ref> with the Adam optimizer. For the proposed NsTKA method, we treat the number of mixture Gaussian components ğ‘š as a hyper-parameter and select from {3, 4, 5, 6, 7, 8}. For all compared methods including ours, we consider latent dimensions {8, 16, 32, 64, 128} and ğ‘™ 2 regularizer {0.0001, 0.001, 0.01, 0.1, 1}. All other hyperparameters and initialization strategies (e.g., learning rate, optimizer, dropout, maximum sequence length, maximum time interval, etc.) are similar to the suggested optimal configurations by the methods' authors.
                </p>
                <p coords="7,53.37,87.79,240.67,7.94;7,53.80,98.75,240.24,7.94;7,53.80,109.71,186.64,7.94">We use the validation set to tune hyper-parameters and terminate training if the validation performance has no improvement after 10 epochs. All performance is reported on the test set.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.3" coords="7,53.80,144.33,118.05,9.37">Evaluation Metrics</head>
                <p coords="7,53.80,159.03,240.23,7.94;7,53.80,169.99,240.24,7.94;7,53.80,180.95,240.40,7.94;7,53.53,191.90,240.51,7.94;7,53.80,202.86,240.24,7.94;7,53.80,213.82,240.24,7.94;7,53.80,224.78,240.24,7.94;7,53.80,235.74,62.67,7.94">For the PM2.5 and IndD multi-class prediction tasks, we evaluate the model performance by classification accuracy. For the ETT and SynD regression tasks, we adopt the normalized mean squared error (NMSE) to measure the performance, in which the continuous-value data are normalized to zero-mean and unit deviation that they are comparable. Specifically, for the ETT dataset, since it is a multi-step prediction task, we report the averaged NMSE for the multi-step predicted results.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.4" coords="7,53.80,270.36,175.66,9.37">Performance Comparison (Q1)</head>
                <p coords="7,53.53,285.05,19.48,7.94;7,81.59,285.05,212.44,7.94;7,53.47,296.01,242.09,7.94;7,53.80,306.97,241.75,7.94;7,53.80,317.93,241.62,7.94;7,53.53,328.89,242.02,7.94;7,53.80,339.85,240.24,7.94;7,53.80,350.81,240.24,7.94;7,53.80,361.77,241.75,7.94;7,53.80,372.73,240.24,7.94;7,53.80,383.68,240.24,7.94;7,53.80,394.64,240.47,7.94;7,53.80,405.60,241.22,7.94;7,53.53,416.56,242.02,7.94;7,53.80,427.52,240.23,7.94;7,53.80,438.48,241.75,7.94;7,53.80,449.44,240.24,7.94;7,53.80,460.40,240.23,7.94;7,53.80,471.36,240.24,7.94;7,53.80,482.31,241.62,7.94;7,53.37,493.27,242.18,7.94;7,53.80,504.23,240.24,7.94;7,53.80,515.19,240.23,7.94;7,53.80,526.15,240.47,7.94;7,53.80,537.11,54.70,7.94">Table 
                    <ref type="table" coords="7,75.25,285.05,4.10,7.94" target="#tab_2">2</ref> shows the experimental results. We can see that, compared with RNN-like methods, Transformer variations show a significant improvement. Through introducing PE for explicitly modeling position information, we observed a further improvement. This demonstrates that temporal correlation among events is critical in forecasting the following event. Replacing PE with ME, the continuous time-span information is taken into consideration so that the fine-grained temporal patterns can be captured. The proposed method, Transformer+NsTKA, outperforms competitors by a large margin on both the synthetic datasets and real-world datasets in most cases. Specifically, on different types of non-stationary datasets and a number of temporal datasets on different domains, Transformer+NsTKA consistently shows the best or the secondbest performance, which indicates that the proposed method can capture non-stationary patterns under complex settings. Furthermore, on two of the synthetic datasets, Transformer + PE has a better performance than ours. It might due to the reason that the classic PE takes the format of predefined sine and cosine functions and our synthetic dataset is also composed of sinusoidal waves. With proper initialization, the Transformer + PE method can capture the dominating temporal patterns well. Nevertheless, in terms of the representation capability in a much wider range, the above results have well verified the necessity of encoding non-stationary characteristics.
                </p>
                <p coords="7,63.76,548.07,231.26,7.94;7,53.47,559.03,115.93,7.94;7,175.54,559.03,40.61,7.94;7,222.31,559.03,71.72,7.94;7,53.80,569.99,240.24,7.94;7,53.80,580.95,241.75,7.94;7,53.80,591.90,240.24,7.94;7,53.80,602.86,240.24,7.94;7,53.80,613.82,240.24,7.94;7,53.80,624.78,241.23,7.94;7,53.80,635.74,241.62,7.94;7,53.80,646.70,241.75,7.94;7,53.80,657.66,240.24,7.94;7,53.80,668.62,240.24,7.94;7,53.80,679.58,240.41,7.94;7,53.80,690.53,240.23,7.94;7,53.80,701.49,183.52,7.94">To qualitatively verify the effectiveness of the proposed method, we plot fitted curves in Figure 
                    <ref type="figure" coords="7,172.41,559.03,3.13,7.94" target="#fig_4">3</ref>. In Figure 
                    <ref type="figure" coords="7,219.18,559.03,3.13,7.94" target="#fig_4">3</ref>, we compared the prediction on one of the synthetic signal between our proposed method and the one of Transformer + ME, which is a typical stationary time-encoding representation. It is readily observed that the proposed method can precisely fit the curve the best. Since these baseline methods are designed for stationary datasets, the models would try to fit the targeted curve with a stationary basis, resulting in a misalignment both on the frequency and amplitude. Specifically, we can see that curves fitted by these baseline methods cannot align well within valleys and peaks because valleys and peaks do not occur in a fixed period. A similar phenomenon can also be observed on the fitting of the amplitudes. This further demonstrates that the model capacity of these baseline methods is not good enough to fit the non-stationary pattern. 
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.5" coords="7,317.96,289.23,121.91,9.37">Ablation Study (Q2)</head>
                <p coords="7,317.64,303.93,240.55,7.94;7,317.62,314.89,240.57,7.94;7,317.96,325.85,241.62,7.94;7,317.69,336.81,240.50,7.94;7,317.96,347.76,240.23,7.94;7,317.96,358.72,241.75,7.94;7,317.96,369.68,240.24,7.94;7,317.96,380.64,240.23,7.94;7,317.73,393.54,7.03,5.34;7,326.22,391.12,16.03,7.70;7,342.36,395.64,2.23,3.24;7,346.05,389.06,212.14,10.49;7,317.96,402.56,240.24,7.94;7,317.96,413.52,240.23,7.94;7,317.96,424.48,228.90,7.94;7,317.96,441.23,240.24,8.04;7,317.96,452.25,241.75,7.94;7,317.96,463.21,240.24,7.94;7,317.96,474.17,55.62,7.94;7,378.87,474.17,179.32,7.94;7,317.96,485.13,240.24,7.94;7,317.96,496.09,240.23,7.94;7,317.96,507.05,240.47,7.94;7,317.96,518.01,233.31,7.94;7,472.70,534.83,85.50,7.94;7,317.96,545.78,203.85,7.94;7,522.82,545.78,8.46,7.94;7,534.54,545.78,22.65,7.94;7,317.96,556.74,233.73,7.94">As mentioned in section 4.2, when the mean, length-scale and weight functions of the GSM kernel reduce to input-independent constants, the GSM kernel would reduce to a stationary SM kernel. To show the effectiveness of encoding non-stationarity through our proposed method, we firstly modify NsTKA by replacing the non-stationary kernel with a stationary kernel, while other components keep unchanged. Furthermore, the GSM kernel can also be considered as the product of three kernels, i.e., the linear kernel ğ›¼ ğ‘– (ğ‘¥)ğ›¼ ğ‘– (ğ‘¥ â€² ), the Gibbs kernel, and the cosine kernel, which are all dependent on the absolute time. By reducing each component into an ğ‘¥-independent term or completely removing the component, we analyze how each component contributes to the improvement. 5.5.1 Effectiveness of Non-stationary Kernel. First, we only replace the non-stationary kernel with a stationary one to verify the necessity of capturing the non-stationary pattern. From the results shown in Table 
                    <ref type="table" coords="7,375.82,474.17,3.05,7.94" target="#tab_3">3</ref>, we can see that using a stationary kernel would lead to performance drop on all the datasets. This suggests that the non-stationarity is served as an important factor in the temporal event prediction, and our non-stationary kernel can successfully take advantage of this information to improve the performance. 
                    <ref type="table" coords="7,466.40,534.83,4.09,7.94" target="#tab_4">4</ref> shows the experimental results with different combinations of kernels, where ğœ‡ ğ‘– , ğ‘™ ğ‘– and ğ›¼ ğ‘– denote these kernels are independent of ğ‘¥. We can observe that:
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="5.5.2" coords="7,317.96,534.76,146.22,8.04">Effectiveness of Each Kernel. Table</head>
                <p coords="7,333.92,569.50,89.56,8.43;7,424.95,569.50,134.77,8.43;7,342.36,580.95,217.34,7.94;7,342.36,591.90,215.83,7.94;7,342.36,602.86,217.34,7.94;7,342.36,613.82,13.74,7.94;7,333.92,624.30,224.27,8.43;7,342.36,635.74,181.10,7.94;7,526.82,635.74,31.39,7.94;7,342.36,646.70,215.83,7.94;7,342.36,657.66,17.87,7.94;7,361.69,657.17,36.02,8.43;7,399.17,657.17,159.02,8.43;7,342.36,668.62,215.83,7.94;7,342.03,679.58,160.80,7.94;8,317.66,560.92,240.86,7.70;8,317.96,571.88,240.23,7.70;8,317.96,582.84,88.31,7.70">â€¢ by singly removing ğ›¼ ğ‘– (ğ‘¥) which makes each mixture component equally weighted, a largest performance drop is observed, which suggests that different components serve as different roles in the model and they require careful weighting. â€¢ removing ğ‘¥ from the input of the three kernels all leads to the degradation of performance except for the ğœ‡ ğ‘– case. On one hand, it suggests that ğ‘¥ is of great importance on both the ğ‘™ ğ‘– (ğ‘¥) and ğ›¼ ğ‘– (ğ‘¥) parts of the kernel, which represents the short-and long-range temporal dependency and component weights separately as described in section 4. Table 5: Mean squared error of methods Transformer+NsTKA and Transformer+ME when fitting the signals with different non-stationary levels.</p>
                <p coords="8,327.92,596.59,230.27,7.94;8,325.13,607.55,233.06,7.94;8,317.96,618.51,92.03,8.97;8,411.40,618.51,80.56,7.94">While keeping all the other parameters in Eq. 13 the same, Table 
                    <ref type="table" coords="8,317.96,607.55,4.25,7.94">5</ref> presents the relative gain between NsTKA and ME under the different values of ğ‘ ğ›¼ ğ‘– , ğ‘ ğ›½ ğ‘– . We can observe that:
                </p>
                <p coords="8,333.92,635.26,224.27,8.43;8,342.03,646.70,217.67,7.94;8,342.36,657.66,215.83,7.94;8,342.03,668.62,216.16,7.94;8,342.36,679.58,55.97,7.94;8,333.92,690.05,225.79,8.43;8,342.36,701.49,215.83,7.94;9,78.21,87.79,217.35,7.94;9,78.21,98.75,215.83,7.94;9,78.21,109.71,212.24,7.94">â€¢ For a stationary dataset, NsTKA shows a similar performance with the related method, ME, which is derived from a stationary kernel. It suggests that NsTKA can also fit the dataset well on a degenerated setting, even though the kernel is non-stationary. â€¢ For non-stationary datasets, as the increasing level of nonstationarity, NsTKA shows a more and more significant improvement comparing with the baseline on both decaying scenarios, which demonstrates the powerful ability of NsTKA to delineate the non-stationary temporal patterns.</p>
                <p coords="9,63.76,125.20,231.79,7.94;9,53.80,136.16,240.24,7.94;9,53.80,147.12,66.39,7.94">In summary, NsTKA is proved to be effective on both stationary and non-stationary datasets, which can be widely applied to different domains.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head n="6" coords="9,53.80,170.01,87.77,9.37">CONCLUSION</head>
                <p coords="9,53.80,184.71,241.75,7.94;9,53.80,195.66,240.47,7.94;9,53.80,206.62,240.47,7.94;9,53.80,217.58,240.24,7.94;9,53.80,228.54,241.75,7.94;9,53.80,239.50,240.24,7.94;9,53.80,250.46,240.24,7.94;9,53.80,261.42,241.75,7.94;9,53.80,272.38,240.24,7.94;9,53.80,283.34,240.24,7.94;9,53.80,294.29,240.24,7.94;9,53.80,305.25,241.75,7.94;9,53.80,316.21,195.95,7.94">In this paper, by introducing the Generalized Spectral Mixture Kernel, and integrating it to the attention module, we mathematically reveal its representation capability in terms of the non-stationary temporal-correlation. Then, a new attention structure is devised for input sequences of neural networks. Finally, exhaustive experiments are conducted to present its competitive performance against related work, followed by an ablation experiment which shows its effectiveness on both stationary and non-stationary temporal correlations. We would further improve the robustness and performance of the method in our future work. Furthermore, we plan to extend our method to the reinforcement learning domain, as the ability to predict future off-policy performance in non-stationary environments is common and critical in real-world scenarios.</p>
            </div>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,53.80,202.85,241.84,7.70;2,53.80,213.81,233.24,7.70">
                <head>Figure 1 :</head>
                <label>1</label>
                <figDesc>Figure 1: Illustration of a discrete mixed periodic signal, including two stationary cases and one non-stationary case.</figDesc>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,53.80,595.67,241.84,7.70;5,53.80,606.63,192.32,7.70">
                <head>Figure 2 :</head>
                <label>2</label>
                <figDesc>Figure 2: Implementation of NsTKA. (a) The original scaledot-product attention; (b) Our proposed NsTKA</figDesc>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="6,486.41,464.01,14.72,7.94;6,333.92,474.49,225.27,8.43;6,342.36,485.93,187.95,7.94;6,333.92,496.40,232.40,8.43;6,342.36,507.85,163.59,7.94;6,333.92,518.32,224.28,8.43;6,342.36,529.76,74.08,7.94;6,333.92,540.24,224.27,8.43;6,342.36,551.68,91.04,7.94;6,333.92,562.16,225.78,8.43;6,342.36,573.60,48.36,7.94;6,333.92,584.07,181.73,8.43">
                <figDesc>
                    <ref type="bibr" coords="6,486.41,464.01,14.72,7.94" target="#b33">[34]</ref> â€¢ RNN+Attention. A multi-time attention network that learns embeddings of irregularly sampled time series. [25] â€¢ TiSASRec. A state-of-the-art time interval aware self-attention method for sequential recommendation. [15] â€¢ Transformer w/o PE. The classic Transformer without positional encoding. â€¢ Transformer+PE. The classic Transformer with sinusoidal positional encoding [27]. â€¢ Transformer+ME. The Transformer with mercer time embedding [30]. â€¢ Transformer+NsTKA. Our proposed method.
                </figDesc>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="7,317.96,261.26,241.38,7.70;7,317.66,272.22,193.04,7.70;7,322.06,83.68,232.03,169.87">
                <head>Figure 3 :</head>
                <label>3</label>
                <figDesc>Figure 3: Fitted curves given by different methods. Upper: Transformer+NsTKA. Lower: Transformer+ME.</figDesc>
                <graphic coords="7,322.06,83.68,232.03,169.87" type="bitmap"/>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,324.01,288.81,228.83,103.39">
                <head>Table 1 :</head>
                <label>1</label>
                <figDesc>Statistics of datasets.</figDesc>
                <table coords="6,324.01,288.81,228.83,89.17">
                    <row>
                        <cell/>
                        <cell>type</cell>
                        <cell>Sequence length</cell>
                        <cell>Prediction length</cell>
                        <cell>Sample numbers</cell>
                    </row>
                    <row>
                        <cell>SynD</cell>
                        <cell>Continuous value</cell>
                        <cell>100</cell>
                        <cell>1</cell>
                        <cell>2,924</cell>
                    </row>
                    <row>
                        <cell>ETT</cell>
                        <cell>Continuous value</cell>
                        <cell>96</cell>
                        <cell>24</cell>
                        <cell>8,640</cell>
                    </row>
                    <row>
                        <cell>PM2.5</cell>
                        <cell>Multi-class</cell>
                        <cell>24</cell>
                        <cell>1</cell>
                        <cell>1,215</cell>
                    </row>
                    <row>
                        <cell>IndD</cell>
                        <cell>Multi-class</cell>
                        <cell>300</cell>
                        <cell>1</cell>
                        <cell>14,049,979</cell>
                    </row>
                </table>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="7,342.36,679.58,217.34,29.86">
                <head>Table 2 :</head>
                <label>2</label>
                <figDesc>Results on one synthetic dataset and three real-world datasets. Bold values denotes the best results.</figDesc>
                <table coords="8,60.46,86.13,397.24,166.58">
                    <row>
                        <cell/>
                        <cell/>
                        <cell/>
                        <cell/>
                        <cell cols="2">Datasets</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell/>
                        <cell/>
                        <cell cols="2">NMSE</cell>
                        <cell>Top-1 Accuracy</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Methods</cell>
                        <cell cols="3">SynD-A SynD-F SynD-AF</cell>
                        <cell>ETT</cell>
                        <cell>PM2.5</cell>
                        <cell>IndD</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>TimeLSTM</cell>
                        <cell>0.0144</cell>
                        <cell>0.0482</cell>
                        <cell>0.0961</cell>
                        <cell>0.1218 0.4827 0.4207</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>RNN+Attention</cell>
                        <cell>0.0092</cell>
                        <cell>0.0534</cell>
                        <cell>0.0439</cell>
                        <cell>0.0521 0.4789 0.4008</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>TiSASRec</cell>
                        <cell>0.0056</cell>
                        <cell>0.1089</cell>
                        <cell>0.0410</cell>
                        <cell>0.0455 0.5057 0.3423</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Transformer w/o PE</cell>
                        <cell>0.0133</cell>
                        <cell>0.0634</cell>
                        <cell>0.0619</cell>
                        <cell>0.0451 0.5172 0.3765</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Transformer+PE</cell>
                        <cell>0.0056</cell>
                        <cell>0.0212</cell>
                        <cell>0.0266</cell>
                        <cell>0.0460 0.5211 0.4993</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>Transformer+ME</cell>
                        <cell>0.0140</cell>
                        <cell>0.0629</cell>
                        <cell>0.0680</cell>
                        <cell>0.0438 0.5326 0.5195</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell cols="2">Transformer+NsTKA 0.0054</cell>
                        <cell>0.0392</cell>
                        <cell>0.0377</cell>
                        <cell>0.0410 0.5460 0.5285</cell>
                    </row>
                    <row>
                        <cell>ğ‘€ğ‘’ğ‘¡â„ğ‘œğ‘‘ğ‘ </cell>
                        <cell cols="3">IndD Top-1 Accuracy SynD NMSE</cell>
                        <cell/>
                    </row>
                    <row>
                        <cell cols="2">Transformer + NsTKA 0.5285</cell>
                        <cell>0.0377</cell>
                        <cell/>
                        <cell/>
                    </row>
                    <row>
                        <cell>Transformer + STKA</cell>
                        <cell>0.5212</cell>
                        <cell>0.0665</cell>
                        <cell/>
                        <cell/>
                    </row>
                </table>
                <note coords="7,502.83,679.58,56.88,7.94;7,342.36,690.53,215.03,7.94;7,342.36,701.49,216.06,7.94">
                    <p>2. And the temporal patterns regarding these components with regard to ğ‘¥ are successfully extracted as well. Meanwhile, the slightly</p>
                </note>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="8,53.50,259.22,242.05,257.69">
                <head>Table 3 :</head>
                <label>3</label>
                <figDesc>Ablation study of the non-stationary part of the NsTKA.</figDesc>
                <table coords="8,121.33,435.44,107.24,81.47">
                    <row>
                        <cell/>
                        <cell>0.0377</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>0.0377</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>0.0418</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>0.0834</cell>
                    </row>
                    <row>
                        <cell>ğœ‡ ğ‘–</cell>
                        <cell>0.0355</cell>
                    </row>
                    <row>
                        <cell>ğ‘™ ğ‘–</cell>
                        <cell>0.0532</cell>
                    </row>
                    <row>
                        <cell>ğ›¼ ğ‘–</cell>
                        <cell>0.0680</cell>
                    </row>
                </table>
                <note coords="8,78.21,286.49,91.80,7.94;8,173.23,286.49,120.81,7.94;8,78.21,297.44,217.35,7.94;8,78.21,308.40,215.83,7.94;8,78.21,319.36,216.06,7.94;8,78.21,330.32,215.83,7.94;8,78.21,341.28,217.34,7.94;8,78.21,352.24,215.83,7.94;8,77.88,363.20,216.16,7.94;8,78.21,374.16,217.35,7.94;8,78.21,385.12,215.83,7.94;8,78.21,396.08,172.28,7.94;8,119.09,420.60,6.25,5.34;8,126.81,418.18,25.76,7.76;8,154.03,418.18,28.34,7.76;8,183.82,418.18,45.06,8.43">
                    <p>contrary results for the ğœ‡ ğ‘– case do not necessarily mean that the frequency components in the cosine term is lack of timedependency, it could be that well capturing the time-varying pattern of the periodical correlation of the signal is relatively difficult during training. The improvement of representation capability of our proposed NsTKA method might be accompanied with certain decrease in robustness when dealing with certain datasets, which could be improved with more careful hyperparameter selection or more available data samples. To further improve the robustness and performance of the model is also a future topic of our research.</p>
                    <p>ğœ‡ ğ‘– (ğ‘¥) ğ‘™ ğ‘– (ğ‘¥) ğ›¼ ğ‘– (ğ‘¥) NMSE</p>
                </note>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="8,53.40,375.70,504.80,333.73">
                <head>Table 4 :</head>
                <label>4</label>
                <figDesc>Results on the dataset SynD with different combinations of kernels.To study how the proposed method performs on different scenarios, we create synthetic datasets with different levels of non-stationarity. Specifically, the coefficients ğ›¼ ğ‘– , ğ›½ ğ‘– in Eq.14 and Eq.15 are changing with different speed with respect to time. Namely, two speedcoefficients ğ‘ ğ›¼ ğ‘– , ğ‘ ğ›½ ğ‘– are further introduced. As a result, the new amplitude and frequency functions are ğ´ ğ‘– (ğ‘¡) = ğ´ ğ‘–,0 (1 + ğ‘ ğ›¼ ğ‘– ğ›¼ ğ‘– ğ‘¡/ğ‘‡ ) and ğœ” ğ‘– (ğ‘¡) = ğœ” ğ‘–,0 * ğ‘’ğ‘¥ğ‘ (ğ‘ ğ›½ ğ‘– ğ›½ ğ‘– ğ‘¡/ğ‘‡ ), respectively. One thing to note is that, when the coefficients change in a negligible magnitude (i.e., ğ‘ ğ›¼ ğ‘– = 0, ğ‘ ğ›½ ğ‘– = 0), the synthetic signal is actually stationary. Intuitively, Figure4illustrated as an example of signals with different values of ğ‘ ğ›¼ ğ‘– , ğ‘ ğ›½ ğ‘– . Compared with the stationary signal, the amplitude and frequency of other two ones change over time to different degrees.Figure 4: An example of signals with different values of ğ‘ ğ›¼ ğ‘– , ğ‘ ğ›½ ğ‘– . ğ‘ ğ›¼ ğ‘– ğ‘ ğ›½ ğ‘–</figDesc>
                <table coords="8,53.80,412.37,497.77,152.23">
                    <row>
                        <cell/>
                        <cell/>
                        <cell/>
                        <cell>Transformer +NsTKA</cell>
                        <cell>Transformer +ME</cell>
                        <cell>gain</cell>
                    </row>
                    <row>
                        <cell cols="2">Stationary 0</cell>
                        <cell>0</cell>
                        <cell>0.032</cell>
                        <cell>0.0352</cell>
                        <cell>-9.0%</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>1</cell>
                        <cell>0.1</cell>
                        <cell>0.0568</cell>
                        <cell>0.1283</cell>
                        <cell>-55.7%</cell>
                    </row>
                    <row>
                        <cell>Frequency fluctuates</cell>
                        <cell>1 1 1</cell>
                        <cell>0.5 1 1.1</cell>
                        <cell>0.0558 0.0490 0.0431</cell>
                        <cell>0.1053 0.1046 0.0926</cell>
                        <cell>-47.0% -53.2% -53.5%</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell>1</cell>
                        <cell>1.5</cell>
                        <cell>0.0388</cell>
                        <cell>0.1347</cell>
                        <cell>-71.2%</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell cols="2">0.1 1</cell>
                        <cell>0.0433</cell>
                        <cell>0.1188</cell>
                        <cell>-63.6%</cell>
                    </row>
                    <row>
                        <cell>Amplitude</cell>
                        <cell cols="2">0.5 1</cell>
                        <cell>0.0400</cell>
                        <cell>0.1257</cell>
                        <cell>-68.2%</cell>
                    </row>
                    <row>
                        <cell>fluctuates</cell>
                        <cell cols="2">1.1 1</cell>
                        <cell>0.0412</cell>
                        <cell>0.1345</cell>
                        <cell>-69.4%</cell>
                    </row>
                    <row>
                        <cell/>
                        <cell cols="2">1.5 1</cell>
                        <cell>0.0325</cell>
                        <cell>0.1475</cell>
                        <cell>-78.0%</cell>
                    </row>
                    <row>
                        <cell>5.6 Case Study (Q3)</cell>
                        <cell/>
                        <cell/>
                        <cell/>
                        <cell/>
                        <cell/>
                    </row>
                </table>
            </figure>
            <note
                xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0" coords="4,56.72,678.45,237.33,6.18;4,53.51,685.83,240.54,6.77;4,53.80,694.57,240.25,6.18;4,53.80,702.79,37.03,6.18">
                <p>Note that, since the concatenating form does not introduce the cross-terms ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘– , ğœ™ ğ‘’,ğ‘— ), ğ‘˜ ğ‘’ğ‘¥ğ‘ (ğœ™ ğ‘¡,ğ‘— , ğœ™ ğ‘’,ğ‘– ), obviously it is consistent with our conclusion that the event and temporal correlations can be calculated separately. We omit its derivation in this paper.</p>
            </note>
            <note
                xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1" coords="6,56.84,662.25,116.40,6.18">
                <p>https://github.com/zhouhaoyi/ETDataset</p>
            </note>
            <note
                xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2" coords="6,56.84,670.66,237.21,6.18;6,53.80,678.63,240.25,6.18;6,53.80,686.60,240.25,6.18;6,53.80,694.57,240.25,6.18;6,53.80,702.79,105.68,6.18">
                <p>Note: The data set does not contain any Personal Identifiable Information (PII); The data set is desensitized and encrypted; Adequate data protection was carried out during the experiment to prevent the risk of data copy leakage, and the data set was destroyed after the experiment; The data set is only used for academic research, it does not represent any real business situation.</p>
            </note>
            <note
                xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3" coords="6,320.88,702.79,109.78,6.18">
                <p>https://github.com/alipay/nstka-kdd22</p>
            </note>
        </body>
        <back>
            <div type="references">
                <listBibl>
                    <biblStruct coords="9,69.23,352.11,225.89,6.18;9,69.23,360.02,224.81,6.25;9,69.23,367.99,223.14,6.25" xml:id="b0">
                        <analytic>
                            <title level="a" type="main" coords="9,69.23,360.08,136.26,6.18">Patient subtyping via time-aware LSTM networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Inci</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Baytas</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Cao</forename>
                                    <surname>Xiao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Xi</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Fei</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Anil</forename>
                                    <forename type="middle">K</forename>
                                    <surname>Jain</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jiayu</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,217.22,360.02,76.83,6.25;9,69.23,367.99,200.51,6.25">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
                            <meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="65" to="74"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,376.02,224.82,6.18;9,69.23,383.93,224.81,6.25;9,69.23,391.90,80.14,6.25" xml:id="b1">
                        <analytic>
                            <title level="a" type="main" coords="9,238.92,376.02,55.13,6.18;9,69.23,383.99,137.32,6.18">Learning long-term dependencies with gradient descent is difficult</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yoshua</forename>
                                    <surname>Bengio</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Patrice</forename>
                                    <forename type="middle">Y</forename>
                                    <surname>Simard</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Paolo</forename>
                                    <surname>Frasconi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="9,214.21,383.93,79.84,6.25;9,69.23,391.90,24.75,6.25">IEEE transactions on neural networks</title>
                            <imprint>
                                <biblScope unit="volume">5</biblScope>
                                <biblScope unit="issue">2</biblScope>
                                <biblScope unit="page" from="157" to="166"/>
                                <date type="published" when="1994">1994. 1994</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,399.93,225.58,6.18;9,69.23,407.90,224.81,6.18;9,69.23,415.87,225.89,6.18;9,69.23,423.78,105.14,6.25" xml:id="b2">
                        <monogr>
                            <title level="m" type="main" coords="9,246.57,407.90,47.47,6.18;9,69.23,415.87,223.18,6.18">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Kyunghyun</forename>
                                    <surname>Cho</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Bart</forename>
                                    <surname>Van MerriÃ«nboer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Caglar</forename>
                                    <surname>Gulcehre</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Dzmitry</forename>
                                    <surname>Bahdanau</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Fethi</forename>
                                    <surname>Bougares</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Holger</forename>
                                    <surname>Schwenk</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yoshua</forename>
                                    <surname>Bengio</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1406.1078</idno>
                            <imprint>
                                <date type="published" when="2014">2014. 2014</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct coords="9,69.23,431.81,225.58,6.18;9,69.12,439.78,226.00,6.18;9,69.23,447.75,224.81,6.18;9,69.03,455.66,122.32,6.25" xml:id="b3">
                        <monogr>
                            <title level="m" type="main" coords="9,85.02,447.75,209.03,6.18;9,69.03,455.72,36.97,6.18">Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Krzysztof</forename>
                                    <surname>Choromanski</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Valerii</forename>
                                    <surname>Likhosherstov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">David</forename>
                                    <surname>Dohan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Xingyou</forename>
                                    <surname>Song</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jared</forename>
                                    <surname>Davis</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">TamÃ¡s</forename>
                                    <surname>SarlÃ³s</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">David</forename>
                                    <surname>Belanger</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Lucy</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Colwell</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Adrian</forename>
                                    <surname>Weller</surname>
                                </persName>
                            </author>
                            <idno>CoRR abs/2006.03555</idno>
                            <imprint>
                                <date type="published" when="2020">2020. 2020</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,463.69,224.81,6.18;9,69.23,471.66,224.81,6.18;9,69.23,479.57,224.82,6.25;9,69.23,487.54,225.57,6.25;9,69.23,495.52,96.72,6.25" xml:id="b4">
                        <analytic>
                            <title level="a" type="main" coords="9,147.80,471.66,146.25,6.18;9,69.23,479.63,65.89,6.18">Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Zihang</forename>
                                    <surname>Dai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Zhilin</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yiming</forename>
                                    <surname>Yang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jaime</forename>
                                    <forename type="middle">G</forename>
                                    <surname>Carbonell</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Quoc</forename>
                                    <surname>Viet Le</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Ruslan</forename>
                                    <surname>Salakhutdinov</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,147.54,479.57,146.51,6.25;9,69.23,487.54,112.54,6.25">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
                            <meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019
                                <address>
                                    <addrLine>Florence, Italy</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
                                <biblScope unit="volume">1</biblScope>
                                <biblScope unit="page" from="2978" to="2988"/>
                            </imprint>
                        </monogr>
                        <note>Long Papers</note>
                    </biblStruct>
                    <biblStruct coords="9,69.23,503.54,224.81,6.18;9,68.98,511.46,218.89,6.25" xml:id="b5">
                        <analytic>
                            <title level="a" type="main" coords="9,191.18,503.54,102.87,6.18;9,68.98,511.51,81.26,6.18">Bayesian dynamic financial networks with time-varying predictors</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Daniele</forename>
                                    <surname>Durante</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">David</forename>
                                    <forename type="middle">B</forename>
                                    <surname>Dunson</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="9,155.50,511.46,83.50,6.25">Statistics &amp; Probability Letters</title>
                            <imprint>
                                <biblScope unit="volume">93</biblScope>
                                <biblScope unit="page" from="19" to="26"/>
                                <date type="published" when="2014">2014. 2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,519.48,224.82,6.18;9,69.23,527.45,224.99,6.18;9,69.23,535.37,197.22,6.25" xml:id="b6">
                        <analytic>
                            <title level="a" type="main" coords="9,217.61,519.48,76.44,6.18;9,69.23,527.45,224.99,6.18;9,69.23,535.42,18.99,6.18">Modeling the Dynamics of User Preferences for Sequence-Aware Recommendation Using Hidden Markov Models</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Farzad</forename>
                                    <surname>Eskandanian</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Bamshad</forename>
                                    <surname>Mobasher</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,100.73,535.37,136.27,6.25">The Thirty-Second International Flairs Conference</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="425" to="430"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,543.39,225.89,6.18;9,69.23,551.31,224.81,6.25;9,69.23,559.28,225.58,6.25;9,69.23,567.25,81.67,6.25" xml:id="b7">
                        <analytic>
                            <title level="a" type="main" coords="9,86.34,551.36,131.69,6.18">Convolutional Sequence to Sequence Learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jonas</forename>
                                    <surname>Gehring</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Michael</forename>
                                    <surname>Auli</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">David</forename>
                                    <surname>Grangier</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Denis</forename>
                                    <surname>Yarats</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yann</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Dauphin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,230.43,551.31,63.62,6.25;9,69.23,559.28,156.13,6.25">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
                            <meeting>the 34th International Conference on Machine Learning, ICML 2017
                                <address>
                                    <addrLine>Sydney, NSW, Australia</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2017-06-11">2017. 6-11 August 2017</date>
                                <biblScope unit="page" from="1243" to="1252"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,575.27,224.81,6.18;9,69.23,583.19,205.28,6.25" xml:id="b8">
                        <analytic>
                            <title level="a" type="main" coords="9,140.29,575.27,153.76,6.18;9,69.23,583.24,31.17,6.18">Classes of kernels for machine learning: a statistics perspective</title>
                            <author>
                                <persName coords="">
                                    <surname>Marc G Genton</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="9,105.75,583.19,102.43,6.25">Journal of machine learning research</title>
                            <imprint>
                                <biblScope unit="volume">2</biblScope>
                                <biblScope unit="page" from="299" to="312"/>
                                <date type="published" when="2001-12">2001. Dec (2001</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,591.16,225.89,6.25;9,69.23,599.18,127.95,6.18" xml:id="b9">
                        <monogr>
                            <title level="m" type="main" coords="9,128.71,591.16,163.96,6.25">Bayesian Gaussian processes for regression and classification</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Mark</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Gibbs</surname>
                                </persName>
                            </author>
                            <imprint>
                                <date type="published" when="1997">1997</date>
                            </imprint>
                            <respStmt>
                                <orgName>University of Cambridge</orgName>
                            </respStmt>
                        </monogr>
                        <note type="report_type">Ph. D. Dissertation</note>
                    </biblStruct>
                    <biblStruct coords="9,69.23,607.10,224.81,6.25;9,69.23,615.07,99.63,6.25" xml:id="b10">
                        <analytic>
                            <title level="a" type="main" coords="9,202.49,607.15,67.85,6.18">Long short-term memory</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Sepp</forename>
                                    <surname>Hochreiter</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">JÃ¼rgen</forename>
                                    <surname>Schmidhuber</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="9,275.61,607.10,18.43,6.25;9,69.23,615.07,34.66,6.25">Neural computation</title>
                            <imprint>
                                <biblScope unit="volume">9</biblScope>
                                <biblScope unit="issue">8</biblScope>
                                <biblScope unit="page" from="1735" to="1780"/>
                                <date type="published" when="1997">1997. 1997</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,623.09,224.81,6.18;9,69.23,631.06,224.82,6.18;9,69.23,638.98,224.81,6.25;9,69.23,646.95,217.27,6.25" xml:id="b11">
                        <analytic>
                            <title level="a" type="main" coords="9,261.51,623.09,32.54,6.18;9,69.23,631.06,224.82,6.18;9,69.23,639.03,164.56,6.18">Explainable Multivariate Time Series Classification: A Deep Neural Network Which Learns to Attend to Important Variables As Well As Time Intervals</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Tsung-Yu</forename>
                                    <surname>Hsieh</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Suhang</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yiwei</forename>
                                    <surname>Sun</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Vasant</forename>
                                    <surname>Honavar</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,245.51,638.98,48.54,6.25;9,69.23,646.95,187.64,6.25">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
                            <meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
                            <imprint>
                                <date type="published" when="2021">2021</date>
                                <biblScope unit="page" from="607" to="615"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,654.97,224.82,6.18;9,69.23,662.94,225.99,6.18;9,69.23,670.86,224.81,6.25;9,69.23,678.83,75.57,6.25" xml:id="b12">
                        <analytic>
                            <title level="a" type="main" coords="9,233.32,654.97,60.73,6.18;9,69.23,662.94,225.99,6.18;9,69.23,670.91,15.45,6.18">When people change their mind: Off-policy evaluation in non-stationary recommendation environments</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Rolf</forename>
                                    <surname>Jagerman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Ilya</forename>
                                    <surname>Markov</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Maarten</forename>
                                    <surname>De Rijke</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,97.03,670.86,197.02,6.25;9,69.23,678.83,45.94,6.25">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
                            <meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="447" to="455"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,69.23,686.85,225.89,6.18;9,69.23,694.82,225.89,6.18;9,69.23,702.74,207.00,6.25" xml:id="b13">
                        <analytic>
                            <title level="a" type="main" coords="9,86.13,694.82,206.38,6.18">Sequential recommendation with relation-aware kernelized self-attention</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Mingi</forename>
                                    <surname>Ji</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Weonyoung</forename>
                                    <surname>Joo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Kyungwoo</forename>
                                    <surname>Song</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yoon-Yeong</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Il-Chul</forename>
                                    <surname>Moon</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,76.83,702.74,163.83,6.25">Proceedings of the AAAI conference on artificial intelligence</title>
                            <meeting>the AAAI conference on artificial intelligence</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="4304" to="4311"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,89.10,224.81,6.18;9,333.39,97.01,224.81,6.25;9,333.39,104.98,225.58,6.25;9,333.39,112.95,78.94,6.25" xml:id="b14">
                        <analytic>
                            <title level="a" type="main" coords="9,496.76,89.10,61.45,6.18;9,333.39,97.07,134.92,6.18">Time Interval Aware Self-Attention for Sequential Recommendation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jiacheng</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yujie</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Julian</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Mcauley</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,481.98,97.01,76.23,6.25;9,333.39,104.98,170.57,6.25">WSDM '20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
                            <meeting>
                                <address>
                                    <addrLine>Houston, TX, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2020-02-03">2020. February 3-7, 2020</date>
                                <biblScope unit="page" from="322" to="330"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,120.98,225.59,6.18;9,333.39,128.95,224.95,6.18;9,333.39,136.86,225.51,6.25;9,333.39,144.83,182.74,6.25" xml:id="b15">
                        <analytic>
                            <title level="a" type="main" coords="9,404.59,128.95,153.75,6.18;9,333.39,136.92,92.34,6.18">Assessing Beijing's PM2. 5 pollution: severity, weather impact, APEC and winter heating</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Xuan</forename>
                                    <surname>Liang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Tao</forename>
                                    <surname>Zou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Bin</forename>
                                    <surname>Guo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shuo</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Haozhe</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shuyi</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Hui</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Song</forename>
                                    <surname>Xi</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Chen</forename>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="9,430.97,136.86,127.94,6.25;9,333.39,144.83,105.23,6.25">Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
                            <imprint>
                                <biblScope unit="volume">471</biblScope>
                                <biblScope unit="page">20150257</biblScope>
                                <date type="published" when="2015">2015. 2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,152.86,225.89,6.18;9,333.39,160.77,224.82,6.25;9,333.39,168.74,83.58,6.25" xml:id="b16">
                        <analytic>
                            <title level="a" type="main" coords="9,352.43,160.83,91.75,6.18">Hybrid time Bayesian networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Manxia</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Arjen</forename>
                                    <surname>Hommersom</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Maarten</forename>
                                    <surname>Van Der Heijden</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Peter Jf</forename>
                                    <surname>Lucas</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="9,451.60,160.77,106.60,6.25;9,333.39,168.74,28.11,6.25">International Journal of Approximate Reasoning</title>
                            <imprint>
                                <biblScope unit="volume">80</biblScope>
                                <biblScope unit="page" from="460" to="474"/>
                                <date type="published" when="2017">2017. 2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,176.77,224.81,6.18;9,333.39,184.74,224.81,6.18;9,333.39,192.65,224.82,6.25;9,333.39,200.62,93.18,6.25" xml:id="b17">
                        <analytic>
                            <title level="a" type="main" coords="9,452.48,184.74,105.73,6.18;9,333.39,192.71,121.96,6.18">Stable, fast and accurate: Kernelized attention with relative positional encoding</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shengjie</forename>
                                    <surname>Luo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shanda</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Tianle</forename>
                                    <surname>Cai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Di</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Dinglan</forename>
                                    <surname>Peng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shuxin</forename>
                                    <surname>Zheng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Guolin</forename>
                                    <surname>Ke</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Liwei</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Tie-Yan</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,467.93,192.65,90.27,6.25;9,333.39,200.62,50.71,6.25">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2021">2021</date>
                                <biblScope unit="page" from="22795" to="22807"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,208.65,224.81,6.18;9,333.39,216.62,224.81,6.18;9,333.39,224.59,224.81,6.18;9,333.39,232.50,224.97,6.25;9,333.18,240.53,45.22,6.18" xml:id="b18">
                        <analytic>
                            <title level="a" type="main" coords="9,376.57,224.59,181.64,6.18;9,333.39,232.56,119.02,6.18">A Dynamic Bayesian Network model for long-term simulation of clinical complications in type 1 diabetes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Simone</forename>
                                    <surname>Marini</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Emanuele</forename>
                                    <surname>Trifoglio</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Nicola</forename>
                                    <surname>Barbarini</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Francesco</forename>
                                    <surname>Sambo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Barbara</forename>
                                    <surname>Di Camillo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Alberto</forename>
                                    <surname>Malovini</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Marco</forename>
                                    <surname>Manfrini</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Claudio</forename>
                                    <surname>Cobelli</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Riccardo</forename>
                                    <surname>Bellazzi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="9,457.65,232.50,92.37,6.25">Journal of biomedical informatics</title>
                            <imprint>
                                <biblScope unit="volume">57</biblScope>
                                <biblScope unit="page" from="369" to="376"/>
                                <date type="published" when="2015">2015. 2015</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,248.50,225.00,6.18;9,333.39,256.41,224.81,6.25;9,333.39,264.38,224.81,6.25;9,333.39,272.35,166.13,6.25" xml:id="b19">
                        <analytic>
                            <title level="a" type="main" coords="9,444.49,248.50,113.90,6.18;9,333.39,256.47,122.78,6.18">The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Hongyuan</forename>
                                    <surname>Mei</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jason</forename>
                                    <surname>Eisner</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,468.46,256.41,89.74,6.25;9,333.39,264.38,224.81,6.25;9,333.39,272.35,11.13,6.25">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
                            <meeting>
                                <address>
                                    <addrLine>Long Beach, CA, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2017-09">2017. December 4-9, 2017</date>
                                <biblScope unit="page" from="6754" to="6764"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,280.38,226.00,6.18;9,333.39,288.29,224.81,6.25;9,333.39,296.26,67.16,6.25" xml:id="b20">
                        <monogr>
                            <author>
                                <persName coords="">
                                    <forename type="first">Subhabrata</forename>
                                    <surname>Mukherjee</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Hemank</forename>
                                    <surname>Lamba</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Gerhard</forename>
                                    <surname>Weikum</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1705.02519</idno>
                            <title level="m" coords="9,533.39,280.38,26.00,6.18;9,333.39,288.35,177.54,6.18">Item recommendation with evolving user preferences and experience</title>
                            <imprint>
                                <date type="published" when="2017">2017. 2017</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct coords="9,333.39,304.29,224.81,6.18;9,333.39,312.20,224.81,6.25;9,333.39,320.17,86.69,6.25" xml:id="b21">
                        <analytic>
                            <title level="a" type="main" coords="9,492.06,304.29,66.14,6.18;9,333.39,312.26,19.26,6.18">Non-stationary spectral kernels</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Sami</forename>
                                    <surname>Remes</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Markus</forename>
                                    <surname>Heinonen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Samuel</forename>
                                    <surname>Kaski</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,364.66,312.20,193.55,6.25;9,333.39,320.17,50.71,6.25">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
                            <meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="4645" to="4654"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,328.20,225.89,6.18;9,333.39,336.11,108.18,6.25" xml:id="b22">
                        <monogr>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yves-Laurent Kom</forename>
                                    <surname>Samo</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Stephen</forename>
                                    <surname>Roberts</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1506.02236</idno>
                            <title level="m" coords="9,479.28,328.20,77.27,6.18">Generalized spectral kernels</title>
                            <imprint>
                                <date type="published" when="2015">2015. 2015</date>
                            </imprint>
                        </monogr>
                        <note type="report_type">arXiv preprint</note>
                    </biblStruct>
                    <biblStruct coords="9,333.39,344.14,224.81,6.18;9,333.39,352.05,224.82,6.25;9,333.39,360.02,224.82,6.25;9,333.39,367.99,225.58,6.25;9,333.39,375.96,93.88,6.25" xml:id="b23">
                        <analytic>
                            <title level="a" type="main" coords="9,501.78,344.14,56.42,6.18;9,333.39,352.11,95.94,6.18">Self-Attention with Relative Position Representations</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Peter</forename>
                                    <surname>Shaw</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jakob</forename>
                                    <surname>Uszkoreit</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Ashish</forename>
                                    <surname>Vaswani</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,442.09,352.05,116.12,6.25;9,333.39,360.02,224.82,6.25;9,333.39,367.99,61.04,6.25">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
                            <meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
                                <address>
                                    <addrLine>New Orleans, Louisiana, USA</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <publisher>NAACL-HLT</publisher>
                                <date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
                                <biblScope unit="volume">2</biblScope>
                                <biblScope unit="page" from="464" to="468"/>
                            </imprint>
                        </monogr>
                        <note>Short Papers</note>
                    </biblStruct>
                    <biblStruct coords="9,333.39,383.99,224.81,6.18;9,333.39,391.90,224.81,6.25;9,333.39,399.87,207.64,6.25" xml:id="b24">
                        <analytic>
                            <title level="a" type="main" coords="9,495.12,383.99,63.09,6.18;9,333.39,391.96,127.49,6.18">Multi-Time Attention Networks for Irregularly Sampled Time Series</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Narayan</forename>
                                    <surname>Satya</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Benjamin</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Shukla</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Marlin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,472.65,391.90,85.55,6.25;9,333.39,399.87,99.37,6.25">9th International Conference on Learning Representations, ICLR 2021</title>
                            <meeting>
                                <address>
                                    <addrLine>Virtual Event, Austria</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,407.90,224.81,6.18;9,333.39,415.87,224.82,6.18;9,333.39,423.78,224.82,6.25;9,333.39,431.75,224.81,6.25;9,333.39,439.72,224.81,6.25;9,333.39,447.69,158.28,6.25" xml:id="b25">
                        <analytic>
                            <title level="a" type="main" coords="9,414.25,415.87,143.96,6.18;9,333.39,423.84,146.48,6.18">Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yao-Hung Hubert</forename>
                                    <surname>Tsai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shaojie</forename>
                                    <surname>Bai</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Makoto</forename>
                                    <surname>Yamada</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Louis-Philippe</forename>
                                    <surname>Morency</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Ruslan</forename>
                                    <surname>Salakhutdinov</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,492.70,423.78,65.51,6.25;9,333.39,431.75,224.81,6.25;9,333.39,439.72,224.81,6.25;9,333.39,447.69,11.13,6.25">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
                            <meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019
                                <address>
                                    <addrLine>Hong Kong, China</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
                                <biblScope unit="page" from="4343" to="4352"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,455.72,225.58,6.18;9,333.15,463.69,225.06,6.18;9,333.39,471.60,212.44,6.25" xml:id="b26">
                        <analytic>
                            <title level="a" type="main" coords="9,513.37,463.69,44.84,6.18;9,333.39,471.66,24.64,6.18">Attention is all you need</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Ashish</forename>
                                    <surname>Vaswani</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Noam</forename>
                                    <surname>Shazeer</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Niki</forename>
                                    <surname>Parmar</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jakob</forename>
                                    <surname>Uszkoreit</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Llion</forename>
                                    <surname>Jones</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Aidan</forename>
                                    <forename type="middle">N</forename>
                                    <surname>Gomez</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Åukasz</forename>
                                    <surname>Kaiser</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Illia</forename>
                                    <surname>Polosukhin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,370.40,471.60,139.53,6.25">Advances in neural information processing systems</title>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="5998" to="6008"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,479.57,224.81,6.25;9,333.39,487.54,158.24,6.25" xml:id="b27">
                        <monogr>
                            <title level="m" type="main" coords="9,505.80,479.57,52.40,6.25;9,333.39,487.54,56.61,6.25">Gaussian processes for machine learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Christopher</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Carl</forename>
                                    <forename type="middle">Edward</forename>
                                    <surname>Williams</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Rasmussen</surname>
                                </persName>
                            </author>
                            <imprint>
                                <date type="published" when="2006">2006</date>
                                <publisher>MIT press</publisher>
                                <biblScope unit="volume">2</biblScope>
                                <pubPlace>Cambridge, MA</pubPlace>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,495.57,224.81,6.18;9,333.39,503.49,225.89,6.25;9,333.23,511.51,31.30,6.18" xml:id="b28">
                        <analytic>
                            <title level="a" type="main" coords="9,451.91,495.57,106.30,6.18;9,333.39,503.54,80.89,6.18">Gaussian process kernels for pattern discovery and extrapolation</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Andrew</forename>
                                    <surname>Wilson</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Ryan</forename>
                                    <surname>Adams</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,427.85,503.49,128.65,6.25">International conference on machine learning</title>
                            <imprint>
                                <date type="published" when="2013">2013</date>
                                <biblScope unit="page" from="1067" to="1075"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,519.48,225.89,6.18;9,333.39,527.40,224.81,6.25;9,333.39,535.37,148.48,6.25" xml:id="b29">
                        <analytic>
                            <title level="a" type="main" coords="9,350.14,527.45,163.15,6.18">Self-attention with functional time representation learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Da</forename>
                                    <surname>Xu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Chuanwei</forename>
                                    <surname>Ruan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Sushant</forename>
                                    <surname>Kumar</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Evren</forename>
                                    <surname>Korpeoglu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Kannan</forename>
                                    <surname>Achan</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,525.26,527.40,32.95,6.25;9,333.39,535.37,106.01,6.25">Advances in Neural Information Processing Systems</title>
                            <imprint>
                                <date type="published" when="2019">2019</date>
                                <biblScope unit="page" from="15915" to="15925"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,543.39,224.81,6.18;9,333.39,551.31,224.81,6.25;9,333.39,559.28,193.60,6.25" xml:id="b30">
                        <analytic>
                            <title level="a" type="main" coords="9,396.62,551.36,110.61,6.18">Do RNN and LSTM have Long Memory?</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jingyu</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Feiqing</forename>
                                    <surname>Huang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jia</forename>
                                    <surname>Lv</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yanjie</forename>
                                    <surname>Duan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Zhen</forename>
                                    <surname>Qin</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Guodong</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Guangjian</forename>
                                    <surname>Tian</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,519.77,551.31,38.43,6.25;9,333.39,559.28,151.15,6.25">Proceedings of the 37th International Conference on Machine Learning</title>
                            <meeting>the 37th International Conference on Machine Learning</meeting>
                            <imprint>
                                <date type="published" when="2020">2020</date>
                                <biblScope unit="page" from="11365" to="11375"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,567.30,224.81,6.18;9,333.39,575.27,224.81,6.18;9,333.18,583.19,225.03,6.25;9,333.39,591.16,225.58,6.25;9,333.39,599.13,46.96,6.25" xml:id="b31">
                        <analytic>
                            <title level="a" type="main" coords="9,381.49,575.27,176.71,6.18;9,333.18,583.24,46.03,6.18">SEISMIC: A Self-Exciting Point Process Model for Predicting Tweet Popularity</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Qingyuan</forename>
                                    <surname>Zhao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Murat</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Erdogdu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Hera</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Anand</forename>
                                    <surname>He</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jure</forename>
                                    <surname>Rajaraman</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <surname>Leskovec</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,390.27,583.19,167.94,6.25;9,333.39,591.16,114.81,6.25">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
                            <meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
                                <address>
                                    <addrLine>Sydney, NSW, Australia</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2015-08-10">2015. August 10-13, 2015</date>
                                <biblScope unit="page" from="1513" to="1522"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,607.15,225.58,6.18;9,333.39,615.12,225.99,6.18;9,333.39,623.04,155.89,6.25" xml:id="b32">
                        <analytic>
                            <title level="a" type="main" coords="9,410.29,615.12,149.09,6.18;9,333.39,623.09,85.34,6.18">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Haoyi</forename>
                                    <surname>Zhou</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shanghang</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jieqi</forename>
                                    <surname>Peng</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Shuai</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Jianxin</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Hui</forename>
                                    <surname>Xiong</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Wancai</forename>
                                    <surname>Zhang</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,430.83,623.04,54.97,6.25">Proceedings of AAAI</title>
                            <meeting>AAAI</meeting>
                            <imprint>
                                <date type="published" when="2021">2021</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="9,333.39,631.06,225.89,6.18;9,333.39,638.98,224.81,6.25;9,333.39,646.95,224.82,6.25;9,333.39,654.92,163.44,6.25" xml:id="b33">
                        <analytic>
                            <title level="a" type="main" coords="9,350.14,639.03,163.73,6.18">What to Do Next: Modeling User Behaviors by Time-LSTM</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yu</forename>
                                    <surname>Zhu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Hao</forename>
                                    <surname>Li</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Yikang</forename>
                                    <surname>Liao</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Beidou</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Ziyu</forename>
                                    <surname>Guan</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Haifeng</forename>
                                    <surname>Liu</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Deng</forename>
                                    <surname>Cai</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="9,526.59,638.98,31.61,6.25;9,333.39,646.95,224.82,6.25;9,333.39,654.92,11.13,6.25">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017</title>
                            <meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017
                                <address>
                                    <addrLine>Melbourne, Australia</addrLine>
                                </address>
                            </meeting>
                            <imprint>
                                <date type="published" when="2017-08-19">2017. August 19-25, 2017</date>
                                <biblScope unit="page" from="3602" to="3608"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>