1484
IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 66, NO. 5, MAY 2019
TABLE III
PERFORMANCE COMPARISON FOR JOINT INTERPOLATION/IMPUTATION ALGORITHMS
TABLE IV
SOURCE OF GAIN OF M-RNN
(PERFORMANCE DEGRADATION FROM ORIGINAL M-RNN)
B. Source of Gains
As illustrated in Fig. 2, our M-RNN consists of an Interpo-
lation block and an Imputation block. To understand where the
gains of our approach come from, we compare the performance
of that is achieved when we use only the Interpolation block or
only the Imputation block; the results are shown in Table IV.
The Interpolation block is intended to exploit the correlations
within each data stream and the Imputation block is intended to
exploit the correlations across streams, so it is to be expected
that the largest gains of our M-RNN method should come from
the Interpolation block for the datasets (MIMIC-III and Deteri-
oration) which are frequently sampled and have large temporal
correlations, and should come from the Imputation block for the
datasets (UNOS-Heart and UNOS-Lung) which are infrequently
sampled but have many data streams. As shown in Table IV,
these intuitions are indeed supported by the experiments.
C. Additional Experiments
The experiments we have described above demonstrate that
our method signiﬁcantly outperforms a wide variety of bench-
marks for the imputation of missing data on ﬁve somewhat
representative datasets. However it is natural to ask how our
method would compare in other circumstances. To get some
understanding of this, we conducted four sets of experiments
based on the MIMIC-III dataset: increasing the amount of miss-
ing data, reducing the number of data streams, reducing the
number of samples, and reducing the number of measurements
per patient. Within each set of experiments, we conducted 10
trials for each value of the parameter being studied (e.g., amount
of missing data), and we report the average over these 10 trials.
The results are described below and in Fig. 4. Although the re-
sults of these experiments are extremely suggestive, we caution
the reader that these are only a speciﬁc set of experiments and
that one should be careful about drawing general conclusions.
1) Amount of Missing Data (Fig. 4(a)): To evaluate the
performance of M-RNN in comparison to benchmarks in set-
tings with more missing data, we constructed sub-samples of
the MIMIC-III dataset by randomly removing 10%, 20%, 30%,
40%, 50% of the actual data and carrying out the same esti-
mation exercise as above on the smaller datasets that remain.
(Recall that in the original MIMIC-III dataset, 75% of the data is
already missing; hence removing 50% of the data present leads
to an artiﬁcial dataset in which 87.5% of the data is missing.)
The graph in Fig. 4(a) shows the performance of M-RNN against
the best benchmarks of each type for these smaller datasets. As
can be seen, M-RNN continues to substantially outperform the
benchmarks. Note that as the amount of missing data increases
the improvement of M-RNN over the imputation benchmark(s)
increases, but the improvement over the interpolation bench-
marks decreases.
2) Number of Data Streams (Fig. 4(b)): As we have noted,
typical medical datasets contain many data streams (many fea-
ture dimensions). To evaluate the performance of M-RNN in
comparison to benchmarks in settings with fewer data streams,
we conducted experiments in which we reduced the number of
data streams (feature dimensions) of MIMIC-III. In the origi-
nal MIMIC-III dataset the number of data streams is D = 40;
we conducted experiments with D = 3, 5, 7, 10, 15, 20 data
streams. (In each case, we conducted 10 trials in which we
selected data streams at random; we report the average of these
10 trials.) As expected, the performance of M-RNN degrades
when there are fewer data streams, but as Fig. 4(b) shows,
M-RNN still outperforms the benchmarks. (Note that interpo-
lation methods are insensitive to the number of data streams
because they operate only within each data stream separately.)
3) Number of Samples (Fig. 4(c)): The original MIMIC-
III dataset has N = 23, 160 samples (patients). To understand
the performance of M-RNN in comparison to benchmarks
in settings with fewer samples, we conducted experiments in
which we used only subsets of all patients (samples) of sizes
N = 500, 1000, 2000, 4000, 8000, 16000. Because M-RNN has
to learn many parameters, it should come as no surprise that,
as Fig. 4(c) shows, the performance of M-RNN degrades badly
– and indeed is worse than that of (some) other benchmarks
– when the number of samples is too small, but M-RNN out-
performs all the benchmarks as soon as the number of training
Authorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on October 28,2024 at 18:11:05 UTC from IEEE Xplore.  Restrictions apply. 
