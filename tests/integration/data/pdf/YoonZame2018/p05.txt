YOON et al.: ESTIMATING MISSING DATA IN TEMPORAL DATA STREAMS USING MULTI-DIRECTIONAL RECURRENT NEURAL NETWORKS
1481
data stream are learned separately, and the number of parame-
ters that must be learned is linear in the number of streams D.
Note that ˜xd
t is not the ﬁnal output of our M-RNN architecture
and is not necessarily an estimate of xd
t .
C. Imputation Block
The Imputation blocks constructs an imputation function Ψ
that operates across streams. To again emphasize that the esti-
mate ˆxd
t for xd
t depends on the data with xd
t removed, we write
ˆxd
t = Ψ(Dt −xd
t ); again, keep in mind that now we are using
only data at time stamp st, not data from other time stamps.
(Dt represents the t-th time stamp of the entire dataset D.) We
construct the function Ψ to be independent of t, so we use fully
connected layers; see the Imputation component of Fig 2. If we
write zt = [˜xt, mt] then a more mathematical description is:
ˆxt = σ(Wht + α)
ht = φ(Uxt + V zt + β)
where σ, φ are activation functions. It is important to keep in
mind that the diagonal entries of U are zero and the off-diagonal
entries of W are zero (i.e., W is diagonal) so that we do not use
xd
t in the estimation of ˆxd
t .
We learn the functions {Φd}D
d=1 and Ψ jointly using the
stacked networks of Bi-RNN and Fully Connected (FC) layers,
using MSE as the objective function.
Ψ∗, {Φ∗
d}D
d=1 = arg min
Φd ,Ψ
L

Ψ

xd
t , Φd

{xd
τ , md
τ , δd
τ }T
τ =1
	
, md
t

D
d=1
	
T
t=1, x
	
(2)
Note that ˜xt is the output of the interpolation block, and ˆxt is
the ﬁnal output of the entire M-RNN architecture.
D. Multiple Imputations
It is well-understood that to account for the uncertainty in
estimating missing values, it is useful to produce multiple esti-
mates and generate multiple imputed datasets. These multiple
imputed datasets can each be analyzed using standard meth-
ods and the results can be combined using Rubin’s rule [3].
In our case, we generate multiple imputed datasets using the
well-known Dropout [18] approach driven from the Bayesian
Neural Network framework [31]: we randomly select neurons
in the fully connected layers and delete those neurons and all
their connections. (The dropout probability p ∈(0, 1) is a hyper-
parameter to be chosen; the neurons to be dropped are chosen
according to the Bernoulli distribution with parameter p.) In
the training stage, we conduct joint optimization (Equation (2))
using the dropout process. We then generate multiple outputs
ot by sampling different dropout vectors R from the Bernoulli
distributions. This yields multiple imputations (MI). (To con-
struct a single imputation (SI) we proceed in precisely the same
way but set the dropout probability to 0. For comparisons, we
normalize the ﬁnal output by multiplying by p.)
E. Overall Structure and Computation Complexity
We refer to the entire structure above as a Multi-directional
Recurrent Neural Network (M-RNN). We use the notations
M-RNN (MI) and M-RNN (SI) to clarify whether we are pro-
ducing multiple or single imputations. The entire training times
for both M-RNN (MI) and M-RNN (SI) are less than 2 hours
for all 6 datasets (described in Section V) on a computer with
Intel Core i7-4770 (3.4 GHz) CPU with 32 GB RAM. With the
same machine, the entire training time for Multiple Imputation
with Chained Equation (MICE) [5] is around 11 hours for all 6
datasets.
V. DATASETS
We use ﬁve datasets, the characteristics of which are summa-
rized in Table I: more detailed descriptions are below.
A. MIMIC-III
The dataset MIMIC-III [19] contains data for patients moni-
tored in intensive care units (ICUs) of various hospitals. Within
the entire MIMIC-III dataset, we only used the data for the
23,160 patients whose measurements are recorded by Metavi-
sion (post 2008). We used the 20 vital signs (e.g., heart rate,
respiratory rate, blood pressures, etc.) and 20 lab tests (e.g.,
creatinine, chloride, etc.) whose missing rates are the least. For
these patients we have 40 physiological data streams in all. Vital
signs were sampled roughly every hour; lab tests were sampled
roughly every 12 hours. Each patient was followed until either
death (1,320 patients (5.7%)) or discharge from ICU (21,840
patients (94.3%)). Note that because lab tests are sampled only
1/12 as often as vital signs, in effect 11/12 of lab test data is
missing – even if every lab test for every patient was actually
conducted and recorded. For the purpose of prediction, we take
the goal as predicting at each time t whether the patient will die
within the next 24 hours. Hence we assign the label yt = 1 if
the patient actually died within the 24 hrs following the time st
and yt = 0 otherwise.
B. Deterioration
The dataset described in [20] provides records for a cohort
of 6,094 patients who were followed for potential clinical de-
terioration while hospitalized. Patients were monitored for 28
vital signs (heart rate, blood pressure, etc.) and 10 lab tests
(creatinine, hemoglobin, etc.) so there are 38 physiological data
streams in all. Vital signs were sampled roughly every 4 hours;
lab tests were sampled roughly every 24 hours. Each patient was
followed until either admission to ICU (306 patients (5.0%)) or
discharge from hospital (5,788 patients (95.0%)). Again, be-
cause lab tests are sampled only 1/6 as often as vital signs, in
effect 5/6 of lab test data is missing – even if every lab test
for every patient was actually conducted and recorded. For the
purpose of prediction, we take the goal as predicting at each
time stamp st whether the patient will be admitted to the ICU
(experienced clinical deterioration) within the next 24 hours.
Hence we assign the label yt = 1 if the patient was admitted to
the ICU within the following 24 hours and yt = 0 otherwise.
C. UNOS-Heart and UNOS-Lung
The UNOS (United Network for Organ Transplanta-
tion) dataset (available at https://www.unos.org/data/) provides
Authorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on October 28,2024 at 18:11:05 UTC from IEEE Xplore.  Restrictions apply. 
