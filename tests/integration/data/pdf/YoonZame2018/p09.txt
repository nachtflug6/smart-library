YOON et al.: ESTIMATING MISSING DATA IN TEMPORAL DATA STREAMS USING MULTI-DIRECTIONAL RECURRENT NEURAL NETWORKS
1485
Fig. 4.
Imputation accuracy for the MIMIC-III dataset with various settings. (a) Additional data missing at random. (b) Feature dimensions chosen
at random. (c) Samples chosen at random. (d) Measurements chosen at random.
samples exceeds N = 7, 000. (However, one should not nec-
essarily take the ﬁgure N = 7, 000 as representing a cut-off
below which M-RNN should not be applied, because M-RNN
outperforms the benchmarks on the Deterioration and Biobank
datasets, which contain only 6,094 samples and 3,902 samples,
respectively.)
4) Number of Measurements Per Patient (Fig. 4(d)): We
have already noted that, in our datasets, MIMIC-III and Deterio-
ration have many (relatively frequent) measurements per patient,
while the other datasets have only a few (and infrequent) mea-
surements per patient and that this leads to differences in per-
formance of M-RNN. To further explore this effect, we created
subsets of the MIMIC-III dataset with T = 1, 3, 5, 10, 20, 30
measurements per patient. As might be expected, and as Fig. 4(d)
shows, having fewer measurements per patient degrades the per-
formance of interpolation-based algorithms but has little effect
on pure imputation-based methods; the performance of M-RNN
is also degraded, but to a much lesser extent.
D. Prediction Accuracy
As we have noted, there are many reasons for imputing miss-
ing data; one such is to improve predictive performance. We
therefore compare our method against the same 11 benchmarks
with respect to the accuracy of predicting labels. (See the de-
scription of the datasets in Section V for labeling in each case.)
For this purpose, we use Area Under the Receiver Operating
Characteristic Curve (AUROC) as the measure of performance.
(AUROC is deﬁned as the area under the receiver operating
characteristic curve, which is the graph of sensitivity (true pos-
itive rate) vs. 1-speciﬁcity (false positive rate).) To be fair to all
methods of imputing missing values, we use the same predictive
model (a simple 1-layer RNN) in all cases.
1) Prediction Accuracy on the Original Datasets: In this
subsection, we evaluate the effects of the imputations on the
prediction of labels (outcomes), which in the cases at hand
correspond to prognoses.
Table V shows the mean and percentage performance gain
of M-RNN (MI) in comparison with the benchmarks on all the
datasets. M-RNN – which we have already shown to achieve
the best imputation accuracy – also yields the best prediction
accuracy. However, even in cases where the improvement in
imputation accuracy is large and statistically signiﬁcant, the im-
provements in prediction accuracy are sometimes smaller and
not always statistically signiﬁcant. For instance, on the Deteri-
oration dataset, the AUROC of M-RNN (MI) is 0.7779 (95%
CI: 0.7678–0.7868); the best benchmark is [24] with AUROC
of 0.7593 (95% CI: 0.7478–0.7702). Similarly, on the UNOS-
Heart dataset, the AUROC of M-RNN (MI) is 0.6855 (95%
CI: 0.6781–0.6913); the best benchmark is MissForest, with
AUROC of 0.6740 (95% CI: 0.6651–0.6817).
It should be noted that, by using mean squared error as the
loss function, we have deliberately optimized M-RNN for impu-
tation accuracy. If we want to optimize M-RNN for prediction
accuracy we might do better by using a different loss func-
tion, such as cross-entropy. In Table XI of the Appendix we
report an experiment in which we have done precisely that; the
short summary is that optimizing for prediction accuracy does
in fact improve the predictive performance of M-RNN but the
improvement is marginal.
The Appendix also reports other experiments that help fur-
ther our understanding of the M-RNN algorithm. Table VIII
demonstrates that using a different predictive model (random
forest, logistic regression or Xgboost [34], rather than a 1-layer
RNN) for prediction after imputation leads to results similar
to those obtained above. Tables IX and X demonstrate that ac-
counting for donor features in the UNOS datasets makes little
difference.
E. Prediction Accuracy With Various Missing Rates
As discussed above, we carried out experiments with in-
creased rates of missing data in order to understand the im-
plications for the accuracy of imputation. We also carried out
Authorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on October 28,2024 at 18:11:05 UTC from IEEE Xplore.  Restrictions apply. 
