Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction
KDD ’22, August 14–18, 2022, Washington DC, USA
improvement comparing with the baseline on both decay-
ing scenarios, which demonstrates the powerful ability of
NsTKA to delineate the non-stationary temporal patterns.
In summary, NsTKA is proved to be effective on both station-
ary and non-stationary datasets, which can be widely applied to
different domains.
6
CONCLUSION
In this paper, by introducing the Generalized Spectral Mixture Ker-
nel, and integrating it to the attention module, we mathematically
reveal its representation capability in terms of the non-stationary
temporal-correlation. Then, a new attention structure is devised
for input sequences of neural networks. Finally, exhaustive experi-
ments are conducted to present its competitive performance against
related work, followed by an ablation experiment which shows its
effectiveness on both stationary and non-stationary temporal corre-
lations. We would further improve the robustness and performance
of the method in our future work. Furthermore, we plan to extend
our method to the reinforcement learning domain, as the ability to
predict future off-policy performance in non-stationary environ-
ments is common and critical in real-world scenarios.
REFERENCES
[1] Inci M Baytas, Cao Xiao, Xi Zhang, Fei Wang, Anil K Jain, and Jiayu Zhou. 2017.
Patient subtyping via time-aware LSTM networks. In Proceedings of the 23rd ACM
SIGKDD international conference on knowledge discovery and data mining. 65–74.
[2] Yoshua Bengio, Patrice Y. Simard, and Paolo Frasconi. 1994. Learning long-term
dependencies with gradient descent is difficult. IEEE transactions on neural
networks 5, 2 (1994), 157–66.
[3] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase
representations using RNN encoder-decoder for statistical machine translation.
arXiv preprint arXiv:1406.1078 (2014).
[4] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
Jared Davis, Tamás Sarlós, David Belanger, Lucy J. Colwell, and Adrian Weller.
2020. Masked Language Modeling for Proteins via Linearly Scalable Long-Context
Transformers. CoRR abs/2006.03555 (2020).
[5] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and
Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond
a Fixed-Length Context. In Proceedings of the 57th Conference of the Association
for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
Volume 1: Long Papers. 2978–2988.
[6] Daniele Durante and David B Dunson. 2014. Bayesian dynamic financial networks
with time-varying predictors. Statistics & Probability Letters 93 (2014), 19–26.
[7] Farzad Eskandanian and Bamshad Mobasher. 2019. Modeling the Dynamics of
User Preferences for Sequence-Aware Recommendation Using Hidden Markov
Models. In The Thirty-Second International Flairs Conference. 425–430.
[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.
2017. Convolutional Sequence to Sequence Learning. In Proceedings of the 34th
International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia,
6-11 August 2017. 1243–1252.
[9] Marc G Genton. 2001. Classes of kernels for machine learning: a statistics
perspective. Journal of machine learning research 2, Dec (2001), 299–312.
[10] Mark N Gibbs. 1997. Bayesian Gaussian processes for regression and classification.
Ph. D. Dissertation. University of Cambridge.
[11] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[12] Tsung-Yu Hsieh, Suhang Wang, Yiwei Sun, and Vasant Honavar. 2021. Explainable
Multivariate Time Series Classification: A Deep Neural Network Which Learns
to Attend to Important Variables As Well As Time Intervals. In Proceedings of the
14th ACM International Conference on Web Search and Data Mining. 607–615.
[13] Rolf Jagerman, Ilya Markov, and Maarten de Rijke. 2019. When people change
their mind: Off-policy evaluation in non-stationary recommendation environ-
ments. In Proceedings of the Twelfth ACM International Conference on Web Search
and Data Mining. 447–455.
[14] Mingi Ji, Weonyoung Joo, Kyungwoo Song, Yoon-Yeong Kim, and Il-Chul Moon.
2020. Sequential recommendation with relation-aware kernelized self-attention.
In Proceedings of the AAAI conference on artificial intelligence. 4304–4311.
[15] Jiacheng Li, Yujie Wang, and Julian J. McAuley. 2020. Time Interval Aware
Self-Attention for Sequential Recommendation. In WSDM ’20: The Thirteenth
ACM International Conference on Web Search and Data Mining, Houston, TX, USA,
February 3-7, 2020. 322–330.
[16] Xuan Liang, Tao Zou, Bin Guo, Shuo Li, Haozhe Zhang, Shuyi Zhang, Hui Huang,
and Song Xi Chen. 2015. Assessing Beijing’s PM2. 5 pollution: severity, weather
impact, APEC and winter heating. Proceedings of the Royal Society A: Mathemati-
cal, Physical and Engineering Sciences 471, 2182 (2015), 20150257.
[17] Manxia Liu, Arjen Hommersom, Maarten van der Heijden, and Peter JF Lucas.
2017. Hybrid time Bayesian networks. International Journal of Approximate
Reasoning 80 (2017), 460–474.
[18] Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin
Ke, Liwei Wang, and Tie-Yan Liu. 2021. Stable, fast and accurate: Kernelized
attention with relative positional encoding. In Advances in Neural Information
Processing Systems. 22795–22807.
[19] Simone Marini, Emanuele Trifoglio, Nicola Barbarini, Francesco Sambo, Barbara
Di Camillo, Alberto Malovini, Marco Manfrini, Claudio Cobelli, and Riccardo
Bellazzi. 2015. A Dynamic Bayesian Network model for long-term simulation
of clinical complications in type 1 diabetes. Journal of biomedical informatics 57
(2015), 369–376.
[20] Hongyuan Mei and Jason Eisner. 2017. The Neural Hawkes Process: A Neurally
Self-Modulating Multivariate Point Process. In Advances in Neural Information
Processing Systems 30: Annual Conference on Neural Information Processing Systems
2017, December 4-9, 2017, Long Beach, CA, USA. 6754–6764.
[21] Subhabrata Mukherjee, Hemank Lamba, and Gerhard Weikum. 2017. Item rec-
ommendation with evolving user preferences and experience. arXiv preprint
arXiv:1705.02519 (2017).
[22] Sami Remes, Markus Heinonen, and Samuel Kaski. 2017. Non-stationary spectral
kernels. In Proceedings of the 31st International Conference on Neural Information
Processing Systems. 4645–4654.
[23] Yves-Laurent Kom Samo and Stephen Roberts. 2015. Generalized spectral kernels.
arXiv preprint arXiv:1506.02236 (2015).
[24] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with
Relative Position Representations. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018,
Volume 2 (Short Papers). 464–468.
[25] Satya Narayan Shukla and Benjamin M. Marlin. 2021. Multi-Time Attention
Networks for Irregularly Sampled Time Series. In 9th International Conference on
Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
[26] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and
Ruslan Salakhutdinov. 2019. Transformer Dissection: An Unified Understanding
for Transformer’s Attention via the Lens of Kernel. In Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
2019, Hong Kong, China, November 3-7, 2019. 4343–4352.
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998–6008.
[28] Christopher K Williams and Carl Edward Rasmussen. 2006. Gaussian processes
for machine learning. Vol. 2. MIT press Cambridge, MA.
[29] Andrew Wilson and Ryan Adams. 2013. Gaussian process kernels for pattern
discovery and extrapolation. In International conference on machine learning.
1067–1075.
[30] Da Xu, Chuanwei Ruan, Sushant Kumar, Evren Korpeoglu, and Kannan Achan.
2019. Self-attention with functional time representation learning. In Advances in
Neural Information Processing Systems. 15915–15925.
[31] Jingyu Zhao, Feiqing Huang, Jia Lv, Yanjie Duan, Zhen Qin, Guodong Li, and
Guangjian Tian. 2020. Do RNN and LSTM have Long Memory?. In Proceedings of
the 37th International Conference on Machine Learning. 11365–11375.
[32] Qingyuan Zhao, Murat A. Erdogdu, Hera Y. He, Anand Rajaraman, and Jure
Leskovec. 2015. SEISMIC: A Self-Exciting Point Process Model for Predicting
Tweet Popularity. In Proceedings of the 21th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, Sydney, NSW, Australia, August 10-13,
2015. 1513–1522.
[33] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
and Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-
quence time-series forecasting. In Proceedings of AAAI.
[34] Yu Zhu, Hao Li, Yikang Liao, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai.
2017. What to Do Next: Modeling User Behaviors by Time-LSTM. In Proceedings
of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI
2017, Melbourne, Australia, August 19-25, 2017. 3602–3608.
1232
