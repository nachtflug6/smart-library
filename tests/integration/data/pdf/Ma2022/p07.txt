Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction
KDD â€™22, August 14â€“18, 2022, Washington DC, USA
We use the validation set to tune hyper-parameters and terminate
training if the validation performance has no improvement after 10
epochs. All performance is reported on the test set.
5.3
Evaluation Metrics
For the PM2.5 and IndD multi-class prediction tasks, we evaluate
the model performance by classification accuracy. For the ETT and
SynD regression tasks, we adopt the normalized mean squared error
(NMSE) to measure the performance, in which the continuous-value
data are normalized to zero-mean and unit deviation that they are
comparable. Specifically, for the ETT dataset, since it is a multi-step
prediction task, we report the averaged NMSE for the multi-step
predicted results.
5.4
Performance Comparison (Q1)
Table 2 shows the experimental results. We can see that, compared
with RNN-like methods, Transformer variations show a signifi-
cant improvement. Through introducing PE for explicitly mod-
eling position information, we observed a further improvement.
This demonstrates that temporal correlation among events is criti-
cal in forecasting the following event. Replacing PE with ME, the
continuous time-span information is taken into consideration so
that the fine-grained temporal patterns can be captured. The pro-
posed method, Transformer+NsTKA, outperforms competitors by a
large margin on both the synthetic datasets and real-world datasets
in most cases. Specifically, on different types of non-stationary
datasets and a number of temporal datasets on different domains,
Transformer+NsTKA consistently shows the best or the second-
best performance, which indicates that the proposed method can
capture non-stationary patterns under complex settings. Further-
more, on two of the synthetic datasets, Transformer + PE has a
better performance than ours. It might due to the reason that the
classic PE takes the format of predefined sine and cosine functions
and our synthetic dataset is also composed of sinusoidal waves.
With proper initialization, the Transformer + PE method can cap-
ture the dominating temporal patterns well. Nevertheless, in terms
of the representation capability in a much wider range, the above
results have well verified the necessity of encoding non-stationary
characteristics.
To qualitatively verify the effectiveness of the proposed method,
we plot fitted curves in Figure 3. In Figure 3, we compared the
prediction on one of the synthetic signal between our proposed
method and the one of Transformer + ME, which is a typical sta-
tionary time-encoding representation. It is readily observed that
the proposed method can precisely fit the curve the best. Since
these baseline methods are designed for stationary datasets, the
models would try to fit the targeted curve with a stationary basis,
resulting in a misalignment both on the frequency and amplitude.
Specifically, we can see that curves fitted by these baseline meth-
ods cannot align well within valleys and peaks because valleys
and peaks do not occur in a fixed period. A similar phenomenon
can also be observed on the fitting of the amplitudes. This further
demonstrates that the model capacity of these baseline methods is
not good enough to fit the non-stationary pattern.
Figure 3: Fitted curves given by different methods. Upper:
Transformer+NsTKA. Lower: Transformer+ME.
5.5
Ablation Study (Q2)
As mentioned in section 4.2, when the mean, length-scale and
weight functions of the GSM kernel reduce to input-independent
constants, the GSM kernel would reduce to a stationary SM kernel.
To show the effectiveness of encoding non-stationarity through
our proposed method, we firstly modify NsTKA by replacing the
non-stationary kernel with a stationary kernel, while other com-
ponents keep unchanged. Furthermore, the GSM kernel can also
be considered as the product of three kernels, i.e., the linear kernel
ğ›¼ğ‘–(ğ‘¥)ğ›¼ğ‘–(ğ‘¥â€²), the Gibbs kernel, and the cosine kernel, which are all
dependent on the absolute time. By reducing each component into
an ğ‘¥-independent term or completely removing the component, we
analyze how each component contributes to the improvement.
5.5.1
Effectiveness of Non-stationary Kernel. First, we only replace
the non-stationary kernel with a stationary one to verify the ne-
cessity of capturing the non-stationary pattern. From the results
shown in Table 3, we can see that using a stationary kernel would
lead to performance drop on all the datasets. This suggests that the
non-stationarity is served as an important factor in the temporal
event prediction, and our non-stationary kernel can successfully
take advantage of this information to improve the performance.
5.5.2
Effectiveness of Each Kernel. Table 4 shows the experimental
results with different combinations of kernels, where ğœ‡ğ‘–, ğ‘™ğ‘–and ğ›¼ğ‘–
denote these kernels are independent of ğ‘¥. We can observe that:
â€¢ by singly removing ğ›¼ğ‘–(ğ‘¥) which makes each mixture com-
ponent equally weighted, a largest performance drop is ob-
served, which suggests that different components serve as
different roles in the model and they require careful weight-
ing.
â€¢ removing ğ‘¥from the input of the three kernels all leads to
the degradation of performance except for the ğœ‡ğ‘–case. On
one hand, it suggests that ğ‘¥is of great importance on both
the ğ‘™ğ‘–(ğ‘¥) and ğ›¼ğ‘–(ğ‘¥) parts of the kernel, which represents the
short- and long-range temporal dependency and component
weights separately as described in section 4.2. And the tem-
poral patterns regarding these components with regard to ğ‘¥
are successfully extracted as well. Meanwhile, the slightly
1230
