KDD â€™22, August 14â€“18, 2022, Washington DC, USA
Yu Ma et al.
class of stationary covariances:
ğ‘˜(ğœ) =
âˆ‘ï¸
ğ‘–
ğ›¼ğ‘–ğ‘’ğ‘¥ğ‘(âˆ’2ğœ‹2ğœ2
ğ‘–ğœ)ğ‘ğ‘œğ‘ (2ğœ‹ğœ‡ğ‘–ğœ),
(6)
where ğ›¼ğ‘–are the weights, ğœğ‘–, and ğœ‡ğ‘–are the variance and mean of
mixture Gaussian components.
A non-stationary kernel, on the other hand, can infer the rela-
tions in a input-dependent manner, i.e., it is a function of ğ‘¥, ğ‘¥â€², and
ğœ= (ğ‘¥âˆ’ğ‘¥â€²)ğ‘‡. A classic extension of the stationary SM kernel (i.e.,
Eq.6) to a non-stationary Spectral Mixture kernel is in the format
of:
ğ‘˜(ğ‘¥,ğ‘¥â€²,ğœ) =
âˆ‘ï¸
ğ‘–
ğ›¼2
ğ‘–ğ‘’ğ‘¥ğ‘(âˆ’2ğœ‹2ğœğ‘‡Î£ğ‘–ğœ)Î¨ğœ‡ğ‘–,ğœ‡â€²
ğ‘–(ğ‘¥)ğ‘‡Î¨ğœ‡ğ‘–,ğœ‡â€²
ğ‘–(ğ‘¥â€²),
where
Î¨ğœ‡ğ‘–ğœ‡â€²
ğ‘–(ğ‘¥) =
ğ‘ğ‘œğ‘ (2ğœ‹ğœ‡ğ‘–ğ‘¥) + ğ‘ğ‘œğ‘ (2ğœ‹ğœ‡â€²
ğ‘–ğ‘¥)
ğ‘ ğ‘–ğ‘›(2ğœ‹ğœ‡ğ‘–ğ‘¥) + ğ‘ ğ‘–ğ‘›(2ğœ‹ğœ‡â€²
ğ‘–ğ‘¥)

,
Î£ğ‘–=

ğœ2
ğ‘–
ğœŒğ‘–ğœğ‘–ğœâ€²
ğ‘–
ğœŒğ‘–ğœğ‘–ğœâ€²
ğ‘–
ğœâ€²2
ğ‘–

.
(7)
ğ›¼ğ‘–is the mixture weight for each component. ğœŒğ‘–, (ğœ‡ğ‘–, ğœ‡â€²
ğ‘–) , (ğœğ‘–, ğœâ€²
ğ‘–)
are the correlation, means and variances of the bivariate Gaussian
components, respectively.
By using a mixture of bi-variate Gaussian components, Eq.7
intrinsically is a closed-form solution to the generalized Fourier
inverse transform:
ğ‘˜(ğ‘¥,ğ‘¥â€²) =
âˆ«
ğ‘…
âˆ«
ğ‘…
ğ‘’2ğœ‹ğ‘–(ğ‘¥ğœ”âˆ’ğ‘¥â€²ğœ”â€²)ğœ‡ğœ”(ğ‘‘ğœ”,ğ‘‘ğœ”â€²),
(8)
where ğœ‡ğœ”is a positive bounded symmetric measure [9] associated
to some PSD spectral density function ğ‘†(ğœ”,ğœ”â€²), which is denoted
as the spectral surface. When the spectral measure mass is concen-
trated only on the diagonal (i.e., ğœ”= ğœ”â€²), its closed-form solution
would reduce to a stationary one, e.g., Eq.6. For the detailed descrip-
tion, please refer to [23].
Our proposed method is on the basis of the non-stationary Spec-
tral Mixture kernel. In the next section, we first present the mathe-
matical rationale of introducing such a non-stationary kernel into
the attention mechanism. Then, in the context of machine learn-
ing, by decomposing the kernel function, we explain why it can
count for the time-dependent periodical and long-range depen-
dency. Finally, a new attention neural structure is devised. In its
implementation, several constraints would be added for robustness.
4
METHODOLOGY
4.1
Attention with Temporal Embedding in
Kernel Perspective
Given a sequence S = {(ğ‘¡1,ğ‘’1), ..., (ğ‘¡ğ‘›,ğ‘’ğ‘›)} and its mapping func-
tions ğœ™ğ‘¡(ğ‘¡) and ğœ™ğ‘’(ğ‘’) that encodes ğ‘¡,ğ‘’into Rğ‘‘, conventional atten-
tion related methods usually add or concatenate the event embed-
ding ğœ™ğ‘’(ğ‘’) and the temporal embedding ğœ™ğ‘¡(ğ‘¡). We use the additive
form (i.e., xğ‘–= ğœ™ğ‘¡,ğ‘–+ğœ™ğ‘’,ğ‘–) as the case to illustrate the attention with
temporal embedding in a kernel view 1. To simplify notations, we
omit the variable ğ‘¡ğ‘–,ğ‘’ğ‘–in the ğœ™ğ‘¡(Â·) and ğœ™ğ‘’(Â·).
1Note that, since the concatenating form does not introduce the cross-terms
ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–,ğœ™ğ‘’,ğ‘—), ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘—,ğœ™ğ‘’,ğ‘–), obviously it is consistent with our conclusion that
the event and temporal correlations can be calculated separately. We omit its derivation
in this paper.
As stated in Eq.2, the output of the attention module can be
viewed as a superposition of relative contributions from each key-
value pair to each query. To be more specific, we would consider the
attention weights which depend on the similarity measure between
the query and key, and the value vector separately. The following
mathematical illustration applies to general attention mechanisms,
but out of simplicity, we use the self-attention case to demonstrate
the derivation where the query, key and value are all the same as
X = {x1, ..., xğ‘›}.
Following Eq.2 and omitting the constant normalizing factor
âˆšï¸
ğ‘‘ğ‘˜for notation simplicity, the attention weights, with explicitly
expressing the temporal and event embedding, can be reformatted
as :
ğ‘˜ğ‘’ğ‘¥ğ‘(xğ‘–, xğ‘—) = ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–+ ğœ™ğ‘’,ğ‘–,ğœ™ğ‘¡,ğ‘—+ ğœ™ğ‘’,ğ‘—)
= ğ‘’ğ‘¥ğ‘((ğœ™ğ‘¡,ğ‘–+ ğœ™ğ‘’,ğ‘–)(ğœ™ğ‘¡,ğ‘—+ ğœ™ğ‘’,ğ‘—)ğ‘‡)
= ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–ğœ™ğ‘‡
ğ‘¡,ğ‘—+ ğœ™ğ‘’,ğ‘–ğœ™ğ‘‡
ğ‘’,ğ‘—+ ğœ™ğ‘¡,ğ‘–ğœ™ğ‘‡
ğ‘’,ğ‘—+ ğœ™ğ‘’,ğ‘–ğœ™ğ‘‡
ğ‘¡,ğ‘—)
= ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–,ğœ™ğ‘¡,ğ‘—) Â· ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘’,ğ‘–,ğœ™ğ‘’,ğ‘—)
Â· ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–,ğœ™ğ‘’,ğ‘—) Â· ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘—,ğœ™ğ‘’,ğ‘–),
(9)
where the first two terms,ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–,ğœ™ğ‘¡,ğ‘—) andğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘’,ğ‘–,ğœ™ğ‘’,ğ‘—) clearly
represent the temporal and event correlation between (ğ‘¡ğ‘–,ğ‘’ğ‘–) and
(ğ‘¡ğ‘—,ğ‘’ğ‘—). With regard to the two cross terms ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–,ğœ™ğ‘’,ğ‘—) and
ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘—,ğœ™ğ‘’,ğ‘–), they represent the correlation between time ğ‘–and
event ğ‘—and vice versa.
The first two terms indicate that the event and temporal
correlation are calculated separately. From Eq.9, since the expo-
nential of a dot-product operation (i.e., ğ‘˜ğ‘’ğ‘¥ğ‘) is noted as an instance
of kernel operation [14, 28], the attention weights with temporal
embedding can be further considered as the product of temporal
kernel and event kernel separately.
The two cross terms are a byproduct of summing up the
temporal and event embedding. Since the time and event are
projected into two separate vector spaces, intuitively the two cross
terms have no valid physical meaning. Even worse, they introduce
a kind of noise in the attention weights. Several recent related work
[5, 26] also discussed this issue. They claimed that using the product
of separate temporal and non-temporal kernels is better than using
the direct sum of temporal and non-temporal embeddings.
Since we focus on improving the representation learning ability
of the non-stationary temporal correlation, in accordance with
Eq.9, in the next subsection, we introduce a non-stationary kernel
function to replace the exponential term ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–,ğœ™ğ‘¡,ğ‘—).
4.2
Encode Temporal Correlation with
Non-stationarity
In this section, we introduce a kernel function to encode tem-
poral correlation with non-stationarity. Note that, the original
ğ‘˜ğ‘’ğ‘¥ğ‘(ğœ™ğ‘¡,ğ‘–,ğœ™ğ‘¡,ğ‘—) function can be regarded as exponential of a lin-
ear kernel. As has been discussed in section 3.3, it suffers from
encoding the non-stationary temporal correlation. In order to infer
non-stationary long-range and periodic correlations in an input-
dependent manner, we utilize the Generalized Spectral Mixture
1227
