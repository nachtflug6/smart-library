<?xml version="1.0" encoding="UTF-8"?>
<TEI
    xmlns="http://www.tei-c.org/ns/1.0"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
    <teiHeader xml:lang="en">
        <fileDesc>
            <titleStmt>
                <title level="a" type="main" coords="1,73.27,61.51,443.35,22.12;1,71.74,89.41,446.40,22.12;1,206.66,117.30,176.56,22.12">Estimating Missing Data in Temporal Data Streams Using Multi-Directional Recurrent Neural Networks</title>
                <funder ref="#_X6gKQre #_TNV4sgs #_Yzwfg48">
                    <orgName type="full">National Science Foundation</orgName>
                </funder>
                <funder>
                    <orgName type="full">Office of Naval Research</orgName>
                    <orgName type="abbreviated">ONR</orgName>
                </funder>
            </titleStmt>
            <publicationStmt>
                <publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
                <availability status="unknown">
                    <licence/>
                </availability>
                <date type="published" when="2019-05">2019-05</date>
            </publicationStmt>
            <sourceDesc>
                <biblStruct>
                    <analytic>
                        <author>
                            <persName coords="1,108.60,150.63,64.86,10.14">
                                <forename type="first">Jinsung</forename>
                                <surname>Yoon</surname>
                            </persName>
                            <idno type="ORCID">0000-0002-5481-5171</idno>
                        </author>
                        <author>
                            <persName coords="1,190.39,150.63,77.20,10.14">
                                <forename type="first">William</forename>
                                <forename type="middle">R</forename>
                                <surname>Zame</surname>
                            </persName>
                        </author>
                        <author>
                            <persName coords="1,298.17,150.63,183.10,10.14">
                                <roleName>Fellow, IEEE</roleName>
                                <forename type="first">Mihaela</forename>
                                <surname>Van Der Schaar</surname>
                            </persName>
                        </author>
                        <author>
                            <affiliation key="aff0" coords="1,112.71,647.05,176.24,7.37;1,37.91,656.02,215.59,7.37">
                                <orgName type="department">Department of Electrical and Computer Engineer- ing</orgName>
                                <orgName type="institution">University of California</orgName>
                                <address>
                                    <postCode>90095</postCode>
                                    <settlement>Los Angeles</settlement>
                                    <region>CA</region>
                                    <country key="US">USA</country>
                                </address>
                            </affiliation>
                        </author>
                        <author>
                            <affiliation key="aff1" coords="1,130.16,673.94,158.81,7.37;1,37.91,682.92,81.69,7.37;1,157.56,691.88,131.43,7.37;1,37.91,700.85,206.02,7.37">
                                <orgName type="department" key="dep1">Department of Economics and Mathematics</orgName>
                                <orgName type="department" key="dep2">Department of Engineering Science</orgName>
                                <orgName type="institution" key="instit1">University of California</orgName>
                                <orgName type="institution" key="instit2">University of Oxford</orgName>
                                <orgName type="institution" key="instit3">Alan Turing Institute</orgName>
                            </affiliation>
                        </author>
                        <title level="a" type="main" coords="1,73.27,61.51,443.35,22.12;1,71.74,89.41,446.40,22.12;1,206.66,117.30,176.56,22.12">Estimating Missing Data in Temporal Data Streams Using Multi-Directional Recurrent Neural Networks</title>
                    </analytic>
                    <monogr>
                        <title level="j" type="main">IEEE Transactions on Biomedical Engineering</title>
                        <title level="j" type="abbrev">IEEE Trans. Biomed. Eng.</title>
                        <idno type="ISSN">0018-9294</idno>
                        <idno type="eISSN">1558-2531</idno>
                        <imprint>
                            <publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
                            <biblScope unit="volume">66</biblScope>
                            <biblScope unit="issue">5</biblScope>
                            <biblScope unit="page" from="1477" to="1490"/>
                            <date type="published" when="2019-05"/>
                        </imprint>
                    </monogr>
                    <idno type="MD5">795617A9ABDF12378F2B6A7D1B475067</idno>
                    <idno type="DOI">10.1109/tbme.2018.2874712</idno>
                    <note type="submission">Manuscript received November 23, 2017; revised February 28, 2018, June 19, 2018, and September 9, 2018; accepted October 1, 2018. October 8, 2018; date of current version April 19, 2019</note>
                </biblStruct>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application version="0.8.2" ident="GROBID" when="2025-12-08T16:33+0000">
                    <desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
                    <label type="revision">a91ee48</label>
                    <label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[ref, p, biblStruct, persName, figure, formula, head, note, title, affiliation], sentenceSegmentation=false, flavor=null</label>
                    <ref target="https://github.com/kermitt2/grobid"/>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords>
                    <term>Missing data</term>
                    <term>temporal data streams</term>
                    <term>imputation</term>
                    <term>recurrent neural nets. I</term>
                </keywords>
            </textClass>
            <abstract>
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <p coords="1,92.16,194.02,196.81,8.29;1,37.91,203.98,251.08,8.29;1,37.91,213.94,251.09,8.29;1,37.91,223.91,251.08,8.29;1,37.91,233.87,251.07,8.29;1,37.91,243.83,251.06,8.29;1,37.91,253.80,251.07,8.29;1,37.91,263.76,251.06,8.29;1,37.91,273.72,251.08,8.29;1,37.91,283.69,251.07,8.29;1,37.91,293.65,251.06,8.29;1,37.91,303.61,251.08,8.29;1,37.91,313.58,251.08,8.29;1,37.91,323.54,251.08,8.29;1,37.91,333.50,251.07,8.29;1,37.91,343.47,251.08,8.29;1,37.91,353.43,251.06,8.29;1,37.91,363.40,251.09,8.29;1,37.91,373.36,251.10,8.29;1,37.91,383.31,251.07,8.29;1,37.91,393.28,251.08,8.29;1,37.91,403.24,251.09,8.29;1,37.91,413.20,251.07,8.29;1,37.91,423.17,251.09,8.29;1,37.91,433.13,251.10,8.29;1,37.91,443.09,90.09,8.29">Missing data is a ubiquitous problem. It is especially challenging in medical settings because many streams of measurements are collected at different-and often irregular-times. Accurate estimation of the missing measurements is critical for many reasons, including diagnosis, prognosis, and treatment. Existing methods address this estimation problem by interpolating within data streams or imputing across data streams (both of which ignore important information) or ignoring the temporal aspect of the data and imposing strong assumptions about the nature of the data-generating process and/or the pattern of missing data (both of which are especially problematic for medical data). We propose a new approach, based on a novel deep learning architecture that we call a Multi-directional Recurrent Neural Network that interpolates within data streams and imputes across data streams. We demonstrate the power of our approach by applying it to five real-world medical datasets. We show that it provides dramatically improved estimation of missing measurements in comparison to 11 state-of-the-art benchmarks (including Spline and Cubic Interpolations, MICE, MissForest, matrix completion, and several RNN methods); typical improvements in Root Mean Squared Error are between 35%-50%. Additional experiments based on the same five datasets demonstrate that the improvements provided by our method are extremely robust.</p>
                </div>
            </abstract>
        </profileDesc>
    </teiHeader>
    <facsimile>
        <surface n="1" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="2" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="3" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="4" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="5" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="6" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="7" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="8" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="9" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="10" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="11" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="12" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="13" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
        <surface n="14" ulx="0.0" uly="0.0" lrx="594.0" lry="792.0"/>
    </facsimile>
    <text xml:lang="en">
        <body>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <p coords="1,300.92,194.15,251.03,8.97;1,300.92,206.11,251.04,8.97;1,300.92,218.06,251.08,8.97;1,300.92,230.02,251.05,8.97;1,300.92,241.97,251.05,8.97;1,300.92,253.92,251.04,8.97;1,300.92,265.88,251.07,8.97;1,300.92,277.84,251.05,8.97;1,300.92,289.79,251.07,8.97;1,300.92,301.75,51.45,8.97">estimation of these missing measurements is often critical for accurate diagnosis, prognosis and treatment, as well as for accurate modeling and statistical analyses. This paper presents a new method for estimating missing measurements in time series data, based on a novel deep learning architecture. By comparing our method with current state-of-the-art benchmarks on a variety of real-world medical datasets, we demonstrate that our method is much more accurate in estimating missing measurements, and that this accuracy is reflected in improved prediction of outcomes.</p>
                <p coords="1,310.89,313.70,241.09,8.97;1,300.92,325.65,251.06,8.97;1,300.92,337.61,251.05,8.97;1,311.50,349.57,3.53,8.97;1,333.18,349.57,218.82,8.97;1,300.92,361.52,251.06,8.97;1,300.92,373.48,148.17,8.97;1,463.98,373.48,4.03,8.97;1,482.91,373.48,69.08,8.97;1,300.92,385.43,251.05,8.97;1,300.92,397.39,251.05,8.97;1,300.92,409.35,251.05,8.97;1,300.92,421.30,251.05,8.97;1,300.92,433.26,251.05,8.97;1,300.92,445.21,251.09,8.97;1,300.92,457.16,173.63,8.97;1,489.24,457.16,4.15,8.97;1,512.24,457.16,39.71,8.97;1,300.92,469.12,251.05,8.97;1,300.92,481.08,251.07,8.97;1,300.92,493.03,251.04,8.97;1,300.92,504.99,251.07,8.97;1,300.92,516.94,251.05,8.97;1,300.92,528.89,251.07,8.97;1,300.92,540.85,251.07,8.97;1,300.92,552.81,251.05,8.97;1,300.92,564.76,251.08,8.97;1,300.92,576.72,251.07,8.97;1,300.92,588.67,251.06,8.97;1,300.92,600.63,251.06,8.97;1,300.92,612.59,74.39,8.97;1,388.60,612.59,163.38,8.97;1,300.92,624.54,30.43,8.97;1,352.34,624.54,199.63,8.97;1,300.92,636.50,30.06,8.97">The most familiar methods for estimating missing data follow one of three approaches, usually called interpolation, imputation and matrix completion. Interpolation methods such as 
                    <ref type="bibr" coords="1,300.92,349.57,10.58,8.97" target="#b0">[1]</ref>, 
                    <ref type="bibr" coords="1,318.30,349.57,11.61,8.97" target="#b1">[2]</ref> exploit the correlation among measurements at different times within each stream but ignore the correlation across streams. Imputation methods such as 
                    <ref type="bibr" coords="1,451.90,373.48,12.09,8.97" target="#b2">[3]</ref>- 
                    <ref type="bibr" coords="1,468.01,373.48,12.09,8.97" target="#b5">[6]</ref> exploit the correlation among measurements at the same time across different streams but ignore the correlation within streams. Because medical measurements are frequently correlated both within streams and across streams (e.g., blood pressure at a given time is correlated both with blood pressure at other times and with heart rate), each of these approaches loses potentially important information. Matrix completion methods such as 
                    <ref type="bibr" coords="1,476.80,457.16,12.44,8.97" target="#b6">[7]</ref>- 
                    <ref type="bibr" coords="1,493.39,457.16,16.59,8.97" target="#b9">[10]</ref> do exploit correlations within and across streams, but assume that the data is static -hence ignore the temporal component of the dataor that the data is perfectly synchronized -an assumption that is routinely violated in medical time series data. Some of these methods also make modeling assumptions about the nature of the data-generating process or of the pattern of missing data. Our approach is expressly designed to exploit both the correlation within streams and the correlation across streams and to take into account the temporal and non-synchronous character of the data; our approach makes no modeling assumptions about the data-generating process or the pattern of missing data. (We do assume -as is standard in most of the literature -that the data is missing at random 
                    <ref type="bibr" coords="1,378.02,612.59,10.58,8.97" target="#b0">[1]</ref>. Dealing with data that is not missing at random 
                    <ref type="bibr" coords="1,333.55,624.54,16.59,8.97" target="#b10">[11]</ref> presents additional challenges and is left for future works.)
                </p>
                <p coords="1,310.89,648.45,241.08,8.97;1,300.92,660.40,250.97,8.97;1,300.92,672.36,251.06,8.97;1,300.92,684.32,251.05,8.97;1,300.92,696.27,100.79,8.97;1,434.46,696.27,117.51,8.97;1,300.93,708.23,40.39,8.97;1,361.05,708.23,190.92,8.97;2,42.12,424.44,251.07,7.37;2,42.12,433.40,251.05,7.37;2,42.12,442.01,251.06,7.74;2,42.12,450.98,251.06,8.08;2,42.12,460.30,62.14,7.37">Our method relies on a novel neural network architecture that we call a Multi-directional Recurrent Neural Network (M-RNN). Our M-RNN contains both an interpolation block and an imputation block and it trains these blocks simultaneously, rather than separately (See Fig. 
                    <ref type="figure" coords="1,404.98,696.27,4.98,8.97" target="#fig_0">1</ref> and 
                    <ref type="figure" coords="1,430.87,696.27,3.59,8.97">2</ref>). Like a bi-directional RNN (Bi-RNN) 
                    <ref type="bibr" coords="1,345.78,708.23,15.27,8.97" target="#b11">[12]</ref>, an M-RNN operates forward and backward Fig. 2. M-RNN architecture. (a) Architecture in the time domain section. (b) Architecture in the feature domain section (Dropout is used for multiple imputations). Note that both x (the output of interpolation block) and x are inputs to the imputation block to construct x (the output of imputation block).
                </p>
                <p coords="2,42.12,484.57,251.06,8.97;2,42.12,496.53,251.07,8.97;2,42.12,508.49,251.05,8.97;2,42.12,520.44,251.07,8.97;2,42.12,532.39,251.04,8.97;2,42.12,544.35,26.77,8.97;2,75.88,544.35,217.29,8.97;2,42.12,556.31,82.98,8.97">within each data stream -in the intra-stream directions. An M-RNN also operates across different data streams -in the inter-stream directions. Unlike a Bi-RNN, the timing of inputs into the hidden layers of our M-RNN is lagged in the forward direction and advanced in the backward direction. As illustrated in Fig. 
                    <ref type="figure" coords="2,72.14,544.35,3.74,8.97">2</ref>, our M-RNN architecture exploits the 3-dimensional nature of the dataset.
                </p>
                <p coords="2,52.08,568.26,241.10,8.97;2,42.12,580.22,251.02,8.97;2,42.12,592.17,251.07,8.97;2,42.12,604.12,229.44,8.97;2,289.35,604.12,3.82,8.97;2,42.12,616.08,232.77,8.97;2,289.65,616.08,3.53,8.97;2,42.12,628.04,251.06,8.97;2,42.12,639.99,251.05,8.97;2,42.12,651.95,251.05,8.97;2,42.12,663.90,212.61,8.97;2,267.92,663.90,3.53,8.97;2,289.34,663.90,3.82,8.97;2,57.39,675.85,235.80,8.97;2,42.12,687.81,251.06,8.97;2,42.12,699.77,251.06,8.97;2,42.12,711.73,50.48,8.97;2,111.03,711.73,3.82,8.97;2,133.27,711.73,129.23,8.97;2,285.41,711.73,7.75,8.97;2,42.12,723.68,192.69,8.97">An important aspect of medical data is that there is often enormous uncertainty in the measured data. As is well-known, although single imputation (SI) methods may yield the most plausible/most likely estimate for each missing data point 
                    <ref type="bibr" coords="2,274.08,604.12,15.27,8.97" target="#b12">[13]</ref>, they do not capture the uncertainty in the imputed data 
                    <ref type="bibr" coords="2,279.07,616.08,10.58,8.97" target="#b2">[3]</ref>. Multiple imputation (MI) methods capture this uncertainty by sampling imputed values several times in order to form multiple complete imputed datasets, analyzing each imputed dataset separately and combining the results via Rubin's rule 
                    <ref type="bibr" coords="2,257.34,663.90,10.58,8.97" target="#b2">[3]</ref>, 
                    <ref type="bibr" coords="2,274.07,663.90,15.27,8.97" target="#b13">[14]</ref>, 
                    <ref type="bibr" coords="2,42.12,675.85,15.27,8.97" target="#b14">[15]</ref>. Capturing the uncertainty in the dataset is especially important in the medical setting, in which diagnostic, prognostic and treatment decisions must be made on the basis of the imputed values 
                    <ref type="bibr" coords="2,95.76,711.73,15.26,8.97" target="#b15">[16]</ref>, 
                    <ref type="bibr" coords="2,118.00,711.73,15.27,8.97" target="#b16">[17]</ref>. In our setting, we use dropout 
                    <ref type="bibr" coords="2,265.67,711.73,16.59,8.97" target="#b17">[18]</ref> to produce multiple imputations; see Section IV-D.
                </p>
                <p coords="2,52.08,735.63,241.10,8.97;2,42.12,747.59,232.77,8.97;2,305.13,221.27,187.21,8.97;2,509.44,221.27,46.76,8.97;2,305.13,233.22,251.04,8.97;2,305.13,245.18,133.95,8.97;2,548.10,245.18,8.10,8.97;2,305.13,257.13,117.42,8.97;2,441.59,257.13,114.57,8.97;2,305.13,269.08,251.05,8.97;2,305.13,281.04,251.03,8.97;2,305.13,293.00,66.62,8.97;2,385.49,293.00,3.53,8.97;2,402.73,293.00,119.62,8.97;2,537.01,293.00,3.84,8.97;2,552.36,293.00,3.84,8.97;2,305.13,304.95,163.06,8.97;2,488.30,304.95,4.24,8.97;2,512.65,304.95,43.53,8.97;2,305.13,316.91,113.92,8.97;2,432.56,316.91,123.63,8.97;2,305.13,328.86,251.03,8.97;2,305.13,340.82,251.04,8.97;2,305.13,352.78,251.06,8.97;2,305.13,364.73,251.04,8.97;2,305.13,376.69,251.05,8.97;2,305.13,388.64,251.07,8.97;2,305.13,400.59,251.04,8.97;2,305.13,412.55,251.04,8.97;2,305.13,424.51,251.05,8.97;2,305.13,436.46,251.06,8.97;2,305.13,448.42,251.06,8.97;2,305.13,460.37,251.05,8.97;2,305.13,472.32,251.05,8.97;2,305.13,484.28,251.03,8.97;2,305.13,496.24,251.06,8.97;2,305.13,508.19,251.06,8.97;2,305.13,520.15,251.06,8.97;2,305.13,532.10,251.04,8.97;2,305.13,544.06,137.95,8.97;2,461.58,544.06,3.82,8.97;2,488.44,544.06,67.75,8.97;2,305.13,556.02,251.06,8.97;2,305.13,567.97,251.06,8.97;2,305.13,579.93,251.04,8.97;2,305.13,591.88,251.05,8.97">To demonstrate the power of our method, we apply it to five different public real-world medical datasets: the MIMIC-III 
                    <ref type="bibr" coords="2,276.57,747.59,16.59,8.97" target="#b18">[19]</ref> dataset, the clinical deterioration dataset used in 
                    <ref type="bibr" coords="2,494.17,221.27,15.27,8.97" target="#b19">[20]</ref>, the UNOS dataset for heart transplantation, the UNOS dataset for lung transplantation (both available at 
                    <ref type="url" coords="2,442.75,245.18,105.35,8.97" target="https://www.unos.org/data/">https://www.unos.org/data/</ref>), and the UK Biobank dataset 
                    <ref type="bibr" coords="2,426.33,257.13,15.26,8.97" target="#b20">[21]</ref>. We show that our method yields large and statistically significant improvements in estimation accuracy over previous methods, including interpolation methods such as 
                    <ref type="bibr" coords="2,374.91,293.00,10.58,8.97" target="#b0">[1]</ref>, 
                    <ref type="bibr" coords="2,392.16,293.00,10.58,8.97" target="#b1">[2]</ref>, imputation methods such as 
                    <ref type="bibr" coords="2,525.50,293.00,11.51,8.97" target="#b2">[3]</ref>- 
                    <ref type="bibr" coords="2,540.85,293.00,11.51,8.97" target="#b5">[6]</ref>, RNN-based imputation methods such as 
                    <ref type="bibr" coords="2,471.34,304.95,16.96,8.97" target="#b21">[22]</ref>- 
                    <ref type="bibr" coords="2,492.54,304.95,16.96,8.97" target="#b23">[24]</ref> and matrix completion methods such as 
                    <ref type="bibr" coords="2,421.98,316.91,10.58,8.97" target="#b6">[7]</ref>. For the MIMIC-III and clinical deterioration datasets the patient measurements were made frequently (hourly basis), and our method provides Root Mean Squared Error (RMSE) improvement of more than 50% over all 11 benchmarks. For the UNOS heart and lung transplantation datasets and the Biobank dataset, the patient measurements were made much less frequently (yearly basis), but our method still provides RMSE improvement of more than 40% in most cases, and significant improvements in the other cases. We also show that this improvement in estimation yields (smaller) improvements in the predictions of outcomes (patients' future states). A number of experiments based on these same datasets show that the extent to which our method improves on outcomes depends on the method used for prediction, on the way in which our model is optimized in training, on the amount of data available (both in terms of the number of patients for whom we have data and on the amount of data available for each patient), and on the nature and extent of missing data. These results illustrate the important point that, as mentioned earlier, there are many reasons for imputing missing data 
                    <ref type="bibr" coords="2,446.31,544.06,15.27,8.97" target="#b14">[15]</ref>, 
                    <ref type="bibr" coords="2,468.61,544.06,16.60,8.97" target="#b24">[25]</ref> -for the estimation of parameters (e.g., means or regression coefficients), for determination of confidence intervals and significance, as well as for prediction -and that no single method for imputing data can be expected to be superior on all datasets or for all reasons.
                </p>
                <p coords="2,315.10,603.83,11.07,8.97;2,350.85,603.83,205.33,8.97;2,305.13,615.79,251.06,8.97;2,305.13,627.75,251.04,8.97;2,305.13,639.70,251.07,8.97;2,305.13,651.66,251.05,8.97;2,305.13,663.61,251.06,8.97;2,305.13,675.56,55.89,8.97">As 
                    <ref type="bibr" coords="2,330.21,603.83,16.59,8.97" target="#b25">[26]</ref> has emphasized, an extremely desirable aspect of any imputation method is that it be congenial; i.e., that it should produce imputed values in a manner that preserves the original relationships between features and labels. As we demonstrate using the complete Biobank dataset, our method is also more congenial than the best competing benchmarks; see Section VI-G.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="2,390.50,706.39,80.32,9.22">II. RELATED WORK</head>
                <p coords="2,315.10,723.68,241.07,8.97;2,305.14,735.63,251.06,8.97;2,305.14,747.59,219.70,8.97;2,538.25,747.59,3.53,8.97;3,37.91,66.09,251.02,8.97;3,37.91,78.04,251.05,8.97;3,37.91,90.01,146.16,8.97;3,199.57,90.01,4.03,8.97;3,219.08,90.01,69.88,8.97;3,37.91,101.96,251.06,8.97;3,37.91,113.91,251.06,8.97;3,37.91,125.87,147.79,8.97;3,201.22,125.87,4.15,8.97;3,225.03,125.87,63.93,8.97;3,37.91,137.82,251.02,8.97;3,37.91,149.78,251.05,8.97;3,37.91,161.74,136.14,8.97">As we have noted, there are three standard and very widelyused methods for dealing with missing data: interpolation, imputation and matrix completion. Interpolation methods 
                    <ref type="bibr" coords="2,527.67,747.59,10.58,8.97" target="#b0">[1]</ref>, 
                    <ref type="bibr" coords="2,544.59,747.59,11.61,8.97" target="#b1">[2]</ref> attempt to reconstruct missing data by capturing the temporal relationship within each data stream but not the relationships across streams. Imputation methods 
                    <ref type="bibr" coords="3,187.48,90.01,12.09,8.97" target="#b2">[3]</ref>- 
                    <ref type="bibr" coords="3,203.60,90.01,12.09,8.97" target="#b5">[6]</ref> attempt to reconstruct missing data by capturing the synchronous relationships across data streams but not the temporal relationships within streams. Matrix completion methods 
                    <ref type="bibr" coords="3,188.78,125.87,12.45,8.97" target="#b6">[7]</ref>- 
                    <ref type="bibr" coords="3,205.37,125.87,16.59,8.97" target="#b9">[10]</ref> treat the data as static -ignoring the temporal aspect -or perfectly synchronized and assume a specific model of the data-generating process and/or the pattern of missing data.
                </p>
                <p coords="3,47.87,173.69,241.10,8.97;3,37.91,185.65,251.06,8.97;3,37.91,197.60,127.56,8.97;3,187.10,197.60,101.87,8.97;3,37.91,209.55,251.06,8.97;3,37.91,221.52,251.04,8.97;3,37.91,233.47,251.05,8.97;3,37.91,245.42,58.18,8.97;3,120.12,245.42,168.83,8.97;3,37.91,257.38,251.04,8.97;3,37.91,269.33,251.04,8.97;3,37.91,281.28,30.16,8.97;3,88.39,281.28,200.57,8.97;3,37.91,293.25,251.07,8.97;3,37.91,305.20,251.05,8.97;3,37.91,317.15,251.06,8.97;3,37.91,329.11,251.04,8.97;3,37.91,341.06,251.06,8.97;3,37.91,353.02,251.07,8.97;3,37.91,364.98,220.90,8.97">There is also a substantial literature that uses Recurrent Neural Networks (RNNs) for prediction on the basis of time series with missing data. For example, 
                    <ref type="bibr" coords="3,168.00,197.60,16.59,8.97" target="#b26">[27]</ref> first replaces all the missing values with a mean value, then uses the feedback loop from the hidden states to update the imputed values and finally uses the reconstructed data streams as inputs to a standard RNN for prediction. 
                    <ref type="bibr" coords="3,99.82,245.42,16.59,8.97" target="#b27">[28]</ref> uses the Expectation-Maximization (EM) algorithm to impute the missing values and again uses the reconstructed data streams as inputs to a standard RNN for prediction. 
                    <ref type="bibr" coords="3,69.93,281.28,16.59,8.97" target="#b28">[29]</ref> uses a linear model to estimate missing values from the latest measurement and the hidden state within each stream followed by a standard RNN for prediction. In the first two of these papers, missing values are imputed by using only the synchronous relationships across data streams but not the temporal relationships within streams; in the third paper, missing values are interpolated by using only the temporal relationships within each stream but not on the relationships across streams.
                </p>
                <p coords="3,47.87,376.93,241.11,8.97;3,37.91,388.88,186.31,8.97;3,242.99,388.88,4.07,8.97;3,263.32,388.88,4.07,8.97;3,285.14,388.88,3.82,8.97;3,37.91,400.84,251.07,8.97;3,37.91,412.79,251.05,8.97;3,37.91,424.76,251.06,8.97;3,37.91,436.71,251.06,8.97;3,37.91,448.66,227.84,8.97;3,285.15,448.66,3.82,8.97;3,53.17,460.62,3.82,8.97;3,81.19,460.62,207.75,8.97;3,37.91,472.57,251.07,8.97;3,37.91,484.52,197.43,8.97;3,257.41,484.52,31.55,8.97;3,37.91,496.49,251.05,8.97;3,37.91,508.44,210.05,8.97">A more recent literature extends these methods to deal with both missing data and irregularly sampled data 
                    <ref type="bibr" coords="3,226.72,388.88,16.27,8.97" target="#b21">[22]</ref>- 
                    <ref type="bibr" coords="3,247.05,388.88,16.27,8.97" target="#b23">[24]</ref>, 
                    <ref type="bibr" coords="3,269.87,388.88,15.27,8.97" target="#b29">[30]</ref>. All of these papers use the sampling times to capture the informative missingness and time interval information to deal with irregular sampling, using the measurements, sampling information and time intervals as the inputs of an RNN. However, they differ in the replacements they use for missing values. 
                    <ref type="bibr" coords="3,269.88,448.66,15.27,8.97" target="#b21">[22]</ref>, 
                    <ref type="bibr" coords="3,37.91,460.62,15.26,8.97" target="#b22">[23]</ref>, 
                    <ref type="bibr" coords="3,60.79,460.62,16.60,8.97" target="#b29">[30]</ref> replace the missing values with 0, mean values or latest measurements -all of which are independent of either the intra-stream or inter-stream relationships or both. 
                    <ref type="bibr" coords="3,238.08,484.52,16.59,8.97" target="#b23">[24]</ref> imputes the missing values using only the most recent measurements, the mean value of each stream, and the time interval.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="3,104.57,539.02,117.74,9.22">III. PROBLEM FORMULATION</head>
                <p coords="3,47.87,556.30,241.07,8.97;3,37.91,568.26,251.03,8.97;3,37.91,580.22,251.06,8.97;3,37.91,592.17,251.04,8.97;3,37.91,604.12,46.20,8.97">Our formulation and method are applicable to a wide variety of settings with missing data. However, for ease of exposition -and to facilitate the discussion of our application to medical datasets -it is convenient to adopt medical terminology throughout.</p>
                <p coords="3,47.87,616.03,241.09,9.02;3,37.91,628.04,251.04,8.97;3,37.91,639.94,251.05,9.02;3,37.91,651.90,251.06,9.02;3,37.91,663.23,251.03,9.96;3,37.91,675.19,251.08,9.96;3,37.91,687.15,104.52,9.96">We consider a dataset consisting of N patients. For each patient, we have a multivariate time series data stream of length T (the length T and the other components of the dataset may depend on the patient n but for the moment we suppress the dependence on n) that consists of time stamps S, measurements X , and labels Y, sampled from an (unknown) underlying distribution F: (S, X , Y) ∼ F.</p>
                <p coords="3,47.87,699.72,113.76,9.80;3,165.38,699.10,123.58,9.96;3,37.91,711.68,251.02,9.80;3,37.91,723.01,251.05,10.42;3,37.91,735.63,251.04,8.97;3,300.92,66.10,251.04,8.97;3,300.92,77.38,251.05,10.42;3,300.92,89.96,194.30,9.80;3,498.41,89.34,53.55,10.42;3,300.92,101.96,187.73,8.97">For each t the time stamp s t ∈ R represents the actual time at which the measurements x t were taken. For convenience we normalize so that s 1 = 0 (so that we are measuring actual times for each patient beginning from the first observation for that patient); we assume actual times are strictly increasing: s t+1 &gt; s t where 0 ≤ t &lt; T . Note that the measurements may not be sampled regularly, so that the interval s t+1s t between successive measurements need not be constant.</p>
                <p coords="3,310.89,113.86,241.10,9.02;3,300.92,125.87,251.05,8.97;3,300.92,137.77,251.04,9.80;3,300.92,149.77,251.05,8.97;3,300.92,161.07,218.03,10.71;3,518.95,160.12,3.63,6.25;3,518.95,166.20,2.52,6.25;3,526.36,161.07,25.63,9.96;3,300.93,173.64,226.62,9.80;3,527.54,172.08,3.63,6.25;3,527.54,178.16,2.52,6.25;3,534.96,173.02,17.02,9.96;3,300.92,185.60,251.04,9.80;3,300.92,197.55,129.51,9.02;3,430.43,195.99,3.63,6.25;3,430.43,202.07,2.52,6.25;3,437.84,196.93,114.12,9.63;3,300.93,209.50,5.70,8.93;3,306.62,207.95,3.63,6.25;3,306.62,214.02,2.52,6.25;3,314.89,209.55,237.07,8.97;3,300.92,221.51,251.05,8.97;3,300.92,232.80,162.15,9.63">There are D streams of measurements. We view each measurement as a real number, but it will typically be the case that not every stream is actually observed/measured at s t . Hence we adopt notation in which the set of possible measurements at the t-th time stamp s t is R * = R ∪ { * }. We interpret x d t = * to mean that the stream d was not measured at s t ; otherwise x d t ∈ R is the actual measurement of stream d at s t . (In computations with neural networks, we set x d t = 0 when the measurement x d t is missing. This guarantees that the missing measurement has no effect on the architecture.) For convenience, we scale all measurements to lie in the interval [0, 1].</p>
                <p coords="3,310.89,245.42,241.07,8.97;3,300.93,257.33,102.79,9.02;3,403.71,255.77,2.52,6.25;3,403.71,262.20,3.63,6.25;3,410.59,257.33,55.05,9.02;3,465.64,255.77,3.63,6.25;3,465.64,261.84,2.52,6.25;3,473.05,256.71,78.93,9.96;3,300.93,269.28,194.81,9.80;3,495.73,267.72,3.63,6.25;3,495.73,273.80,2.52,6.25;3,503.14,268.66,48.84,9.96;3,300.92,281.23,178.03,9.80;3,479.34,279.68,3.63,6.25;3,478.96,285.75,2.52,6.25;3,487.71,281.28,64.26,8.97;3,300.93,293.19,251.05,9.80;3,300.92,305.15,93.33,9.02;3,394.63,303.58,3.63,6.25;3,394.25,309.67,2.52,6.25;3,402.35,305.15,108.89,9.02;3,511.61,303.58,3.63,6.25;3,511.24,309.24,3.49,6.69;3,519.02,304.53,32.95,9.63;3,300.92,317.15,155.95,8.97">It is convenient to introduce some additional notation. For each t, define the index m t d to equal 0 if x d t = * (i.e., the stream d was not measured at s t ) and to equal 1 if x d t ∈ [0, 1] (the stream d was measured at s t ). We define δ d t to be the actual amount of time that has elapsed from s t since the stream d was measured previously; δ d t can be defined by setting δ d 1 = 0 and then proceeding recursively as follows:</p>
                <formula xml:id="formula_0" coords="3,333.00,339.36,185.70,26.69">δ d t = s t -s t-1 + δ d t-1 if t &gt; 1, m d t-1 = 0 s t -s t-1 if t &gt; 1, m d t-1 = 1.</formula>
                <p coords="3,300.93,377.47,31.57,9.96;3,332.88,378.09,219.09,9.80;3,300.92,389.43,73.42,10.42;3,374.76,393.60,4.07,6.25;3,381.04,389.43,7.47,9.96">Write δ t for the vector of elapsed times at time stamp t and Δ = {δ 1 , δ 2 , ..., δ T }.</p>
                <p coords="3,310.89,402.00,241.09,9.80;3,300.92,413.96,251.04,9.80;3,300.92,425.30,251.05,9.96;3,300.92,437.25,251.03,9.63;3,300.92,449.82,191.07,9.80;3,495.85,449.20,34.38,10.42;3,533.99,449.20,17.99,9.55">The label y t represents the outcome realized at time stamp t (actual time s t ) such as discharge, clinical deterioration, death. Y is the vector of outcomes for this patient. Again, we scale so the labels (and eventually predictions) lie in the interval [0, 1]. Frequently the outcome is binary in which case y t = 0 or y t = 1.</p>
                <p coords="3,310.89,461.78,241.09,9.02;3,300.92,473.78,251.04,8.97;3,300.92,485.74,251.02,8.97;3,300.92,497.70,251.07,8.97;3,300.92,509.65,251.06,8.97;3,300.92,521.55,207.89,9.02;3,508.81,520.00,3.63,6.25;3,508.81,526.07,2.52,6.25;3,513.45,520.93,38.53,9.63;3,300.92,533.51,251.06,9.02;3,300.92,545.51,251.05,8.97">The information available for a particular patient n is therefore a triple consisting of a sequence of time stamps, an array of measurements at each time stamp (with the above convention about missing measurements), and an array of labels at each time stamp. It is convenient to use functional notation to identify information about a particular patient, so x d t (n) is the measurement of stream d at time stamp t for patient n, etc. The entire dataset consists of all the triples for all the patients</p>
                <formula xml:id="formula_1" coords="3,300.92,555.86,125.94,12.34">D = {(S(n), X (n), Y(n)} N n =1 .</formula>
                <p coords="3,310.89,568.76,241.08,9.96;3,300.92,581.38,251.06,8.97;3,300.92,593.34,251.05,8.97;3,300.92,605.24,85.50,9.02;3,386.42,603.68,3.63,6.25;3,386.42,609.76,2.52,6.25;3,394.17,605.29,157.79,8.97;3,300.92,616.58,104.45,9.96;3,401.75,621.72,2.52,6.25;3,409.16,616.58,15.40,9.55;3,425.63,615.64,3.63,6.25;3,424.56,621.72,2.52,6.25;3,430.27,616.58,121.70,9.96;3,300.92,629.20,251.05,8.97;3,300.92,640.49,117.85,9.96">Our objective is to find a function f that provides the best estimate of missing values; i.e., the estimate that minimizes the estimation loss. As is usually done, we measure loss as the squared error, so if x d t is an (unobserved) actual measurement (sampled from F) and xd t = f d t (S, X ) is the estimate formed on the basis of observed data, then the squared loss for this particular measurement is L(</p>
                <formula xml:id="formula_2" coords="3,418.78,639.11,89.61,12.77">x d t , x d t ) = (x d t -x d t ) 2 .</formula>
                <p coords="3,511.38,641.16,40.59,8.97;3,300.92,652.44,238.54,9.96">Hence the formal optimization problem is to find a function f to solve:</p>
                <formula xml:id="formula_3" coords="3,313.15,674.04,238.83,68.85">min f E F T t=1 D d=1 (1 -m d t )L(x d t , x d t ) = min f E F T t=1 D d=1 (1 -m d t )(f d t (S, X , Y) -x d t ) 2 . (1)</formula>
                <p coords="4,42.12,65.43,251.05,9.96;4,42.12,77.99,251.05,9.02;4,42.12,90.01,251.06,8.97;4,42.12,101.29,251.06,9.96;4,42.12,113.91,251.06,8.97;4,42.12,125.87,251.05,8.97;4,42.12,137.82,112.90,8.97">Note that the function f we seek depends on the particular d and t, and on the entire array of time stamps and measurementsbut not on labels (which may not be observed). Also note that the formal problem asks to find an f that minimizes the loss with respect to the true distribution. Of course we do not observe the true distribution and cannot compute the true loss, so we will minimize the empirical loss.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="4,72.54,162.40,190.23,9.22;4,122.68,174.36,89.94,9.22">IV. MULTI-DIRECTIONAL RECURRENT NEURAL NETWORKS (M-RNN)</head>
                <p coords="4,52.08,191.59,241.10,9.02;4,42.12,203.55,35.45,9.02;4,77.57,201.99,3.63,6.25;4,77.57,208.07,2.52,6.25;4,84.99,202.94,173.56,9.96;4,254.92,208.07,2.52,6.25;4,262.53,203.60,30.63,8.97;4,42.12,215.56,250.99,8.97;4,42.12,227.47,245.69,9.02;4,287.80,225.90,3.63,6.25;4,287.80,232.28,2.52,6.25;4,42.12,238.80,251.04,9.63;4,42.12,251.37,251.05,9.02;4,42.12,262.71,251.04,9.63;4,42.12,275.28,80.34,9.02;4,122.46,273.72,3.63,6.25;4,122.46,279.81,2.52,6.25;4,131.71,275.28,161.47,9.02;4,42.12,286.62,251.05,9.63;4,42.12,298.58,251.05,9.63;4,42.12,311.20,251.04,8.97;4,42.12,323.15,251.07,8.97;4,42.12,334.44,46.88,9.63;4,85.37,339.57,2.52,6.25;4,94.37,335.11,198.80,8.97;4,42.12,347.06,251.04,8.97;4,42.12,358.35,108.58,9.63;4,147.08,363.49,2.52,6.25;4,154.85,358.35,138.33,9.96;4,42.12,370.98,251.05,8.97;4,42.12,382.93,251.06,8.97;4,42.12,394.88,251.06,8.97;4,42.12,406.84,251.04,8.97;4,42.12,418.79,251.04,8.97;4,42.12,430.75,251.05,8.97;4,42.12,442.71,251.06,8.97;4,42.12,454.66,251.07,8.97;4,42.12,466.62,226.55,8.97;4,274.90,466.62,3.74,8.97">Suppose that stream d was not measured at time stamp t, so that x d t = * . We would like to form an estimate xd t of what the actual measurement would have been. As we have noted, familiar interpolation methods use only the measurements x d t of the fixed data stream d for other time stamps t = t (perhaps both before and after t) -but ignore the information contained in other data streams d = d; familiar imputation methods use only the measurements x d t at the fixed time t for other data streams d = d -but ignores the information contained at other times t = t. Because information is often correlated both within and across data streams, each of these familiar approaches throws away potentially useful information. Our approach forms an estimate xd t using measurements both within the given data stream and across other data streams. In principle, we could try to form the estimate xd t by using all the information in D. However, this would be impractical because it would require learning a number of parameters that is on the order of the square of the number of data streams, and also because it would create a serious danger of over-fitting. Instead, we propose an efficient hierarchical learning framework using a novel RNN architecture that effectively allows us to capture the correlations both within streams and across streams. Our approach limits the number of parameters to be learned to be of the linear order of the number data streams and avoids over-fitting. See Fig. 
                    <ref type="figure" coords="4,271.17,466.62,3.74,8.97" target="#fig_0">1</ref>.
                </p>
                <p coords="4,52.08,478.57,241.08,8.97;4,42.12,490.52,218.05,8.97;4,267.27,490.52,25.90,8.97;4,42.12,502.49,251.06,8.97;4,42.12,514.44,251.07,8.97;4,42.12,526.39,251.03,8.97;4,42.12,538.35,251.03,8.97;4,42.12,550.30,251.07,8.97;4,42.12,562.26,251.07,8.97;4,42.12,574.22,251.06,8.97;4,42.12,586.17,251.06,8.97;4,42.12,598.12,84.15,8.97;4,279.99,598.12,4.45,8.97">Our basic single-imputation M-RNN consists of 2 blocks: an Interpolation block and an Imputation block; see Fig. 
                    <ref type="figure" coords="4,263.54,490.52,3.74,8.97">2</ref>. (Our construction puts the Imputation block after the Interpolation block in order to use the outputs of the Interpolation block to improve the accuracy of the Imputation block; as we discuss later, it would not be useful to put the Interpolation block after the Imputation block.) To produce multiple imputations, we adjoin an additional dropout layer to the basic single-imputation M-RNN. (We defer the details until Section IV-D.) The entire source codes of M-RNN implementation are publicly available in the following link: 
                    <ref type="url" coords="4,128.77,598.12,151.22,8.97" target="http://github.com/jsyoon0823/MRNN/">http://github.com/jsyoon0823/MRNN/</ref>.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="4,42.12,622.71,60.83,9.22">A. Error/Loss</head>
                <p coords="4,52.08,639.99,241.08,8.97;4,42.12,651.95,251.04,8.97;4,42.12,663.90,251.08,8.97;4,42.12,675.86,251.05,8.97;4,42.12,687.77,171.45,9.02;4,213.57,686.20,3.63,6.25;4,213.57,692.28,2.52,6.25;4,220.36,687.81,72.82,8.97;4,42.12,699.10,178.12,9.63;4,216.61,704.24,2.52,6.25;4,223.92,699.71,19.98,9.02;4,243.90,698.15,3.63,6.25;4,243.90,704.24,2.52,6.25;4,251.21,699.76,41.96,8.97;4,42.12,711.68,69.89,9.02;4,112.01,710.11,3.63,6.25;4,112.01,716.19,2.52,6.25;4,118.86,711.06,147.27,9.96;4,266.13,710.11,3.63,6.25;4,266.13,716.19,2.52,6.25;4,270.77,711.73,22.40,8.97;4,42.12,723.01,191.05,9.63;4,229.55,728.15,2.52,6.25;4,237.07,723.68,56.09,8.97;4,42.12,735.58,60.54,9.02;4,102.66,734.03,3.63,6.25;4,102.66,740.10,2.52,6.25;4,107.31,735.63,139.07,8.97">As formalized above in Equation (1), our overall objective is to minimize the error that would be made in estimating missing measurements. Evidently, we cannot estimate the error of a measurement that was not made and hence is truly missing in the dataset. Instead we fix a measurement x d t that was made and is present in the dataset, form an estimate xd t for x d t using only the dataset with x d t removed (which we denote by Dx d t ), and then compute the error between the estimate xd t and the actual measurement x d t . As above, we use the squared error</p>
                <formula xml:id="formula_4" coords="4,248.10,733.59,44.08,12.76">(x d t -x d t ) 2</formula>
                <p coords="4,305.14,66.09,251.05,8.97;4,305.14,77.38,236.24,9.96">as the loss for this particular estimate; as the total loss/error for the entire dataset D we use the mean squared error (MSE):</p>
                <formula xml:id="formula_5" coords="4,310.85,96.82,231.62,30.35">L(x, x) = N n =1 T n t=1 D d=1 m d t (n) × (x d t (n) -x d t (n)) 2 T n t=1 D d=1 m d t (n)</formula>
                <p coords="4,305.14,135.53,251.04,8.97;4,305.14,147.49,82.79,8.97">Note that this is the empirical error, which only utilized actually achievable variables.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="4,305.14,173.00,96.74,9.22">B. Interpolation Block</head>
                <p coords="4,315.10,190.28,241.11,8.97;4,305.14,201.57,8.28,9.96;4,313.41,202.19,242.78,9.80;4,305.14,213.53,38.08,9.63;4,339.59,218.66,2.52,6.25;4,347.54,214.15,208.66,9.02;4,305.14,226.10,78.91,9.02;4,384.05,224.54,3.63,6.25;4,384.05,230.62,2.52,6.25;4,392.81,226.15,77.37,8.97">The Interpolation block constructs an interpolation function Φ d that operates within the d-th stream. To emphasize that the output xd t of the interpolation block depends only on the d-th data stream with x d t removed, we write</p>
                <formula xml:id="formula_6" coords="4,474.92,224.54,78.09,12.33">xd t = Φ d (D d -x d t )</formula>
                <p coords="4,553.01,225.48,3.18,9.55;4,305.14,237.43,35.74,9.96;4,341.15,236.50,3.63,6.25;4,349.52,237.43,206.67,9.96;4,305.14,249.39,43.92,9.96;4,349.34,248.45,3.63,6.25;4,356.19,249.39,15.66,9.96;4,371.85,248.45,3.63,6.25;4,371.85,254.52,2.52,6.25;4,380.07,250.01,149.10,9.02;4,529.17,248.45,3.63,6.25;4,529.17,254.52,2.52,6.25;4,533.81,250.06,22.39,8.97;4,305.14,262.01,251.07,8.97;4,305.14,273.92,251.00,9.02;4,305.14,285.26,8.28,9.96;4,313.41,285.93,242.79,9.75;4,305.14,297.88,168.69,8.97;4,493.29,297.88,62.91,8.97;4,305.14,309.83,251.07,8.97;4,305.14,321.74,251.07,9.02;4,305.14,333.08,251.04,9.96;4,305.14,345.03,251.05,9.63;4,305.14,357.61,29.08,9.02;4,334.22,356.05,3.63,6.25;4,334.22,362.12,2.52,6.25;4,340.91,356.99,131.07,9.63;4,468.36,362.12,2.52,6.25;4,473.00,357.66,83.18,8.97;4,305.14,369.61,251.06,9.75;4,305.14,381.51,251.05,9.02;4,305.14,393.47,251.07,9.02;4,305.14,405.47,228.20,8.97">, where D d is the d-th stream of the entire dataset D, and the notation D dx d t emphasizes that we have removed x d t . It is important to keep in mind that the construction uses only the data from stream d, not the data from other streams. We construct Φ d using a bi-directional recurrent neural network (Bi-RNN). However, unlike a conventional Bi-RNN 
                    <ref type="bibr" coords="4,478.02,297.88,15.27,8.97" target="#b11">[12]</ref>, the timing of inputs into the hidden layer is lagged in the forward direction and advanced in the backward direction: at t, inputs of forward hidden states come from t -1 and inputs of backward hidden states come from t + 1. (This procedure ensures that the actual value x d t is not used in the estimation of xd t .) Note that each data stream uses its own Bi-RNN architecture (Φ d ). The inputs of the Interpolation block consist of the feature vector x, the mask vector m, and the elapsed time vector δ (defined in Section III, and extracted from the original data streams). If we write
                </p>
                <formula xml:id="formula_7" coords="4,305.14,403.86,251.06,24.28">z d t = [x d t , m d t , δ d t ]</formula>
                <p coords="4,354.92,417.38,129.31,9.02;4,484.61,415.82,3.63,6.25;4,484.23,421.90,2.52,6.25;4,491.45,417.43,64.72,8.97;4,305.14,429.39,251.08,8.97;4,305.14,441.34,135.32,8.97">(note that we explicitly include δ d t as the additional input to deal with the irregular sampling procedures) then a more mathematical description is:</p>
                <formula xml:id="formula_8" coords="4,314.15,455.40,233.04,187.37">xd t = g(U d [ -→ h d t ; ← - h d t ] + c d o ) = g( -→ U d -→ h d t + ← - U d ← - h d t + c d o ) -→ h d t = (1 --→ u d t ) • -→ h d t-1 + -→ u d t • q( -→ W d h ( -→ r d t • -→ h d t-1 ) + -→ V d h z d t-1 + -→ c d h ) -→ u d t = γ( -→ W d u -→ h d t-1 + -→ V d u z d t-1 + -→ c d u ) -→ r d t = γ( -→ W d r -→ h d t-1 + -→ V d r z d t-1 + -→ c d r ) ← - h d t = (1 -← - u d t ) • ← - h d t+1 + ← - u d t • q( ← - W d h ( ← -r d t • ← - h d t+1 ) + ← - V d h z d t+1 + ← -c d h ) ← - u d t = γ( ← - W d u ← - h d t+1 + ← - V d u z d t+1 + ← -c d u ) ← -r d t = γ( ← - W d r ← - h d t+1 + ← - V d r z d t+1 + ← -c d r )</formula>
                <p coords="4,305.14,651.95,251.05,8.97;4,305.14,663.85,251.05,9.02;4,305.14,675.85,251.05,8.97;4,305.14,687.81,251.07,8.97;4,305.14,699.10,251.05,9.96;4,305.14,711.73,251.05,8.97;4,305.14,723.68,251.03,8.97;4,305.14,735.63,251.04,8.97;5,37.91,66.09,251.05,8.97;5,37.91,77.99,251.05,9.02;5,37.91,89.34,49.07,9.63;5,83.35,94.47,2.52,6.25;5,90.70,90.01,198.26,8.97;5,37.91,101.91,152.07,9.02;5,189.97,100.35,3.63,6.25;5,189.97,106.43,2.52,6.25;5,194.62,101.96,2.49,8.97">(As can be seen from these equations, we are using a bidirectional GRU.) Here, g, q, γ are activation functions. (In principle, any activation functions, such as Rectified Linear Unit (ReLU), tanh, etc., could be used; here we use ReLU.) The arrows indicate forward/backward direction and • indicates element-wise multiplication. As we have emphasized, in this interpolation block, we are only using/capturing the temporal correlation within each data stream. In particular, the parameters for each data stream are learned separately, and the number of parameters that must be learned is linear in the number of streams D. Note that xd t is not the final output of our M-RNN architecture and is not necessarily an estimate of x d t .</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="5,37.91,125.50,88.58,9.22">C. Imputation Block</head>
                <p coords="5,47.87,142.12,241.09,9.96;5,37.91,154.75,115.39,8.97">The Imputation blocks constructs an imputation function Ψ that operates across streams.</p>
                <p coords="5,156.22,154.75,132.74,8.97;5,37.91,166.04,31.26,9.63;5,65.54,171.18,2.52,6.25;5,72.75,166.65,19.89,9.02;5,92.64,165.09,3.63,6.25;5,92.64,171.18,2.52,6.25;5,99.85,166.65,107.63,9.02;5,207.48,165.09,3.63,6.25;5,207.48,171.18,2.52,6.25;5,214.69,166.70,74.29,8.97;5,38.54,177.99,8.69,9.55;5,43.61,183.13,2.52,6.25;5,51.01,177.99,30.98,9.96;5,82.00,182.16,2.52,6.25;5,87.72,177.99,15.66,9.96;5,103.38,177.05,3.63,6.25;5,103.38,183.13,2.52,6.25;5,108.03,177.99,180.95,9.63;5,37.91,190.57,251.06,9.80;5,37.91,201.90,251.05,10.42;5,37.91,213.85,251.06,9.96;5,37.91,226.48,251.06,8.97;5,37.91,237.77,28.06,9.96;5,65.97,241.93,2.52,6.25;5,72.24,237.77,19.32,9.55;5,91.57,237.77,17.47,10.42;5,109.04,241.93,2.52,6.25;5,112.56,237.77,167.14,9.63">To again emphasize that the estimate xd t for x d t depends on the data with x d t removed, we write xd t = Ψ(D tx d t ); again, keep in mind that now we are using only data at time stamp s t , not data from other time stamps. (D t represents the t-th time stamp of the entire dataset D.) We construct the function Ψ to be independent of t, so we use fully connected layers; see the Imputation component of Fig 2. If we write z t = [x t , m t ] then a more mathematical description is:</p>
                <formula xml:id="formula_9" coords="5,70.34,255.53,186.74,10.55">xt = σ(W h t + α) h t = φ(U x t + V z t + β)</formula>
                <p coords="5,37.91,274.19,251.07,9.02;5,37.91,286.15,251.04,9.02;5,37.91,298.10,251.07,9.02;5,37.91,310.06,5.70,8.93;5,43.61,308.50,3.63,6.25;5,43.61,314.57,2.52,6.25;5,50.74,309.44,89.02,9.63;5,136.13,314.57,2.52,6.25;5,140.78,310.11,2.49,8.97;5,47.87,321.39,112.68,9.96;5,160.55,325.56,3.63,6.25;5,165.20,321.39,4.98,9.96;5,170.17,320.45,5.77,6.25;5,170.17,326.88,13.76,6.25;5,189.22,321.39,99.75,9.96;5,37.91,334.01,251.04,8.97;5,37.91,345.97,146.80,8.97">where σ, φ are activation functions. It is important to keep in mind that the diagonal entries of U are zero and the off-diagonal entries of W are zero (i.e., W is diagonal) so that we do not use x d t in the estimation of xd t . We learn the functions {Φ d } D d=1 and Ψ jointly using the stacked networks of Bi-RNN and Fully Connected (FC) layers, using MSE as the objective function.</p>
                <formula xml:id="formula_10" coords="5,48.39,361.31,240.58,57.02">Ψ * , {Φ * d } D d=1 = arg min Φ d ,Ψ L Ψ x d t , Φ d {x d τ , m d τ , δ d τ } T τ =1 , m d t D d=1 T t=1 , x
                    <label>(2)</label>
                </formula>
                <p coords="5,47.87,426.14,241.10,9.63;5,37.91,438.76,198.63,8.97">Note that xt is the output of the interpolation block, and xt is the final output of the entire M-RNN architecture.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="5,37.91,462.32,103.31,9.22">D. Multiple Imputations</head>
                <p coords="5,47.87,479.61,241.11,8.97;5,37.91,491.56,251.07,8.97;5,37.91,503.51,251.07,8.97;5,37.91,515.47,251.06,8.97;5,37.91,527.42,232.97,8.97;5,285.45,527.42,3.53,8.97;5,37.91,539.38,251.07,8.97;5,37.91,551.34,84.00,8.97;5,145.45,551.34,143.49,8.97;5,37.91,563.29,111.62,8.97;5,168.24,563.29,120.73,8.97;5,37.91,575.25,251.06,8.97;5,37.91,586.54,251.02,9.96;5,37.91,599.16,251.06,8.97;5,37.91,611.07,251.06,9.02;5,37.91,623.07,251.05,8.97;5,37.91,635.02,251.07,8.97;5,37.91,646.31,5.73,9.96;5,43.63,646.31,245.32,10.42;5,37.91,658.93,251.06,8.97;5,37.91,670.88,251.06,8.97;5,37.91,682.84,251.05,8.97;5,37.91,694.75,189.88,9.02">It is well-understood that to account for the uncertainty in estimating missing values, it is useful to produce multiple estimates and generate multiple imputed datasets. These multiple imputed datasets can each be analyzed using standard methods and the results can be combined using Rubin's rule 
                    <ref type="bibr" coords="5,274.87,527.42,10.58,8.97" target="#b2">[3]</ref>. In our case, we generate multiple imputed datasets using the well-known Dropout 
                    <ref type="bibr" coords="5,125.39,551.34,16.59,8.97" target="#b17">[18]</ref> approach driven from the Bayesian Neural Network framework 
                    <ref type="bibr" coords="5,152.75,563.29,15.49,8.97" target="#b30">[31]</ref>: we randomly select neurons in the fully connected layers and delete those neurons and all their connections. (The dropout probability p ∈ (0, 1) is a hyperparameter to be chosen; the neurons to be dropped are chosen according to the Bernoulli distribution with parameter p.) In the training stage, we conduct joint optimization (Equation (2)) using the dropout process. We then generate multiple outputs o t by sampling different dropout vectors R from the Bernoulli distributions. This yields multiple imputations (MI). (To construct a single imputation (SI) we proceed in precisely the same way but set the dropout probability to 0. For comparisons, we normalize the final output by multiplying by p.)
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="5,37.91,718.34,219.78,9.22">E. Overall Structure and Computation Complexity</head>
                <p coords="5,47.87,735.63,241.08,8.97;5,37.91,747.59,251.04,8.97;5,300.92,66.10,251.05,8.97;5,300.92,78.05,251.05,8.97;5,300.92,90.01,251.07,8.97;5,300.92,101.96,251.07,8.97;5,300.92,113.91,251.06,8.97;5,300.92,125.87,251.05,8.97;5,300.92,137.83,126.07,8.97;5,443.71,137.83,108.27,8.97;5,300.92,149.78,34.03,8.97">We refer to the entire structure above as a Multi-directional Recurrent Neural Network (M-RNN). We use the notations M-RNN (MI) and M-RNN (SI) to clarify whether we are producing multiple or single imputations. The entire training times for both M-RNN (MI) and M-RNN (SI) are less than 2 hours for all 6 datasets (described in Section V) on a computer with Intel Core i7-4770 (3.4 GHz) CPU with 32 GB RAM. With the same machine, the entire training time for Multiple Imputation with Chained Equation (MICE) 
                    <ref type="bibr" coords="5,429.54,137.83,11.61,8.97" target="#b4">[5]</ref> is around 11 hours for all 6 datasets.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="5,398.99,174.35,54.94,9.22">V. DATASETS</head>
                <p coords="5,310.89,191.63,241.08,8.97;5,300.92,203.59,54.52,8.97;5,360.99,203.59,153.76,8.97">We use five datasets, the characteristics of which are summarized in Table 
                    <ref type="table" coords="5,357.95,203.59,3.05,8.97">I</ref>: more detailed descriptions are below.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="5,300.92,228.16,55.85,9.22">A. MIMIC-III</head>
                <p coords="5,310.89,245.44,92.64,8.97;5,425.32,245.44,126.67,8.97;5,300.92,257.40,251.08,8.97;5,300.92,269.36,251.04,8.97;5,300.92,281.31,251.04,8.97;5,300.92,293.26,251.03,8.97;5,300.92,305.22,251.04,8.97;5,300.92,317.17,251.04,8.97;5,300.92,329.13,251.06,8.97;5,300.92,341.09,251.07,8.97;5,300.92,353.04,251.08,8.97;5,300.92,365.00,251.03,8.97;5,300.92,376.95,251.06,8.97;5,300.92,388.91,251.05,8.97;5,300.92,400.87,251.04,8.97;5,300.92,412.82,251.06,8.97;5,300.92,424.72,251.04,9.02;5,300.92,436.68,222.49,9.80;5,527.17,436.06,24.82,9.63;5,300.92,448.63,250.06,9.80;5,300.92,460.58,24.28,9.80;5,328.96,459.97,59.21,9.63">The dataset MIMIC-III 
                    <ref type="bibr" coords="5,406.11,245.44,16.59,8.97" target="#b18">[19]</ref> contains data for patients monitored in intensive care units (ICUs) of various hospitals. Within the entire MIMIC-III dataset, we only used the data for the 23,160 patients whose measurements are recorded by Metavision (post 2008). We used the 20 vital signs (e.g., heart rate, respiratory rate, blood pressures, etc.) and 20 lab tests (e.g., creatinine, chloride, etc.) whose missing rates are the least. For these patients we have 40 physiological data streams in all. Vital signs were sampled roughly every hour; lab tests were sampled roughly every 12 hours. Each patient was followed until either death (1,320 patients (5.7%)) or discharge from ICU (21,840 patients (94.3%)). Note that because lab tests are sampled only 1/12 as often as vital signs, in effect 11/12 of lab test data is missing -even if every lab test for every patient was actually conducted and recorded. For the purpose of prediction, we take the goal as predicting at each time t whether the patient will die within the next 24 hours. Hence we assign the label y t = 1 if the patient actually died within the 24 hrs following the time s t and y t = 0 otherwise.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="5,300.92,485.21,71.76,9.22">B. Deterioration</head>
                <p coords="5,310.89,502.49,99.22,8.97;5,433.47,502.49,118.49,8.97;5,300.92,514.45,251.05,8.97;5,300.92,526.41,251.05,8.97;5,300.92,538.36,251.07,8.97;5,300.92,550.31,251.04,8.97;5,300.92,562.27,251.08,8.97;5,300.92,574.22,251.08,8.97;5,300.92,586.18,251.05,8.97;5,300.92,598.14,251.03,8.97;5,300.92,610.09,251.06,8.97;5,300.92,622.05,251.05,8.97;5,300.92,634.00,251.06,8.97;5,300.92,645.95,251.04,8.97;5,300.92,657.87,251.05,9.80;5,300.92,669.87,251.06,8.97;5,300.92,681.77,113.42,9.80;5,418.09,681.15,133.90,9.63;5,300.92,693.73,179.28,9.80;5,483.96,693.11,59.21,9.63">The dataset described in 
                    <ref type="bibr" coords="5,413.49,502.49,16.59,8.97" target="#b19">[20]</ref> provides records for a cohort of 6,094 patients who were followed for potential clinical deterioration while hospitalized. Patients were monitored for 28 vital signs (heart rate, blood pressure, etc.) and 10 lab tests (creatinine, hemoglobin, etc.) so there are 38 physiological data streams in all. Vital signs were sampled roughly every 4 hours; lab tests were sampled roughly every 24 hours. Each patient was followed until either admission to ICU (306 patients (5.0%)) or discharge from hospital (5,788 patients (95.0%)). Again, because lab tests are sampled only 1/6 as often as vital signs, in effect 5/6 of lab test data is missing -even if every lab test for every patient was actually conducted and recorded. For the purpose of prediction, we take the goal as predicting at each time stamp s t whether the patient will be admitted to the ICU (experienced clinical deterioration) within the next 24 hours. Hence we assign the label y t = 1 if the patient was admitted to the ICU within the following 24 hours and y t = 0 otherwise.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="5,300.93,718.34,148.38,9.22">C. UNOS-Heart and UNOS-Lung</head>
                <p coords="5,310.89,735.63,241.08,8.97;5,300.93,747.59,251.02,8.97;5,310.89,735.63,241.08,8.97;5,300.93,747.59,100.55,8.97;5,511.00,747.59,40.95,8.97">The UNOS (United Network for Organ Transplantation) dataset (available at 
                    <ref type="url" coords="5,404.16,747.59,106.83,8.97" target="https://www.unos.org/data/">https://www.unos.org/data/</ref>) provides
                </p>
                <p coords="6,284.80,65.71,28.72,7.37;6,153.43,74.67,291.45,7.37">TABLE I SUMMARY OF THE DATASETS (CONT: CONTINUOUS, CAT: CATEGORICAL, AVG: AVERAGE)</p>
                <p coords="6,42.12,238.02,251.01,8.97;6,42.12,249.97,251.05,8.97;6,42.12,261.92,251.06,8.97;6,42.12,273.88,251.05,8.97;6,42.12,285.84,251.06,8.97;6,42.12,297.80,251.07,8.97;6,42.12,309.75,251.06,8.97;6,42.12,321.70,251.04,8.97;6,42.12,333.66,251.07,8.97;6,42.12,345.62,251.05,8.97;6,42.12,357.57,251.06,8.97;6,42.12,369.53,251.05,8.97;6,42.12,381.48,251.04,8.97;6,42.12,393.43,251.06,8.97;6,42.12,405.39,251.04,8.97;6,42.12,417.35,251.03,8.97;6,42.12,429.30,251.04,8.97;6,42.12,441.26,251.02,8.97;6,42.12,453.21,251.06,8.97;6,42.12,465.16,251.03,8.97;6,42.12,477.12,251.04,8.97;6,42.12,489.03,251.06,9.80;6,42.12,501.04,251.06,8.97;6,42.12,512.94,7.40,9.80;6,53.28,512.32,239.91,9.63;6,42.12,524.89,7.40,9.80;6,53.28,524.27,59.21,9.63">yearly follow-up information for the entire U.S. cohort of 69,205 patients who received heart transplants and 32,986 patients who received lung transplants during the period 1985-2015. We view patients in the dataset as described by a total of 34 clinical features. (In fact, a total of 232 features are recorded in the UNOS dataset, but many features are not recorded for most patients. We therefore excluded the 198 features for which missing rates were higher than 80%; this is in keeping with standard medical statistical practice.) For each patient, a number of yearly followups are recorded; the smallest number of yearly follow-ups is 1, the largest is 26; the median number of follow-ups for heart transplantation is 6 and the median number of follow-ups for lung transplantation is 4. (In the main text, we focus entirely on features of the patient, ignoring features of the donor. We do this for two reasons: (i) the features of the patient change over time but the features of the donor do not (because the donor is dead); (ii) the relevant features of the donor appear to be largely captured in the time series measurements of the patient. However, as we show in the Appendix, taking features of the donor into account seems to make little difference for either imputation or prediction.) For the purpose of prediction, we take the goal in each case as predicting at each follow up time s t whether the patient will be dead one year later. Hence we assign the label y t = 1 if the patient actually died within the following year and y t = 0 otherwise.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="6,42.12,550.97,50.72,9.22">D. Biobank</head>
                <p coords="6,52.08,568.26,240.95,8.97;6,42.12,580.22,251.04,8.97;6,42.12,592.17,251.07,8.97;6,42.12,604.12,250.94,8.97;6,42.12,616.08,251.07,8.97;6,42.12,628.04,251.05,8.97;6,42.12,639.99,251.03,8.97;6,42.12,651.95,251.07,8.97;6,42.12,663.90,251.08,8.97;6,42.12,675.85,251.03,8.97;6,42.12,687.81,251.07,8.97;6,42.12,699.77,251.03,8.97;6,42.12,711.73,251.06,8.97;6,42.12,723.68,251.05,8.97;6,42.12,735.63,251.07,8.97;6,42.12,747.59,251.05,8.97">We used the UK Biobank dataset gathered from 21 assessment centers across England, Wales, and Scotland using standardized procedures from 2007 to 2014. (The UK Biobank protocol is available online.) UK Biobank recorded various patient information including baseline measurements, physical measurements, and evaluations of biological samples. For this paper, we excluded all the variables that were missing for more than 80% of the participants and all the static measurements and only used the 113 longitudinal measurements. Of the 4,096 total patients, we used only the data for the 3,902 patients who missed no admissions to assessment centers (and hence were assessed the maximum number of times, which was three) and for whom there are no missing measurements of these 113 variables; thus we have a complete dataset. A complete dataset is required for the congeniality experiments described in Section VI-G, and we can extract a complete dataset from the UK Biobank dataset.</p>
                <p coords="6,305.14,238.02,251.07,8.97;6,305.14,249.98,251.04,8.97;6,305.14,261.93,251.02,8.97;6,305.14,273.89,251.06,8.97;6,305.14,285.84,251.06,8.97;6,305.14,297.75,29.26,9.79;6,338.15,297.13,153.79,10.42;6,495.68,297.13,59.21,9.63">In the other datasets we consider, there is no single patient for whom the information is complete, and so a complete dataset cannot be extracted and these datasets cannot be used for congeniality experiments. For the purpose of prediction we take the goal to be the correct prediction of diabetes, so we assign the label y t = 1 if diabetes is diagnosed at t and y t = 0 otherwise.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="6,363.82,329.80,133.70,9.22">VI. RESULTS AND DISCUSSIONS</head>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="6,305.14,347.73,206.12,9.22">A. Imputation Accuracy on the Given Datasets</head>
                <p coords="6,315.10,365.02,240.96,8.97;6,305.14,376.98,251.02,8.97;6,305.14,388.93,251.06,8.97;6,305.14,400.88,251.03,8.97;6,305.14,412.85,162.05,8.97;6,486.30,412.85,4.09,8.97;6,506.77,412.85,49.44,8.97;6,305.14,424.80,77.73,8.97;6,396.12,424.80,30.95,8.97;6,440.32,424.80,50.17,8.97;6,503.75,424.80,20.99,8.97;6,537.98,424.80,18.22,8.97;6,305.14,436.75,122.73,8.97;6,440.30,436.75,115.83,8.97;6,305.14,448.71,46.74,8.97;6,369.83,448.71,186.37,8.97;6,305.14,460.66,29.88,8.97;6,352.24,460.66,203.93,8.97;6,305.14,472.61,251.06,8.97;6,305.14,484.58,251.06,8.97;6,305.14,496.53,251.04,8.97;6,305.14,508.49,21.87,8.97;6,341.80,508.49,214.37,8.97;6,305.14,520.44,251.06,8.97;6,305.14,532.39,251.06,8.97;6,305.14,544.35,251.05,8.97;6,305.14,556.31,251.05,8.97;6,305.14,568.26,251.05,8.97;6,305.14,580.22,251.05,8.97;6,305.14,592.17,117.43,8.97">We begin by comparing the performance of our method (using both multiple imputations and single imputation) on the given datasets against 11 benchmarks with respect to the accuracy of imputing missing values. The benchmarks against which we compare are: the algorithms proposed in 
                    <ref type="bibr" coords="6,469.92,412.85,16.37,8.97" target="#b21">[22]</ref>- 
                    <ref type="bibr" coords="6,490.39,412.85,16.37,8.97" target="#b23">[24]</ref>; Spline and Cubic Interpolation 
                    <ref type="bibr" coords="6,385.33,424.80,10.79,8.97" target="#b0">[1]</ref>; MICE 
                    <ref type="bibr" coords="6,429.53,424.80,10.79,8.97" target="#b4">[5]</ref>; MissForest 
                    <ref type="bibr" coords="6,492.95,424.80,10.79,8.97" target="#b5">[6]</ref>; EM 
                    <ref type="bibr" coords="6,527.19,424.80,10.79,8.97" target="#b3">[4]</ref>; the matrix completion algorithm of 
                    <ref type="bibr" coords="6,429.50,436.75,10.79,8.97" target="#b6">[7]</ref>; the Auto-Encoder algorithm proposed in 
                    <ref type="bibr" coords="6,354.34,448.71,15.49,8.97" target="#b31">[32]</ref>; and the Markov chain Monte Carlo (MCMC) method 
                    <ref type="bibr" coords="6,336.97,460.66,15.26,8.97" target="#b32">[33]</ref>. (The details of the implementations for the various benchmarks are presented in the Appendix.) As is common, we use root mean squared error (RMSE) as the measure of performance. In each experiment, we use 5-fold cross-validation. Table 
                    <ref type="table" coords="6,331.09,508.49,6.64,8.97">II</ref> shows the mean RSME for our method and benchmarks, and the percentage improvement of RMSE for M-RNN (MI) over the benchmarks. (Note that we are unable to provide results for the EM algorithm on the UNOS-Heart and UNOS-Lung datasets because -at least for the implementation we use -the EM algorithm requires at least one patient for whom data is complete, and the UNOS-Heart and UNOS-Lung datasets do not contain any such patient.)
                </p>
                <p coords="6,315.10,604.12,98.17,8.97;6,422.67,604.12,133.51,8.97;6,305.14,616.09,251.06,8.97;6,305.14,628.04,251.09,8.97;6,305.14,639.99,251.05,8.97;6,305.14,651.95,251.08,8.97;6,305.14,663.24,251.06,9.63;6,305.14,675.86,251.05,8.97;6,305.14,687.82,251.06,8.97;6,305.14,699.77,251.04,8.97;6,305.14,711.73,251.04,8.97;6,305.14,723.68,249.56,8.97">As can be seen in Table 
                    <ref type="table" coords="6,416.58,604.12,6.09,8.97">II</ref>, M-RNN achieves better performance (smaller RMSE) than all of the benchmarks on all of the datasets (for all comparisons are possible). With a single exception (the comparison with MissForest on the UNOS-Lung dataset) the performance improvements are statistically significant at the 95% level (i.e., p &lt; 0.05), and many of the improvements are very large. For instance, for the Deterioration dataset, M-RNN using multiple imputations achieves RMSE of 0.0105 (95% CI: 0.0071-0.0138), while the best benchmark (Spline interpolation) achieves RMSE of 0.0215 (95% CI: 0.0178-0.0255); this represents an improvement of 51.2%.
                </p>
                <p coords="6,315.10,735.63,241.09,8.97;6,305.14,747.59,251.04,8.97">The performance comparisons across datasets are revealing, if not necessarily surprising. The interpolation benchmarks</p>
                <p coords="7,279.48,65.71,30.93,7.37;7,193.42,74.67,203.06,7.37">TABLE II PERFORMANCE COMPARISON FOR MISSING DATA ESTIMATION</p>
                <p coords="7,37.91,291.48,251.05,8.97;7,37.91,303.44,251.05,8.97;7,37.91,315.39,251.04,8.97;7,37.91,327.34,121.51,8.97;7,165.95,327.34,122.99,8.97;7,37.91,339.31,251.03,8.97;7,37.91,351.26,251.05,8.97;7,37.91,363.21,251.04,8.97;7,37.91,375.17,251.05,8.97;7,37.91,387.12,251.05,8.97;7,37.91,399.08,251.04,8.97;7,37.91,411.04,251.04,8.97;7,37.91,422.99,251.03,8.97;7,37.91,434.94,251.06,8.97;7,37.91,446.90,251.05,8.97;7,37.91,458.85,251.06,8.97;7,37.91,470.81,251.06,8.97;7,37.91,482.77,251.06,8.97;7,37.91,494.72,251.05,8.97;7,37.91,506.68,251.08,8.97;7,37.91,518.63,135.59,8.97">(such as Spline, Cubic and RNN-based methods) work best on datasets, such as MIMIC-III and Deterioration, for which measurements were more frequent (and more highly correlated within each stream (see Table 
                    <ref type="table" coords="7,162.77,327.34,3.18,8.97">I</ref>)); the imputation benchmarks work best on datasets, such as UNOS-Heart and UNOS-Lung, for which measurements were less frequent but for which there were many streams of data (many dimensions). The improvement of our method over all benchmarks is larger for the MIMIC-III and Deterioration datasets because those datasets have many streams of frequently sampled data, so that our method gains a great deal from exploiting both the correlations within each data stream and the correlations across data streams. Conversely, the improvement of our method is smaller for the UNOS-Heart and UNOS-Lung datasets, because streams in those datasets are infrequently sampled to that there is less to be gained by exploiting the correlations within data streams. (The performances of the benchmarks for the Biobank dataset are mixed, and don't quite fit this same pattern, perhaps because Biobank is a small dataset (less than 4,000 patients with complete temporal data streams)).
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="7,49.86,530.95,206.22,8.29">1) Multiple Imputations vs. Single Imputation:</head>
                <p coords="7,261.05,530.58,27.92,8.97;7,37.91,542.54,251.05,8.97;7,37.91,554.50,251.08,8.97;7,37.91,566.45,251.05,8.97;7,37.91,578.41,25.55,8.97;7,77.51,578.41,211.46,8.97;7,37.91,590.36,250.96,8.97;7,37.91,602.31,251.07,8.97;7,37.91,614.27,251.07,8.97;7,37.91,626.23,46.03,8.97;7,101.89,626.23,187.07,8.97;7,37.91,638.19,162.15,8.97">As we have noted, the purpose of conducting multiple imputations is to reduce uncertainty/shrink confidence intervals (rather than to improve average performance). As is illustrated in the box-plot in Fig. 
                    <ref type="figure" coords="7,65.49,578.41,12.03,8.97" target="#fig_1">3(a</ref>) which shows the comparison of M-RNN with multiple imputations and M-RNN with a single imputation against the best benchmark (Cubic interpolation) on the MIMIC-III dataset, our multiple imputations do achieve this purpose. (For discussion of Fig. 
                    <ref type="figure" coords="7,86.62,626.23,15.27,8.97" target="#fig_1">3(b)</ref>, which illustrates the corresponding reduction in uncertainty for prediction, see below).
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="7,49.86,650.51,239.13,8.29">2) Combining Models of Interpolation and Imputation:</head>
                <p coords="7,37.91,662.09,251.06,8.97;7,37.91,674.05,251.05,8.97;7,37.91,686.00,251.07,8.97;7,37.91,697.96,251.07,8.97;7,37.91,709.92,251.06,8.97;7,37.91,721.87,251.04,8.97;7,37.91,733.82,251.04,8.97;7,300.92,520.44,251.07,8.97;7,300.92,532.39,251.08,8.97;7,300.92,544.35,251.04,8.97;7,300.92,556.30,251.05,8.97;7,300.92,568.26,251.06,8.97;7,300.92,580.22,251.05,8.97;7,300.92,592.17,251.06,8.97;7,300.92,604.12,251.07,8.97;7,300.92,616.08,249.06,8.97">As we have already discussed, standard interpolation algorithms cannot capture the patterns across streams and standard imputation algorithms cannot capture the patterns within the streams. However, it is possible to combine a standard interpolation algorithm and standard imputation algorithm in an attempt to capture both patterns, and it might be thought that such a combination would be a fairer benchmark against which to compare our method. To put this idea to the test, we create a family of "joint algorithms" by first using an interpolation algorithm to interpolate the missing values, and then using the interpolated values as the initial points of an imputation algorithm to provide final imputed values. For this exercise, we use two standard interpolation methods (Cubic and Spline), and two standard imputation methods (MICE and MissForest) so that we have 4 interpolation-imputation combination models: Cubic + MICE, Cubic + MissForest, Spline + MICE, and Spline + MissForest.</p>
                <p coords="7,310.89,628.04,37.97,8.97;7,368.84,628.04,183.15,8.97;7,300.92,639.99,251.06,8.97;7,300.92,651.95,251.04,8.97;7,300.92,663.90,251.05,8.97;7,300.92,675.85,251.06,8.97;7,300.92,687.81,251.06,8.97;7,300.92,699.77,251.04,8.97;7,300.92,711.73,251.04,8.97;7,300.92,723.68,251.04,8.97;7,300.92,735.63,124.50,8.97">As Table 
                    <ref type="table" coords="7,353.88,628.04,9.95,8.97">III</ref> shows, however, the performances of these interpolation-imputation combination models are very similar to those of the performance of the simple imputation model that is used. Indeed, the largest RMSE performance improvement is only 0.0018. The reason for this is that imputation methods use algorithms that operate iteratively until they converge, so that their performance is rather robust to the initialization. Hence, although the interpolation part of the joint models captures some of the inter-stream information, the iterative imputation part ignores most of what is captured.
                </p>
                <p coords="8,282.58,65.71,33.15,7.37;8,162.61,74.67,273.09,7.37;8,150.63,182.11,34.04,7.37;8,118.49,191.08,98.30,7.37;8,73.33,200.04,188.62,7.37">TABLE III PERFORMANCE COMPARISON FOR JOINT INTERPOLATION/IMPUTATION ALGORITHMS TABLE IV SOURCE OF GAIN OF M-RNN (PERFORMANCE DEGRADATION FROM ORIGINAL M-RNN)</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="8,42.12,385.36,86.11,9.22">B. Source of Gains</head>
                <p coords="8,52.08,402.65,83.85,8.97;8,142.80,402.65,150.38,8.97;8,42.12,414.60,251.06,8.97;8,42.12,426.56,251.05,8.97;8,42.12,438.51,251.04,8.97;8,42.12,450.46,226.68,8.97;8,279.97,450.46,4.33,8.97">As illustrated in Fig. 
                    <ref type="figure" coords="8,139.07,402.65,3.74,8.97">2</ref>, our M-RNN consists of an Interpolation block and an Imputation block. To understand where the gains of our approach come from, we compare the performance of that is achieved when we use only the Interpolation block or only the Imputation block; the results are shown in Table 
                    <ref type="table" coords="8,271.30,450.46,8.67,8.97">IV</ref>.
                </p>
                <p coords="8,52.08,462.43,241.06,8.97;8,42.12,474.38,251.04,8.97;8,42.12,486.33,251.04,8.97;8,42.12,498.29,251.06,8.97;8,42.12,510.24,251.04,8.97;8,42.12,522.20,251.03,8.97;8,42.12,534.16,251.04,8.97;8,42.12,546.11,250.99,8.97;8,42.12,558.07,234.46,8.97;8,288.85,558.07,4.33,8.97;8,42.12,570.02,226.74,8.97">The Interpolation block is intended to exploit the correlations within each data stream and the Imputation block is intended to exploit the correlations across streams, so it is to be expected that the largest gains of our M-RNN method should come from the Interpolation block for the datasets (MIMIC-III and Deterioration) which are frequently sampled and have large temporal correlations, and should come from the Imputation block for the datasets (UNOS-Heart and UNOS-Lung) which are infrequently sampled but have many data streams. As shown in Table 
                    <ref type="table" coords="8,280.18,558.07,8.67,8.97">IV</ref>, these intuitions are indeed supported by the experiments.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="8,42.12,595.81,117.16,9.22">C. Additional Experiments</head>
                <p coords="8,52.08,613.10,241.09,8.97;8,42.12,625.05,251.02,8.97;8,42.12,637.00,251.04,8.97;8,42.12,648.96,251.06,8.97;8,42.12,660.91,251.06,8.97;8,42.12,672.87,251.03,8.97;8,42.12,684.83,251.07,8.97;8,42.12,696.78,251.06,8.97;8,42.12,708.73,251.05,8.97;8,42.12,720.69,251.05,8.97;8,42.12,732.64,251.02,8.97;8,305.13,182.50,251.06,8.97;8,305.13,194.46,172.21,8.97;8,483.71,194.46,72.48,8.97;8,305.14,206.42,251.05,8.97;8,305.14,218.37,251.04,8.97;8,305.14,230.33,246.36,8.97">The experiments we have described above demonstrate that our method significantly outperforms a wide variety of benchmarks for the imputation of missing data on five somewhat representative datasets. However it is natural to ask how our method would compare in other circumstances. To get some understanding of this, we conducted four sets of experiments based on the MIMIC-III dataset: increasing the amount of missing data, reducing the number of data streams, reducing the number of samples, and reducing the number of measurements per patient. Within each set of experiments, we conducted 10 trials for each value of the parameter being studied (e.g., amount of missing data), and we report the average over these 10 trials. The results are described below and in Fig. 
                    <ref type="figure" coords="8,479.98,194.46,3.74,8.97" target="#fig_2">4</ref>. Although the results of these experiments are extremely suggestive, we caution the reader that these are only a specific set of experiments and that one should be careful about drawing general conclusions.
                </p>
                <p coords="8,317.09,242.64,144.20,8.29;8,480.07,242.28,76.12,8.97;8,305.14,254.24,251.08,8.97;8,305.14,266.19,251.05,8.97;8,305.14,278.14,251.06,8.97;8,305.14,290.10,251.06,8.97;8,305.14,302.06,251.03,8.97;8,305.14,314.01,251.06,8.97;8,305.14,325.97,251.04,8.97;8,305.14,337.92,251.07,8.97;8,305.14,349.87,66.62,8.97;8,377.41,349.87,178.72,8.97;8,305.14,361.83,251.06,8.97;8,305.14,373.79,251.07,8.97;8,305.14,385.74,251.04,8.97;8,305.14,397.70,251.02,8.97;8,305.14,409.65,251.03,8.97;8,305.14,421.60,67.49,8.97">1) Amount of Missing Data (Fig. 
                    <ref type="figure" coords="8,465.46,242.64,14.62,8.29" target="#fig_2">4(a)</ref>): To evaluate the performance of M-RNN in comparison to benchmarks in settings with more missing data, we constructed sub-samples of the MIMIC-III dataset by randomly removing 10%, 20%, 30%, 40%, 50% of the actual data and carrying out the same estimation exercise as above on the smaller datasets that remain. (Recall that in the original MIMIC-III dataset, 75% of the data is already missing; hence removing 50% of the data present leads to an artificial dataset in which 87.5% of the data is missing.) The graph in Fig. 
                    <ref type="figure" coords="8,373.40,349.87,4.01,8.97" target="#fig_2">4</ref>(a) shows the performance of M-RNN against the best benchmarks of each type for these smaller datasets. As can be seen, M-RNN continues to substantially outperform the benchmarks. Note that as the amount of missing data increases the improvement of M-RNN over the imputation benchmark(s) increases, but the improvement over the interpolation benchmarks decreases.
                </p>
                <p coords="8,317.09,433.93,137.82,8.29;8,471.92,433.56,84.27,8.97;8,305.14,445.51,251.05,8.97;8,305.14,457.48,251.07,8.97;8,305.14,469.43,251.07,8.97;8,305.14,481.38,251.04,8.97;8,305.14,493.34,251.06,8.97;8,305.14,504.63,251.05,9.63;8,305.14,516.58,251.05,9.63;8,305.14,529.21,251.06,8.97;8,305.14,541.16,251.06,8.97;8,305.14,553.11,251.01,8.97;8,305.14,565.07,198.29,8.97;8,529.05,565.07,27.15,8.97;8,305.14,577.02,251.06,8.97;8,305.14,588.98,251.07,8.97;8,305.14,600.94,250.68,8.97">2) Number of Data Streams (Fig. 
                    <ref type="figure" coords="8,456.97,433.93,14.95,8.29" target="#fig_2">4(b)</ref>): As we have noted, typical medical datasets contain many data streams (many feature dimensions). To evaluate the performance of M-RNN in comparison to benchmarks in settings with fewer data streams, we conducted experiments in which we reduced the number of data streams (feature dimensions) of MIMIC-III. In the original MIMIC-III dataset the number of data streams is D = 40; we conducted experiments with D = 3, 5, 7, 10, 15, 20 data streams. (In each case, we conducted 10 trials in which we selected data streams at random; we report the average of these 10 trials.) As expected, the performance of M-RNN degrades when there are fewer data streams, but as Fig. 
                    <ref type="figure" coords="8,507.93,565.07,16.59,8.97" target="#fig_2">4(b)</ref> shows, M-RNN still outperforms the benchmarks. (Note that interpolation methods are insensitive to the number of data streams because they operate only within each data stream separately.)
                </p>
                <p coords="8,317.09,613.25,121.43,8.29;8,456.40,613.25,3.65,8.29;8,468.68,612.89,87.52,8.97;8,305.14,624.18,251.05,9.63;8,305.14,636.80,251.02,8.97;8,305.14,648.75,251.07,8.97;8,305.14,660.71,251.06,8.97;8,305.14,672.00,250.89,9.63;8,305.14,684.62,251.05,8.97;8,305.14,696.58,26.56,8.97;8,338.20,696.58,218.00,8.97;8,305.14,708.53,251.03,8.97;8,305.14,720.48,251.05,8.97;8,305.14,732.44,251.05,8.97;9,275.94,376.75,13.03,8.97;9,37.91,388.71,251.06,8.97;9,37.91,400.67,250.95,8.97;9,37.91,412.62,251.07,8.97;9,37.91,424.57,251.05,8.97;9,37.91,436.53,251.03,8.97;9,37.91,447.82,250.98,9.63;9,37.91,460.44,232.87,8.97;9,37.91,472.40,251.05,8.97;9,37.91,484.35,251.06,8.97;9,37.91,496.30,251.05,8.97;9,37.91,508.26,177.69,8.97">3) Number of Samples (Fig. 
                    <ref type="figure" coords="8,441.79,613.25,14.62,8.29" target="#fig_2">4(c)</ref>) 
                    <ref type="bibr" coords="8,460.06,613.25,3.65,8.29">:</ref> The original MIMIC-III dataset has N = 23, 160 samples (patients). To understand the performance of M-RNN in comparison to benchmarks in settings with fewer samples, we conducted experiments in which we used only subsets of all patients (samples) of sizes N = 500, 1000, 2000, 4000, 8000, 16000. Because M-RNN has to learn many parameters, it should come as no surprise that, as Fig. 
                    <ref type="figure" coords="8,334.19,696.58,4.01,8.97" target="#fig_2">4</ref>(c) shows, the performance of M-RNN degrades badly -and indeed is worse than that of (some) other benchmarks -when the number of samples is too small, but M-RNN outperforms all the benchmarks as soon as the number of training We have already noted that, in our datasets, MIMIC-III and Deterioration have many (relatively frequent) measurements per patient, while the other datasets have only a few (and infrequent) measurements per patient and that this leads to differences in performance of M-RNN. To further explore this effect, we created subsets of the MIMIC-III dataset with T = 1, 3, 5, 10, 20, 30 measurements per patient. As might be expected, and as Fig. 
                    <ref type="figure" coords="9,272.30,460.44,16.59,8.97" target="#fig_2">4(d)</ref> shows, having fewer measurements per patient degrades the performance of interpolation-based algorithms but has little effect on pure imputation-based methods; the performance of M-RNN is also degraded, but to a much lesser extent.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="9,37.91,539.02,102.66,9.22">D. Prediction Accuracy</head>
                <p coords="9,47.87,556.31,241.12,8.97;9,37.91,568.26,251.07,8.97;9,37.91,580.22,251.04,8.97;9,37.91,592.17,251.07,8.97;9,37.91,604.12,251.04,8.97;9,37.91,616.09,251.05,8.97;9,37.91,628.04,251.04,8.97;9,37.91,639.99,251.05,8.97;9,37.91,651.95,251.03,8.97;9,37.91,663.90,251.05,8.97;9,37.91,675.85,251.04,8.97;9,37.91,687.82,170.44,8.97">As we have noted, there are many reasons for imputing missing data; one such is to improve predictive performance. We therefore compare our method against the same 11 benchmarks with respect to the accuracy of predicting labels. (See the description of the datasets in Section V for labeling in each case.) For this purpose, we use Area Under the Receiver Operating Characteristic Curve (AUROC) as the measure of performance. (AUROC is defined as the area under the receiver operating characteristic curve, which is the graph of sensitivity (true positive rate) vs. 1-specificity (false positive rate).) To be fair to all methods of imputing missing values, we use the same predictive model (a simple 1-layer RNN) in all cases.</p>
                <p coords="9,49.86,699.76,239.12,8.97;9,37.91,711.73,251.05,8.97;9,37.91,723.68,251.04,8.97;9,37.91,735.63,99.88,8.97">1) Prediction Accuracy on the Original Datasets: In this subsection, we evaluate the effects of the imputations on the prediction of labels (outcomes), which in the cases at hand correspond to prognoses.</p>
                <p coords="9,310.89,305.03,21.87,8.97;9,347.12,305.03,204.85,8.97;9,300.92,316.98,251.04,8.97;9,300.92,328.93,251.04,8.97;9,300.92,340.89,251.06,8.97;9,300.92,352.84,251.06,8.97;9,300.92,364.80,251.07,8.97;9,300.92,376.76,251.07,8.97;9,300.92,388.71,251.06,8.97;9,300.92,400.66,251.05,8.97;9,300.92,412.62,173.43,8.97;9,497.22,412.62,54.76,8.97;9,300.92,424.57,251.04,8.97;9,300.92,436.54,251.05,8.97;9,300.92,448.49,251.08,8.97;9,300.92,460.44,181.93,8.97">Table 
                    <ref type="table" coords="9,336.35,305.03,7.19,8.97">V</ref> shows the mean and percentage performance gain of M-RNN (MI) in comparison with the benchmarks on all the datasets. M-RNN -which we have already shown to achieve the best imputation accuracy -also yields the best prediction accuracy. However, even in cases where the improvement in imputation accuracy is large and statistically significant, the improvements in prediction accuracy are sometimes smaller and not always statistically significant. For instance, on the Deterioration dataset, the AUROC of M-RNN (MI) is 0.7779 (95% CI: 0.7678-0.7868); the best benchmark is 
                    <ref type="bibr" coords="9,477.48,412.62,16.60,8.97" target="#b23">[24]</ref> with AUROC of 0.7593 (95% CI: 0.7478-0.7702). Similarly, on the UNOS-Heart dataset, the AUROC of M-RNN (MI) is 0.6855 (95% CI: 0.6781-0.6913); the best benchmark is MissForest, with AUROC of 0.6740 (95% CI: 0.6651-0.6817).
                </p>
                <p coords="9,310.89,472.40,241.07,8.97;9,300.92,484.35,251.05,8.97;9,300.92,496.31,251.04,8.97;9,300.92,508.27,251.05,8.97;9,300.92,520.22,251.05,8.97;9,300.92,532.17,251.04,8.97;9,300.92,544.13,251.04,8.97;9,300.92,556.08,251.06,8.97;9,300.92,568.04,102.18,8.97">It should be noted that, by using mean squared error as the loss function, we have deliberately optimized M-RNN for imputation accuracy. If we want to optimize M-RNN for prediction accuracy we might do better by using a different loss function, such as cross-entropy. In Table XI of the Appendix we report an experiment in which we have done precisely that; the short summary is that optimizing for prediction accuracy does in fact improve the predictive performance of M-RNN but the improvement is marginal.</p>
                <p coords="9,310.89,580.00,241.10,8.97;9,300.92,591.95,251.07,8.97;9,300.92,603.90,251.04,8.97;9,300.92,615.86,146.92,8.97;9,465.53,615.86,86.43,8.97;9,300.92,627.81,251.04,8.97;9,300.92,639.78,251.06,8.97;9,300.93,651.73,251.05,8.97;9,300.93,663.68,42.60,8.97">The Appendix also reports other experiments that help further our understanding of the M-RNN algorithm. Table VIII demonstrates that using a different predictive model (random forest, logistic regression or Xgboost 
                    <ref type="bibr" coords="9,450.26,615.86,15.27,8.97" target="#b33">[34]</ref>, rather than a 1-layer RNN) for prediction after imputation leads to results similar to those obtained above. Tables IX and X demonstrate that accounting for donor features in the UNOS datasets makes little difference.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="9,300.93,694.44,226.83,9.22">E. Prediction Accuracy With Various Missing Rates</head>
                <p coords="9,310.89,711.73,241.10,8.97;9,300.93,723.68,251.07,8.97;9,300.93,735.64,251.03,8.97">As discussed above, we carried out experiments with increased rates of missing data in order to understand the implications for the accuracy of imputation. We also carried out</p>
                <p coords="10,283.24,65.71,31.83,7.37;10,55.10,74.67,488.09,7.37;10,42.12,508.49,251.05,8.97;10,42.12,520.44,251.04,8.97;10,42.12,532.39,251.03,8.97;10,42.12,544.35,251.05,8.97;10,42.12,556.31,251.05,8.97;10,42.12,568.26,251.07,8.97;10,42.12,580.22,251.08,8.97;10,42.12,592.17,251.05,8.97;10,42.12,604.12,251.03,8.97;10,42.12,616.08,251.07,8.97;10,42.12,628.04,251.07,8.97;10,42.12,639.99,251.03,8.97;10,42.12,651.95,36.40,8.97">TABLE V PERFORMANCE COMPARISON FOR PATIENT STATE PREDICTION WITH A 1-LAYER RNN (PERFORMANCE GAIN IS COMPUTED IN TERMS OF 1-AUROC) experiments with increased rates of missing data in order to understand the implications for the accuracy of prediction. To explore the predictive performance for a wide range of missing rates (from 0% to 90%), we begin with the Biobank dataset, which is a complete dataset. We randomly remove 10% to 90% of the measurements (with increments of 10%) to create multiple datasets with different missing rates. (In each case we use 80% of the data for training and 20% for testing.) As before, we use M-RNN and various benchmarks for imputing missing data and a 1-layer RNN as the predictive model. (In this setting we are predicting a clinical diagnosis of diabetes.) In each setting, we conducted 10 trials, and report the performance in terms of AUROC.</p>
                <p coords="10,52.08,663.90,15.77,8.97;10,76.78,663.90,216.38,8.97;10,42.12,675.85,251.05,8.97;10,42.12,687.81,85.06,8.97;10,134.98,687.81,158.18,8.97;10,42.12,699.77,251.04,8.97;10,42.12,711.73,164.55,8.97;10,214.01,711.73,79.17,8.97;10,42.12,723.68,251.06,8.97;10,42.12,735.63,251.04,8.97;10,305.14,449.42,251.04,8.97;10,305.14,461.37,173.22,8.97">Fig. 
                    <ref type="figure" coords="10,72.77,663.90,4.01,8.97" target="#fig_3">5</ref>(a) illustrates the impact (in terms of AUROC) of increasing amounts of missing data for M-RNN and various benchmarks. As Fig. 
                    <ref type="figure" coords="10,130.97,687.81,4.01,8.97" target="#fig_3">5</ref>(a) shows, for M-RNN and all benchmarks, the prediction performance decreases as the amount of missing data increases. However, as Fig. 
                    <ref type="figure" coords="10,209.86,711.73,4.15,8.97" target="#fig_3">5</ref>(b) shows, M-RNN continues to outperform the benchmarks; indeed, the performance gap between M-RNN and the benchmarks widens when more data is missing. That is: the importance of accurate imputation is greater when more data is missing.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="10,305.14,491.20,173.50,9.22">F. The Importance of Specific Features</head>
                <p coords="10,315.10,508.49,241.08,8.97;10,305.14,520.44,251.04,8.97;10,305.14,532.39,251.03,8.97;10,305.14,544.35,251.06,8.97;10,305.14,556.31,251.04,8.97;10,305.14,568.26,251.08,8.97;10,305.14,580.22,251.03,8.97;10,305.14,592.17,251.05,8.97;10,305.14,604.12,251.06,8.97;10,305.14,616.08,251.04,8.97;10,305.14,628.04,251.04,8.97;10,305.14,639.99,251.05,8.97;10,305.14,651.95,251.06,8.97;10,305.14,663.90,251.02,8.97;10,305.14,675.85,251.05,8.97;10,305.14,687.81,251.05,8.97;10,305.14,699.77,251.04,8.97;10,305.14,711.73,251.03,8.97;10,305.14,723.68,251.05,8.97;10,305.14,735.63,146.13,8.97;10,457.50,735.63,3.74,8.97">To this point, we have treated all missing data as equally important and given the same weight to all errors. However, this is not always the right thing to do. In particular, it is clear that not all missing data is equally important for prediction. To understand the importance of missing data for purposes of prediction we conduct two experiments in parallel. For the first experiment (which we call Setting A: Purely Random Removal), we construct 5 sub-samples of the MIMIC-III dataset by randomly removing an additional 10%, 20%, 30%, 40%, 50% of the measurements for randomly chosen features. For the second experiment (which we call Setting B: Correlated Random Removal) we first identify the four features that are most highly correlated with the mortality label; those are anion gap, bicarbonate, systolic blood pressure, and pottasium. We then construct 5 sub-samples of the MIMIC-III dataset by removing an additional 10%, 20%, 30%, 40%, 50% of the measurements for these specific features. In both cases we repeat the exercise 10 times and report average results. We then compare the prediction performance of M-RNN (MI) with the best benchmarks; the results are shown visually in Fig. 
                    <ref type="figure" coords="10,453.76,735.63,3.74,8.97" target="#fig_4">6</ref>.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,146.42,65.71,34.04,7.37;11,98.13,74.67,130.63,7.37">TABLE VI CONGENIALITY OF IMPUTATION MODELS</head>
                <p coords="11,47.87,211.53,15.77,8.97;11,74.25,211.53,214.69,8.97;11,37.91,223.48,251.05,8.97;11,37.91,235.44,251.07,8.97;11,37.91,247.40,251.04,8.97;11,37.91,259.30,251.04,9.02;11,37.91,271.30,251.05,8.97;11,37.91,283.26,251.05,8.97;11,37.91,295.22,251.04,8.97;11,37.91,307.18,251.06,8.97;11,37.91,319.13,42.07,8.97">Fig. 
                    <ref type="figure" coords="11,66.45,211.53,4.98,8.97" target="#fig_4">6</ref> shows that M-RNN outperforms the best benchmarks for every sub-sample and the improvement in performance is greater for the sub-samples for which more data is missing. The improvement in performance is statistically significant (p-value &lt; 0.05) when an additional 30% or more of the measurements -i.e., a total of 82.5% of the measurements -(or of the most important features for Setting B) -are missing. In particular, the prediction performance of M-RNN is much less sensitive to the amount of data that is missing and to which data is missing.
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,37.91,347.73,128.39,9.22">G. Congeniality of the Model</head>
                <p coords="11,47.87,365.02,11.07,8.97;11,79.57,365.02,209.39,8.97;11,37.91,376.98,251.04,8.97;11,37.91,388.93,251.06,8.97;11,37.91,400.89,113.33,8.97;11,173.30,400.89,115.65,8.97">As 
                    <ref type="bibr" coords="11,60.95,365.02,16.59,8.97" target="#b25">[26]</ref> has emphasized, an extremely desirable aspect of any imputation method is that it produce imputed values in a manner that is consistent and preserves the original relationships between features and labels; 
                    <ref type="bibr" coords="11,153.97,400.89,16.59,8.97" target="#b25">[26]</ref> refers to this as congeniality.
                </p>
                <p coords="11,37.91,412.85,251.06,8.97;11,37.91,424.80,251.03,8.97;11,37.91,436.75,251.06,8.97;11,37.91,448.71,251.05,8.97;11,37.91,460.66,251.06,8.97;11,37.91,472.62,251.07,8.97;11,37.91,484.58,251.05,8.97;11,37.91,496.53,251.03,8.97;11,37.91,508.49,251.03,8.97;11,37.91,520.44,251.05,8.97;11,37.91,532.40,251.03,8.97;11,37.91,544.36,126.16,8.97">Congeniality of an imputation model can be evaluated with respect to a particular model of the feature-label relationships by computing the model parameters for the true complete data and the imputed data and measuring the difference between parameters according to some specified metric. Of course no imputation method can be expected to be perfectly congenial, but we argue that our method is more congenial -i.e., better preserves the relationships between features and labels -than benchmarks. To see this, we exploit the Biobank dataset; this is a complete dataset, so that it is possible to compare the relationship between the actual (original) data and labels and the relationship between the the imputed data and labels.</p>
                <p coords="11,47.87,556.31,241.09,8.97;11,37.91,568.26,251.02,8.97;11,37.91,580.22,84.66,8.97;11,140.05,580.22,148.92,8.97;11,37.91,592.17,251.03,8.97;11,37.91,604.13,251.03,8.97;11,37.91,615.42,251.04,9.96;11,37.91,627.24,251.04,9.77;11,37.91,639.99,251.05,8.97;11,37.91,651.15,251.06,10.55;11,42.89,663.10,40.99,10.56;11,47.87,675.85,241.09,8.97;11,37.91,687.81,250.97,8.97;11,37.91,699.76,251.05,8.97;11,37.91,711.73,251.04,8.97;11,37.91,723.68,251.05,8.97;11,37.91,735.63,251.06,8.97">In our particular experiment, we delete 20% of the data and impute the missing data using our M-RNN and the 4 best benchmarks (the method of 
                    <ref type="bibr" coords="11,124.78,580.22,15.27,8.97" target="#b23">[24]</ref>, Cubic Interpolation, MissForest and Matrix Completion). As a model of the feature-label relationship, we use a logistic regression. As a metric of the difference between the logistic regression parameters w for the actual data and ŵ for the imputed data (which can be interpreted as a measure of the uncongeniality of the imputation) we report both the mean bias w -ŵ 1 and the root mean squared error w -ŵ 2 . As can be seen in Table VI, in comparison with the 4 best benchmarks, M-RNN achieves both smaller mean bias and small root mean squared error between the original and imputed representations of feature-label relationship. (With the exception of MissForest, all the performance improvements of M-RNN are statistically significant at the 95% level.) Thus our method
                </p>
                <p coords="11,408.32,65.71,36.26,7.37;11,304.25,74.67,244.40,7.37;11,347.49,83.64,157.93,7.37">TABLE VII PERFORMANCE COMPARISON FOR MISSING DATA ESTIMATION FOR MCAR AND MAR SETTINGS ON THE BIOBANK DATASET</p>
                <p coords="11,300.92,226.81,251.05,8.97;11,300.92,238.76,50.61,8.97">is more congenial (to the logistic regression model) than the benchmarks.</p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,300.92,262.68,199.21,9.22">H. M-RNN When Data is Missing at Random</head>
                <p coords="11,310.89,279.97,241.08,8.97;11,300.92,291.92,251.04,8.97;11,300.92,303.87,251.02,8.97;11,300.92,315.83,35.97,8.97;11,352.93,315.83,199.05,8.97;11,300.92,327.79,251.05,8.97;11,300.92,339.74,250.98,8.97;11,300.92,351.70,251.07,8.97;11,300.92,363.65,251.06,8.97;11,300.92,375.60,172.94,8.97">The experiments above are designed to demonstrate the superiority of the M-RNN framework in comparison to the benchmarks in settings where data is Missing Completely at Random (MCAR) 
                    <ref type="bibr" coords="11,339.11,315.83,11.61,8.97" target="#b2">[3]</ref> but it is important to understand the comparison in the settings where data is Missing at Random (MAR) -but not Missing Completely at Random. In this subsection, we show that M-RNN also outperforms the benchmarks when data is Missing at Random (MAR). (The assumption that data is Missing at Random is standard in the medical setting.)
                </p>
                <p coords="11,310.89,387.57,241.08,8.97;11,300.92,399.52,251.04,8.97;11,300.92,411.47,251.06,8.97;11,300.92,423.43,117.02,8.97;11,425.55,423.43,126.43,8.97;11,300.92,435.33,251.06,9.02;11,300.93,447.33,251.02,8.97;11,300.93,458.63,169.22,9.96;11,486.73,459.29,19.64,8.97">To accomplish this, we again begin with the complete Biobank dataset and remove 20% of the data. However in this case we do not remove data completely at random; rather, we use the following procedure. 
                    <ref type="foot" coords="11,417.94,421.85,3.49,6.28" target="#foot_1">1</ref> Using induction, we define the probability that the component i of sample n at time t is observed, conditional on the missingness and values (if observed) of the previous i -1 components at time t 
                    <ref type="bibr" coords="11,472.63,459.29,11.61,8.97" target="#b2">[3]</ref> to be
                </p>
                <formula xml:id="formula_11" coords="11,320.70,483.19,35.53,12.83">P t i (n) =</formula>
                <p coords="11,360.19,478.50,5.01,8.93;11,365.20,476.94,6.12,6.25;11,374.99,477.89,25.91,9.96;11,400.89,475.22,46.26,8.98;11,445.35,475.22,21.18,9.13;11,464.73,475.22,49.60,9.13;11,512.53,475.94,17.58,8.41">p m • N • e -j &lt; i w j m t j (n )x t j (n )+b j (1-m t j (n ))</p>
                <p coords="11,379.62,491.21,5.60,6.25;11,379.62,498.82,12.17,6.25;11,394.43,493.93,4.64,8.93;11,399.06,490.64,46.26,8.86;11,443.52,495.31,2.05,4.46;11,447.24,490.64,15.10,7.29;11,460.54,495.31,2.05,4.46;11,464.27,490.64,43.51,7.91;11,505.98,495.31,2.05,4.46;11,509.70,491.24,11.50,6.69">N l=1 e -j &lt; i w j m t j (l)x t j (l)+b j (1-m t j (l))</p>
                <p coords="11,300.92,510.80,32.61,9.02;11,333.53,509.24,6.12,6.25;11,344.36,510.85,207.60,8.97;11,300.92,522.76,45.44,9.02;11,346.37,521.19,6.12,6.25;11,356.70,522.14,195.28,10.42;11,300.92,534.77,251.06,8.97;11,300.92,546.67,39.46,9.02;11,340.38,545.11,2.52,6.25;11,340.38,550.77,3.49,6.69;11,344.85,546.67,25.89,8.93;11,370.73,545.11,2.52,6.25;11,370.73,551.47,5.77,6.25;11,380.48,546.72,91.67,8.97;11,310.89,558.67,241.11,8.97;11,300.92,570.63,99.55,8.97;11,417.78,570.63,134.22,8.97;11,300.92,582.58,251.07,8.97;11,300.92,594.54,83.27,8.97;11,399.62,594.54,152.35,8.97;11,300.92,606.50,247.70,8.97">where p m corresponds to the average missing rate (in our experiment, p m = 0.2), and w j , b j are sampled from U(0, 1) (but are only sampled once for the entire dataset). We sequentially sample m t 1 , ..., m t D for each feature vector. We compare the RMSE of M-RNN architecture against four competitive benchmarks: 
                    <ref type="bibr" coords="11,402.51,570.63,15.27,8.97" target="#b23">[24]</ref>, Cubic Interpolation, MissForest, and Matrix Completion in both MCAR and MAR settings. As can be seen in Table 
                    <ref type="table" coords="11,387.39,594.54,12.24,8.97">VII</ref>, M-RNN outperforms other state-ofthe-art imputation methods in both MCAR and MAR settings.
                </p>
                <p coords="11,310.89,618.45,241.10,8.97;11,300.92,630.40,251.07,8.97;11,300.92,642.36,251.06,8.97;11,300.92,654.32,248.54,8.97">Additional experimental results and the details of the experiments, including standard deviations of the reported results, can be found in the Supplementary Materials: 
                    <ref type="url" coords="11,300.92,654.32,248.54,8.97" target="http://medianetlab.ee.ucla.edu/papers/TBME_MRNN_SM.pdf">http://medianetlab.ee.ucla.edu/papers/TBME_MRNN_SM.pdf</ref>
                </p>
            </div>
            <div
                xmlns="http://www.tei-c.org/ns/1.0">
                <head coords="11,388.94,678.23,75.00,9.22">VII. CONCLUSION</head>
                <p coords="11,310.89,695.52,241.09,8.97;11,300.92,707.48,251.06,8.97">The problem of reconstructing/estimating missing data is ubiquitous in many settings -especially in longitudinal medical</p>
                <p coords="13,279.04,65.71,31.82,7.37;13,97.13,74.67,395.65,7.37;13,277.92,274.62,34.05,7.37;13,184.81,283.59,220.27,7.37;13,276.81,402.05,36.26,7.37;13,142.33,411.01,305.25,7.37">TABLE X PERFORMANCE COMPARISON FOR LABEL PREDICTION (AUROC) WITH/WITHOUT DONOR FEATURES IN UNOS DATASET TABLE XI PERFORMANCE COMPARISON FOR PREDICTION ORIENTED M-RNN TABLE XII PERFORMANCE COMPARISON FOR MISSING DATA ESTIMATION FOR CATEGORICAL VARIABLES</p>
                <p coords="13,37.91,610.87,251.03,8.97;13,37.91,622.83,251.06,8.97;13,37.91,634.78,251.06,8.97;13,37.91,646.74,251.05,8.97;13,37.91,658.70,251.05,8.97;13,37.91,670.65,251.06,8.97;13,37.91,682.60,251.05,8.97;13,37.91,694.56,21.87,8.97;13,70.45,694.56,8.16,8.97">presents the comparisons for prediction. As noted in the main text, the differences are not large, presumably because the donor information is completely static and the relevant aspects are captured implicitly in the recipient data. Note that the RNN-based Interpolation-based methods do not utilize information across features; hence, whether or not donor features are available, the imputation performance of these methods is unchanged (See Table 
                    <ref type="table" coords="13,62.29,694.56,8.16,8.97">IX</ref>).
                </p>
                <p coords="13,47.87,706.52,240.99,8.97;13,37.91,718.47,251.05,8.97;13,37.91,730.43,251.06,8.97;13,37.91,742.38,251.05,8.97;13,300.92,610.87,251.04,8.97;13,300.92,622.83,251.05,8.97;13,300.92,634.79,251.04,8.97;13,300.92,646.74,84.10,8.97">As can be seen in Tables IX and X, the effects of donor features on imputation and prediction accuracy are not large. The RMSE of M-RNN (MI) improves from from 0.0479 to 0.0451 (5.8%) on the UNOS-Heart dataset and 0.0606 to 0.0579 (4.5%) on the UNOS-Lung dataset. In terms of prediction, AUROC of M-RNN (MI) improves from from 0.6855 to 0.7153 (9.5%) on the UNOS-Heart dataset and from 0.6762 to 0.6883 (3.7%) on UNOS-Lung dataset.</p>
            </div>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,42.12,187.02,514.08,7.64;2,42.12,195.99,397.15,7.64">
                <head>Fig. 1 .</head>
                <label>1</label>
                <figDesc>Fig. 1. Block diagram of missing data estimation process: X = missing measurements; red lines = connections between observed values and missing values in each layer; blue lines = connections between interpolated values; and dashed lines = dropout.</figDesc>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="7,300.92,466.67,251.05,7.37;7,300.92,475.63,251.04,7.37;7,300.92,484.59,251.07,7.37;7,300.92,493.56,27.79,7.37">
                <head>Fig. 3 .</head>
                <label>3</label>
                <figDesc>Fig. 3. Box-plot comparisons between M-RNN (MI), M-RNN (SI) and the best benchmark. (a) RMSE comparison using MIMIC-III dataset. (b) AUROC comparison using MIMIC-III dataset. Red crosses represents outliers.</figDesc>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="9,37.91,274.02,514.09,7.37;9,37.91,282.99,285.92,7.37;9,37.91,304.36,251.02,9.63;9,37.91,316.31,251.06,9.63;9,37.91,328.93,251.06,8.97;9,37.91,340.89,251.04,8.97;9,37.91,352.84,251.08,8.97;9,37.91,364.79,52.88,8.97;9,49.86,376.75,239.10,8.97;9,37.91,388.71,251.06,8.97;9,37.91,400.67,250.95,8.97;9,37.91,412.62,251.07,8.97;9,37.91,424.57,251.05,8.97;9,37.91,436.53,251.03,8.97;9,37.91,447.82,250.98,9.63;9,37.91,460.44,250.98,8.97;9,37.91,472.40,251.05,8.97;9,37.91,484.35,251.06,8.97;9,37.91,496.30,251.05,8.97;9,37.91,508.26,177.69,8.97;9,78.35,66.83,433.10,194.44">
                <head>Fig. 4 .</head>
                <label>4</label>
                <figDesc>Fig. 4. Imputation accuracy for the MIMIC-III dataset with various settings. (a) Additional data missing at random. (b) Feature dimensions chosen at random. (c) Samples chosen at random. (d) Measurements chosen at random. samples exceeds N = 7, 000. (However, one should not necessarily take the figure N = 7, 000 as representing a cut-off below which M-RNN should not be applied, because M-RNN outperforms the benchmarks on the Deterioration and Biobank datasets, which contain only 6,094 samples and 3,902 samples, respectively.) 4) Number of Measurements Per Patient (Fig. 4(d)):We have already noted that, in our datasets, MIMIC-III and Deterioration have many (relatively frequent) measurements per patient, while the other datasets have only a few (and infrequent) measurements per patient and that this leads to differences in performance of M-RNN. To further explore this effect, we created subsets of the MIMIC-III dataset with T = 1, 3, 5, 10, 20, 30 measurements per patient. As might be expected, and as Fig.4(d)shows, having fewer measurements per patient degrades the performance of interpolation-based algorithms but has little effect on pure imputation-based methods; the performance of M-RNN is also degraded, but to a much lesser extent.</figDesc>
                <graphic coords="9,78.35,66.83,433.10,194.44" type="bitmap"/>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="10,42.12,468.18,251.06,7.37;10,42.12,477.15,228.28,7.37">
                <head>Fig. 5 .</head>
                <label>5</label>
                <figDesc>Fig. 5. (a) The AUROC performance with various missing rates. (b) The AUROC gain over the two most competitive benchmarks.</figDesc>
            </figure>
            <figure
                xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="10,305.14,407.19,251.06,7.37;10,305.14,416.15,28.35,7.37">
                <head>Fig. 6 .</head>
                <label>6</label>
                <figDesc>Fig. 6. AUROC comparisons in settings A and B using MIMIC-III dataset.</figDesc>
            </figure>
            <note
                xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="2,34.58,765.07,522.89,6.48">
                <p>Authorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on October 28,2024 at 18:11:05 UTC from IEEE Xplore. Restrictions apply.</p>
            </note>
            <note
                xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="11,312.38,736.99,126.81,7.17;11,34.58,765.07,323.30,6.48">
                <p>Other procedures are certainly possible.Authorized licensed use limited to: Chalmers University of Technology Sweden. Downloaded on October</p>
            </note>
            <note
                xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2" coords="11,359.83,765.07,197.65,6.48">
                <p>28,2024 at 18:11:05 UTC from IEEE Xplore. Restrictions apply.</p>
            </note>
        </body>
        <back>
            <div type="acknowledgement">
                <div>
                    <head coords="14,125.77,554.59,80.98,9.22">ACKNOWLEDGMENT</head>
                    <p coords="14,52.08,571.87,241.08,8.97;14,42.12,583.84,207.96,8.97">The authors would like thank 
                        <rs type="person">K. Poppe</rs> (
                        <rs type="affiliation">University of Auckland</rs>) and 
                        <rs type="person">A. Wood</rs> (
                        <rs type="affiliation">University of Cambridge</rs>).
                    </p>
                </div>
            </div>
            <div type="funding">
                <div>
                    <p coords="1,60.21,611.19,228.76,7.37;1,37.91,620.15,251.07,7.37;1,37.91,629.12,191.93,7.37">This work was supported in part by the 
                        <rs type="funder">Office of Naval Research (ONR)</rs> and in part by the 
                        <rs type="funder">National Science Foundation</rs> under Grants 
                        <rs type="grantNumber">ECCS1462245</rs>, 
                        <rs type="grantNumber">ECCS1533983</rs>, and 
                        <rs type="grantNumber">ECCS1407712</rs>.
                    </p>
                </div>
            </div>
            <listOrg type="funding">
                <org type="funding" xml:id="_X6gKQre">
                    <idno type="grant-number">ECCS1462245</idno>
                </org>
                <org type="funding" xml:id="_TNV4sgs">
                    <idno type="grant-number">ECCS1533983</idno>
                </org>
                <org type="funding" xml:id="_Yzwfg48">
                    <idno type="grant-number">ECCS1407712</idno>
                </org>
            </listOrg>
            <div type="annex">
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <p coords="12,42.12,475.96,251.06,8.97;12,42.12,487.92,251.05,8.97;12,42.12,499.87,251.04,8.97;12,42.12,511.83,251.07,8.97;12,42.12,523.79,251.07,8.97;12,42.12,535.74,251.06,8.97;12,42.12,547.69,251.07,8.97;12,42.12,559.65,251.05,8.97;12,42.12,571.61,191.71,8.97">datasets -and is of enormous importance for many reasons, including statistical analysis, diagnosis, prognosis and treatment. In this paper we have presented a new method, based on a novel deep learning architecture, for reconstructing/estimating missing data that exploits both the correlation within data streams and the correlation across data streams. We have demonstrated on the basis of a variety of real-world medical datasets that our method makes large and statistically significant improvements in comparison with state-of-the-art benchmarks.</p>
                </div>
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="12,142.13,598.80,51.05,9.22;12,53.42,610.75,225.69,9.22">APPENDIX A PREDICTIONS WITH ALTERNATIVE PREDICTIVE MODELS</head>
                    <p coords="12,52.08,628.03,241.09,8.97;12,42.12,639.99,251.04,8.97;12,42.12,651.95,251.04,8.97;12,42.12,663.90,251.05,8.97;12,42.12,675.85,251.03,8.97;12,42.12,687.81,251.05,8.97;12,42.12,699.77,199.93,8.97;12,260.36,699.77,32.84,8.97;12,42.12,711.73,109.32,8.97;12,169.09,711.73,124.09,8.97;12,42.12,723.68,251.08,8.97;12,42.12,735.63,251.07,8.97;12,42.12,747.59,251.04,8.97">Table V compares the implied predictive performance of M-RNN with the benchmarks when imputation is followed by prediction using a particularly simple predictive model. We now compare the implied predictive performance of M-RNN with the benchmarks when imputation is followed by prediction using other predictive models: in addition to the 1-layer RNN model used already, we use Random Forest 
                        <ref type="bibr" coords="12,245.09,699.77,15.27,8.97" target="#b34">[35]</ref>, Logistic Regression and Xgboost 
                        <ref type="bibr" coords="12,153.82,711.73,15.27,8.97" target="#b33">[34]</ref>. (For implementations, we use the randomForest package, the glm package and the xgboost package, all in R.) We restrict our attention to the MIMIC-III dataset and continue to use AUROC as the performance metric.
                    </p>
                    <p coords="12,305.14,475.96,21.88,8.97;12,348.89,475.96,207.30,8.97;12,305.14,487.92,251.02,8.97;12,305.14,499.88,124.97,8.97">Table 
                        <ref type="table" coords="12,329.39,475.96,17.14,8.97">VIII</ref> shows the mean and performance gain (%) (in terms of AUROC) of M-RNN in comparison with the benchmarks with various predictive models.
                    </p>
                    <p coords="12,315.10,511.83,94.31,8.97;12,427.67,511.83,128.50,8.97;12,305.13,523.79,251.03,8.97;12,305.13,535.74,251.07,8.97;12,305.13,547.69,251.04,8.97;12,322.09,559.66,4.24,8.97;12,345.27,559.66,210.91,8.97;12,305.13,571.61,251.05,8.97;12,305.13,583.56,251.04,8.97;12,305.14,595.52,251.07,8.97;12,305.14,607.47,251.03,8.97;12,305.14,619.42,251.04,8.97;12,305.14,631.39,167.68,8.97">As can be seen in Table 
                        <ref type="table" coords="12,411.96,511.83,15.71,8.97">VIII</ref>, the different predictive models do not yield much different prediction accuracy, and no one predictive model seems to consistently lead to more or less accurate predictions. (Because the RNN imputation methods 
                        <ref type="bibr" coords="12,305.13,559.66,16.96,8.97" target="#b21">[22]</ref>- 
                        <ref type="bibr" coords="12,326.33,559.66,16.96,8.97" target="#b23">[24]</ref> can only be combined with an RNN predictive model, there are no RF, LR, XG results for these methods.) Note that prediction accuracy is not perfectly correlated with imputation accuracy, but the differences are not statistically significant. As before, M-RNN leads to the most accurate predictions when followed by each of the various predictive models; again, not all the differences are statistically significant.
                    </p>
                </div>
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="12,405.13,658.57,51.06,9.22;12,320.66,670.53,217.27,9.22">APPENDIX B EFFECTS OF DONOR FEATURES IN UNOS DATASETS</head>
                    <p coords="12,315.10,687.81,241.08,8.97;12,305.14,699.76,251.06,8.97;12,305.14,711.73,251.07,8.97;12,305.14,723.68,251.04,8.97;12,305.14,735.63,251.04,8.97;12,305.14,747.59,21.87,8.97;12,343.13,747.59,213.06,8.97">The UNOS organ transplant dataset records features of both the recipient and the donor. In the main text we ignored donor features; here we expand the analysis to include these features, and compare the results obtained using only recipient features with the results obtained using both recipient and donor features. Table 
                        <ref type="table" coords="12,329.82,747.59,10.51,8.97">IX</ref> presents the comparison for imputations and Table X
                    </p>
                </div>
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="13,400.65,677.50,51.60,9.22;13,359.60,689.45,130.95,9.22">APPENDIX C PREDICTION-ORIENTED M-RNN</head>
                    <p coords="13,310.89,706.74,241.10,8.97;13,300.93,718.69,251.05,8.97;13,300.93,730.65,251.03,8.97;13,300.93,741.94,124.19,9.96;13,430.62,740.25,3.49,6.69">Finally, we report the results of an experiment in which we optimized M-RNN for prediction accuracy rather than imputation accuracy. To do this, we trained M-RNN to minimize the cross-entropy L = 1</p>
                    <p coords="14,42.12,65.72,251.03,9.96;14,42.12,78.35,251.06,8.97;14,42.12,90.30,44.40,8.97;14,98.37,90.30,194.80,8.97;14,42.12,102.25,251.07,8.97;14,42.12,114.22,251.07,8.97;14,42.12,126.17,251.04,8.97;14,42.12,138.12,156.74,8.97">log(1 -ŷn )], rather than the mean squared error. (We continue to evaluate performance in terms of AUROC.) As can be seen from Table 
                        <ref type="table" coords="14,89.70,90.30,8.67,8.97">XI</ref>, doing this does improve the predictions of M-RNN, but the improvement is marginal and not statistically significant. (However, doing this does have the advantage that it creates an end-to-end prediction algorithm that does not require any preprocessing or imputation steps.)
                    </p>
                </div>
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="14,141.85,156.08,51.60,9.22;14,97.38,168.03,137.78,9.22">APPENDIX D CATEGORICAL VALUE IMPUTATION</head>
                    <p coords="14,52.08,185.32,241.08,8.97;14,42.12,197.27,251.07,8.97;14,42.12,208.56,148.83,9.96;14,190.94,209.23,102.21,9.75;14,42.12,221.18,129.82,8.97">As an additional performance metric for comparing the imputation quality of categorical features, we compute the proportion of falsely classified entries (PFC). If S cat is the set of categorical variables, then PFC is defined as</p>
                    <p coords="14,289.39,247.48,2.77,8.93;14,42.12,274.25,251.05,8.97;14,42.12,286.21,65.34,8.97;14,122.58,286.21,170.56,8.97;14,42.12,298.17,210.31,8.97">, Keep in mind that smaller PFC means better imputation. As can be seen in Table 
                        <ref type="table" coords="14,110.35,286.21,12.24,8.97">XII</ref>, M-RNN outperforms all the benchmarks in all the datasets in terms of the PFC metric as well.
                    </p>
                </div>
                <div
                    xmlns="http://www.tei-c.org/ns/1.0">
                    <head coords="14,142.13,316.11,51.05,9.22;14,47.22,328.08,238.09,9.22">APPENDIX E IMPLEMENTATIONS AND HYPER-PARAMETER OPTIMIZATION</head>
                    <p coords="14,52.08,345.35,241.09,8.97;14,42.12,357.31,251.05,8.97;14,42.12,369.27,251.06,8.97;14,42.12,381.23,251.00,8.97;14,42.12,393.18,251.05,8.97;14,42.12,405.08,251.05,9.02;14,42.12,417.09,251.06,8.97;14,42.12,429.05,251.05,8.97;14,42.12,441.00,251.04,8.97;14,42.12,452.96,251.05,8.97;14,42.12,464.91,159.08,8.97">In all of our experiments, we set the depth of the networks for M-RNN and for other neural network benchmarks (including RNN-based benchmarks and auto-encoder) to 4. (In the case of M-RNN, the interpolation block uses 2 layers and the imputation block uses 2 layers.) For M-RNN, there are 4 hidden nodes in each layer in the interpolation block and D hidden nodes in each layer in the imputation block. For the benchmarks, in order to make a fair comparison, we adjusted the number of hidden nodes in each layer to match the model capacity (the number of parameters for all models) of M-RNN. The number of batches is 64 for both M-RNN and benchmarks.</p>
                    <p coords="14,52.08,476.86,241.11,8.97;14,42.12,488.82,251.04,8.97;14,42.12,500.78,251.07,8.97;14,42.12,512.73,251.04,8.97;14,42.12,524.69,251.06,8.97;14,42.12,536.64,192.05,8.97">For some of these algorithms, we are able to use off-the-shelf implementations. For Spline and Cubic Interpolation, we use the interp1 package in MATLAB; for MICE we use the mice package in R; for MissForest we use the MissForest package in R; for EM we use the Amelia package in R; for matrix completion we use the softImpute package in R.</p>
                </div>
            </div>
            <div type="references">
                <listBibl>
                    <biblStruct coords="14,60.38,625.41,232.81,7.17;14,60.38,634.38,232.81,7.17;14,60.38,643.34,140.77,7.17" xml:id="b0">
                        <analytic>
                            <title level="a" type="main" coords="14,182.82,625.41,110.36,7.17;14,60.38,634.38,121.54,7.17">The effects of the irregular sample and missing data in time series analysis</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Kreindler</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Lumsden</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,189.55,634.38,103.64,7.17;14,60.38,643.34,48.27,7.17">Nonlinear Dynamical Syst. Anal. Behavioral Sci</title>
                            <imprint>
                                <biblScope unit="volume">10</biblScope>
                                <biblScope unit="page" from="187" to="214"/>
                                <date type="published" when="2006">2006</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,60.38,652.31,232.83,7.17;14,60.38,661.27,222.25,7.17" xml:id="b1">
                        <analytic>
                            <title level="a" type="main" coords="14,162.57,652.31,130.63,7.17;14,60.38,661.27,17.33,7.17">Wavelet variance analysis for gappy time series</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">B</forename>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,85.49,661.27,83.99,7.17">Ann. Inst. Statistical Math</title>
                            <imprint>
                                <biblScope unit="volume">62</biblScope>
                                <biblScope unit="issue">5</biblScope>
                                <biblScope unit="page" from="943" to="966"/>
                                <date type="published" when="2010">2010</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,60.38,670.25,232.81,7.17;14,60.38,679.21,107.08,7.17" xml:id="b2">
                        <monogr>
                            <title level="m" type="main" coords="14,105.59,670.25,156.50,7.17">Multiple Imputation for Nonresponse in Surveys</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">B</forename>
                                    <surname>Rubin</surname>
                                </persName>
                            </author>
                            <imprint>
                                <date type="published" when="2004">2004</date>
                                <publisher>Wiley</publisher>
                                <biblScope unit="volume">81</biblScope>
                                <pubPlace>Hoboken, NJ, USA</pubPlace>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,60.38,688.17,232.80,7.17;14,60.38,697.14,210.21,7.17" xml:id="b3">
                        <analytic>
                            <title level="a" type="main" coords="14,157.10,688.17,136.09,7.17;14,60.38,697.14,19.69,7.17">Pattern classification with missing data: A review</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <forename type="middle">J</forename>
                                    <surname>García-Laencina</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,88.63,697.14,69.11,7.17">Neural Comput. Appl</title>
                            <imprint>
                                <biblScope unit="volume">19</biblScope>
                                <biblScope unit="issue">2</biblScope>
                                <biblScope unit="page" from="263" to="282"/>
                                <date type="published" when="2010">2010</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,60.38,706.11,232.79,7.17;14,60.38,715.07,232.82,7.17" xml:id="b4">
                        <analytic>
                            <title level="a" type="main" coords="14,125.15,706.11,168.02,7.17;14,60.38,715.07,78.04,7.17">Multiple imputation using chained equations: Issues and guidance for practice</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">I</forename>
                                    <forename type="middle">R</forename>
                                    <surname>White</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,145.90,715.07,37.89,7.17">Statist. Med</title>
                            <imprint>
                                <biblScope unit="volume">30</biblScope>
                                <biblScope unit="issue">4</biblScope>
                                <biblScope unit="page" from="377" to="399"/>
                                <date type="published" when="2011">2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,60.38,724.04,232.80,7.17;14,60.38,733.00,232.81,7.17;14,60.38,741.97,61.78,7.17" xml:id="b5">
                        <analytic>
                            <title level="a" type="main" coords="14,175.90,724.04,117.28,7.17;14,60.38,733.00,122.65,7.17">Missforest-non-parametric missing value imputation for mixed-type data</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Stekhoven</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Bühlmann</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,192.48,733.00,46.07,7.17">Bioinformatics</title>
                            <imprint>
                                <biblScope unit="volume">28</biblScope>
                                <biblScope unit="issue">1</biblScope>
                                <biblScope unit="page" from="112" to="118"/>
                                <date type="published" when="2011">2011</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,67.45,232.81,7.17;14,323.40,76.42,232.83,7.17" xml:id="b6">
                        <analytic>
                            <title level="a" type="main" coords="14,392.43,67.45,163.78,7.17;14,323.40,76.42,62.56,7.17">Spectral regularization algorithms for learning large incomplete matrices</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Mazumder</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,393.95,76.42,63.51,7.17">J. Mach. Learn. Res</title>
                            <imprint>
                                <biblScope unit="volume">11</biblScope>
                                <biblScope unit="page" from="2287" to="2322"/>
                                <date type="published" when="2010">2010</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,85.39,232.82,7.17;14,323.40,94.35,232.79,7.17;14,323.40,103.32,80.61,7.17" xml:id="b7">
                        <analytic>
                            <title level="a" type="main" coords="14,385.64,85.39,170.56,7.17;14,323.40,94.35,112.19,7.17">Temporal regularized matrix factorization for highdimensional time series prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H.-F</forename>
                                    <surname>Yu</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,454.55,94.35,101.63,7.17;14,323.40,103.32,14.01,7.17">Proc Adv. Neural Inf. Process. Syst</title>
                            <meeting>Adv. Neural Inf. ess. Syst</meeting>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="847" to="855"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,112.28,232.82,7.17;14,323.40,121.25,232.82,7.17;14,323.40,130.22,49.83,7.17" xml:id="b8">
                        <analytic>
                            <title level="a" type="main" coords="14,392.94,112.28,163.28,7.17;14,323.40,121.25,60.32,7.17">Recommendations as treatments: Debiasing learning and evolution</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Schnabel</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,406.45,121.25,123.40,7.17">Proc 33rd Int. Conf. Mach. Learn</title>
                            <meeting>33rd Int. Conf. Mach. Learn</meeting>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="1670" to="1679"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,139.18,232.81,7.17;14,323.40,148.15,232.83,7.17" xml:id="b9">
                        <analytic>
                            <title level="a" type="main" coords="14,392.43,139.18,163.78,7.17;14,323.40,148.15,62.56,7.17">Spectral regularization algorithms for learning large incomplete matrices</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">R</forename>
                                    <surname>Mazumder</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,393.95,148.15,63.51,7.17">J. Mach. Learn. Res</title>
                            <imprint>
                                <biblScope unit="volume">11</biblScope>
                                <biblScope unit="page" from="2287" to="2322"/>
                                <date type="published" when="2010">2010</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,157.12,232.82,7.17;14,323.39,166.08,232.81,7.17;14,323.39,175.05,131.06,7.17" xml:id="b10">
                        <analytic>
                            <title level="a" type="main" coords="14,393.73,157.12,162.47,7.17;14,323.39,166.08,180.55,7.17">Learning from clinical judgments: Semi-Markovmodulated marked Hawkes processes for risk prognosis</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Alaa</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,522.17,166.08,34.03,7.17;14,323.39,175.05,71.74,7.17">Proc. 34th Int. Conf. Mach. Learn</title>
                            <meeting>34th Int. Conf. Mach. Learn</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="60" to="69"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,184.01,232.80,7.17;14,323.39,192.98,232.80,7.17;14,323.40,201.95,107.67,7.17" xml:id="b11">
                        <analytic>
                            <title level="a" type="main" coords="14,429.81,184.01,126.38,7.17;14,323.39,192.98,182.43,7.17">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Graves</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <surname>Schmidhuber</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,513.15,192.98,39.75,7.17">Neural Netw</title>
                            <imprint>
                                <biblScope unit="volume">18</biblScope>
                                <biblScope unit="issue">5</biblScope>
                                <biblScope unit="page" from="602" to="610"/>
                                <date type="published" when="2005">2005</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,210.91,232.81,7.17;14,323.40,219.88,219.99,7.17" xml:id="b12">
                        <analytic>
                            <title level="a" type="main" coords="14,407.71,210.91,148.48,7.17;14,323.40,219.88,18.85,7.17">A gentle introduction to imputation of missing values</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">R T</forename>
                                    <surname>Donders</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,350.52,219.88,67.67,7.17">J. Clin. Epidemiology</title>
                            <imprint>
                                <biblScope unit="volume">59</biblScope>
                                <biblScope unit="issue">10</biblScope>
                                <biblScope unit="page" from="1087" to="1091"/>
                                <date type="published" when="2006">2006</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,228.85,232.80,7.17;14,323.39,237.81,125.39,7.17" xml:id="b13">
                        <analytic>
                            <title level="a" type="main" coords="14,381.11,228.85,122.73,7.17">Multiple imputation for missing data</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Patrician</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,513.50,228.85,42.69,7.17;14,323.39,237.81,20.30,7.17">Res. Nursing Health</title>
                            <imprint>
                                <biblScope unit="volume">25</biblScope>
                                <biblScope unit="issue">1</biblScope>
                                <biblScope unit="page" from="76" to="84"/>
                                <date type="published" when="2002">2002</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,246.78,232.81,7.17;14,323.39,255.75,232.84,7.17;14,323.39,264.71,17.94,7.17" xml:id="b14">
                        <analytic>
                            <title level="a" type="main" coords="14,382.67,246.78,173.53,7.17;14,323.39,255.75,82.09,7.17">Combining multiple imputation and meta-analysis with individual participant data</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Burgess</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,413.42,255.75,38.31,7.17">Statist. Med</title>
                            <imprint>
                                <biblScope unit="volume">32</biblScope>
                                <biblScope unit="issue">26</biblScope>
                                <biblScope unit="page" from="4499" to="4514"/>
                                <date type="published" when="2013">2013</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,273.68,232.80,7.17;14,323.39,282.64,232.82,7.17;14,323.39,291.61,17.94,7.17" xml:id="b15">
                        <analytic>
                            <title level="a" type="main" coords="14,378.80,273.68,177.39,7.17;14,323.39,282.64,70.44,7.17">The use and reporting of multiple imputation in medical research-A review</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <surname>Mackinnon</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,403.05,282.64,51.09,7.17">J. Internal Med</title>
                            <imprint>
                                <biblScope unit="volume">268</biblScope>
                                <biblScope unit="issue">6</biblScope>
                                <biblScope unit="page" from="586" to="593"/>
                                <date type="published" when="2010">2010</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,300.58,232.82,7.17;14,323.39,309.54,227.85,7.17" xml:id="b16">
                        <analytic>
                            <title level="a" type="main" coords="14,387.00,300.58,169.21,7.17;14,323.39,309.54,148.70,7.17">Multiple imputation for missing data in epidemiological and clinical research: Potential and pitfalls</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">J</forename>
                                    <forename type="middle">A</forename>
                                    <surname>Sterne</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,479.51,309.54,15.50,7.17">BMJ</title>
                            <imprint>
                                <biblScope unit="volume">338</biblScope>
                                <biblScope unit="page">2393</biblScope>
                                <date type="published" when="2009">2009</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,318.51,232.82,7.18;14,323.40,327.48,232.83,7.17;14,323.40,336.45,17.94,7.17" xml:id="b17">
                        <analytic>
                            <title level="a" type="main" coords="14,393.66,318.51,162.54,7.17;14,323.40,327.48,50.91,7.17">Dropout: A simple way to prevent neural networks from overfitting</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">N</forename>
                                    <surname>Srivastava</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,383.06,327.48,67.28,7.17">J. Mach. Learn. Res</title>
                            <imprint>
                                <biblScope unit="volume">15</biblScope>
                                <biblScope unit="issue">1</biblScope>
                                <biblScope unit="page" from="1929" to="1958"/>
                                <date type="published" when="2014">2014</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,345.41,232.81,7.17;14,323.40,354.37,162.45,7.17" xml:id="b18">
                        <analytic>
                            <title level="a" type="main" coords="14,408.53,345.41,147.67,7.17;14,323.40,354.37,25.92,7.17">MIMIC-III, a freely accessible critical care database</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">E</forename>
                                    <surname>Johnson</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,357.81,354.37,28.06,7.17">Sci. Data</title>
                            <imprint>
                                <biblScope unit="volume">3</biblScope>
                                <biblScope unit="page">160035</biblScope>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,363.35,232.81,7.17;14,323.40,372.31,232.81,7.17;14,323.40,381.27,96.31,7.17" xml:id="b19">
                        <analytic>
                            <title level="a" type="main" coords="14,389.42,363.35,166.78,7.17;14,323.40,372.31,114.56,7.17">Personalized risk scoring for critical care prognosis using mixtures of gaussian processes</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">A</forename>
                                    <forename type="middle">M</forename>
                                    <surname>Alaa</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,446.00,372.31,81.73,7.17">IEEE Trans. Biomed. Eng</title>
                            <imprint>
                                <biblScope unit="volume">65</biblScope>
                                <biblScope unit="issue">1</biblScope>
                                <biblScope unit="page" from="207" to="218"/>
                                <date type="published" when="2018-01">Jan. 2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,390.24,232.83,7.17;14,323.40,399.21,69.75,7.17" xml:id="b20">
                        <analytic>
                            <title level="a" type="main" coords="14,375.24,390.24,81.32,7.17">UK biobank: Bank on it</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <forename type="middle">J</forename>
                                    <surname>Palmer</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,464.75,390.24,20.30,7.17">Lancet</title>
                            <imprint>
                                <biblScope unit="volume">369</biblScope>
                                <biblScope unit="page" from="1980" to="1982"/>
                                <date type="published" when="2007">9578. 2007</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.40,408.17,232.82,7.17;14,323.40,417.14,210.04,7.17" xml:id="b21">
                        <analytic>
                            <title level="a" type="main" coords="14,375.16,408.17,181.05,7.17;14,323.40,417.14,27.28,7.17">Doctor AI: Predicting clinical events via recurrent neural networks</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">E</forename>
                                    <surname>Choi</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,359.50,417.14,106.94,7.17">Mach. Learning Healthcare Conf</title>
                            <imprint>
                                <biblScope unit="page" from="301" to="318"/>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,426.11,232.81,7.17;14,323.40,435.08,232.80,7.17;14,323.40,444.04,118.92,7.17" xml:id="b22">
                        <analytic>
                            <title level="a" type="main" coords="14,393.51,426.11,162.69,7.17;14,323.40,435.08,172.34,7.17">Directly modeling missing data in sequences with RNNs: Improved classification of clinical time series</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <forename type="middle">C</forename>
                                    <surname>Lipton</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,504.25,435.08,51.95,7.17;14,323.40,444.04,52.21,7.17">Mach. Learning Healthcare conf</title>
                            <imprint>
                                <biblScope unit="page" from="253" to="270"/>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.40,453.00,232.81,7.17;14,323.40,461.98,218.20,7.17" xml:id="b23">
                        <analytic>
                            <title level="a" type="main" coords="14,370.55,453.00,185.66,7.17;14,323.40,461.98,45.65,7.17">Recurrent neural networks for multivariate time series with missing values</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Che</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,377.32,461.98,52.84,7.17">Scientific reports</title>
                            <imprint>
                                <biblScope unit="volume">8</biblScope>
                                <biblScope unit="issue">1</biblScope>
                                <biblScope unit="page">6085</biblScope>
                                <date type="published" when="2018">2018</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,470.94,232.82,7.17;14,323.40,479.90,232.82,7.17;14,323.40,488.87,48.49,7.17" xml:id="b24">
                        <analytic>
                            <title level="a" type="main" coords="14,386.81,470.94,169.40,7.17;14,323.40,479.90,149.61,7.17">Multiple imputation for general missing data patterns in the presence of high-dimensional data</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Deng</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,481.70,479.90,27.45,7.17">Sci. Rep</title>
                            <imprint>
                                <biblScope unit="volume">6</biblScope>
                                <biblScope unit="page">21689</biblScope>
                                <date type="published" when="2016">2016</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,497.84,232.82,7.17;14,323.40,506.80,165.35,7.17" xml:id="b25">
                        <analytic>
                            <title level="a" type="main" coords="14,370.31,497.84,185.89,7.17;14,323.40,506.80,23.89,7.17">Multiple-imputation inferences with uncongenial sources of input</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">X.-L</forename>
                                    <surname>Meng</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,355.38,506.80,44.86,7.17">Statistical Sci</title>
                            <imprint>
                                <biblScope unit="volume">9</biblScope>
                                <biblScope unit="page" from="538" to="558"/>
                                <date type="published" when="1994">1994</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,515.77,232.83,7.17;14,323.40,524.73,232.80,7.17;14,323.40,533.71,83.77,7.17" xml:id="b26">
                        <analytic>
                            <title level="a" type="main" coords="14,418.05,515.77,138.17,7.17;14,323.40,524.73,59.45,7.17">Recurrent neural networks for missing or asynchronous data</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">F</forename>
                                    <surname>Gingras</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Bengio</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,402.03,524.73,151.36,7.17">Proc 8th Int. Conf. Neural Inf. Process. Syst</title>
                            <meeting>8th Int. Conf. Neural Inf. ess. Syst</meeting>
                            <imprint>
                                <date type="published" when="1996">1996</date>
                                <biblScope unit="volume">8</biblScope>
                                <biblScope unit="page" from="395" to="401"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,542.67,232.81,7.17;14,323.40,551.63,232.80,7.17;14,323.40,560.61,146.20,7.17" xml:id="b27">
                        <analytic>
                            <title level="a" type="main" coords="14,407.35,542.67,148.85,7.17;14,323.40,551.63,182.44,7.17">A solution for missing data in recurrent neural networks with an application to blood glucose prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">V</forename>
                                    <surname>Tresp</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Briegel</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,522.94,551.63,33.26,7.17;14,323.40,560.61,79.61,7.17">Proc. Adv. Neural Inf. Process. Syst</title>
                            <meeting>Adv. Neural Inf. ess. Syst</meeting>
                            <imprint>
                                <date type="published" when="1998">1998</date>
                                <biblScope unit="page" from="971" to="977"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,569.57,232.82,7.17;14,323.40,578.53,232.82,7.17;14,323.40,587.50,49.83,7.17" xml:id="b28">
                        <analytic>
                            <title level="a" type="main" coords="14,412.59,569.57,143.63,7.17;14,323.40,578.53,67.51,7.17">Speech recognition with missing data using recurrent neural nets</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">S</forename>
                                    <surname>Parveen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">P</forename>
                                    <surname>Green</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,410.02,578.53,121.96,7.17">Proc. Adv. Neural Inf. Process. Syst</title>
                            <meeting>Adv. Neural Inf. ess. Syst</meeting>
                            <imprint>
                                <date type="published" when="2002">2002</date>
                                <biblScope unit="page" from="1189" to="1195"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,596.47,232.81,7.17;14,323.40,605.43,232.82,7.17;14,323.40,614.40,145.04,7.17" xml:id="b29">
                        <analytic>
                            <title level="a" type="main" coords="14,387.11,596.47,169.09,7.17;14,323.40,605.43,161.55,7.17">Recurrent neural networks with missing information imputation for medical examination data prediction</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">H.-G</forename>
                                    <surname>Kim</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,500.90,605.43,55.31,7.17;14,323.40,614.40,77.61,7.17">Proc. IEEE Conf. Big Data Smart Comput</title>
                            <meeting>IEEE Conf. Big Data Smart Comput</meeting>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                                <biblScope unit="page" from="317" to="323"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,623.36,232.81,7.17;14,323.40,632.34,232.82,7.17;14,323.40,641.30,94.78,7.17" xml:id="b30">
                        <analytic>
                            <title level="a" type="main" coords="14,415.70,623.36,140.50,7.17;14,323.40,632.34,143.07,7.17">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">Y</forename>
                                    <surname>Gal</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">Z</forename>
                                    <surname>Ghahramani</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,483.66,632.34,72.56,7.17;14,323.40,641.30,19.73,7.17">Proc. Int. Conf. Mach. Learn</title>
                            <meeting>Int. Conf. Mach. Learn</meeting>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="1050" to="1059"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,650.26,232.81,7.17;14,323.40,659.23,128.97,7.17" xml:id="b31">
                        <monogr>
                            <title level="m" type="main" coords="14,417.45,650.26,138.75,7.17;14,323.40,659.23,40.31,7.17">Multiple imputation using deep denoising autoencoders</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Gondara</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">K</forename>
                                    <surname>Wang</surname>
                                </persName>
                            </author>
                            <idno type="arXiv">arXiv:1705.02737</idno>
                            <imprint>
                                <date type="published" when="2017">2017</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,668.20,232.81,7.17;14,323.40,677.16,232.81,7.17;14,323.40,686.13,61.78,7.17" xml:id="b32">
                        <analytic>
                            <title level="a" type="main" coords="14,368.44,668.20,187.76,7.17;14,323.40,677.16,80.82,7.17">A Markov chain monte carlo algorithm for multiple imputation in large surveys</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">D</forename>
                                    <surname>Schunk</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,414.06,677.16,87.58,7.17">AStA Adv. Statistical Anal</title>
                            <imprint>
                                <biblScope unit="volume">92</biblScope>
                                <biblScope unit="issue">1</biblScope>
                                <biblScope unit="page" from="101" to="114"/>
                                <date type="published" when="2008">2008</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,695.09,232.80,7.17;14,323.40,704.06,232.81,7.17;14,323.40,713.03,61.79,7.17" xml:id="b33">
                        <analytic>
                            <title level="a" type="main" coords="14,413.75,695.09,135.67,7.17">Xgboost: A scalable tree boosting system</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">T</forename>
                                    <surname>Chen</surname>
                                </persName>
                            </author>
                            <author>
                                <persName coords="">
                                    <forename type="first">C</forename>
                                    <surname>Guestrin</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="m" coords="14,331.95,704.06,220.68,7.17">Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</title>
                            <meeting>22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining</meeting>
                            <imprint>
                                <date type="published" when="2016">2016</date>
                                <biblScope unit="page" from="785" to="794"/>
                            </imprint>
                        </monogr>
                    </biblStruct>
                    <biblStruct coords="14,323.39,721.99,232.83,7.17;14,323.40,730.97,17.94,7.17" xml:id="b34">
                        <analytic>
                            <title level="a" type="main" coords="14,369.67,721.99,49.38,7.17">Random forests</title>
                            <author>
                                <persName coords="">
                                    <forename type="first">L</forename>
                                    <surname>Breiman</surname>
                                </persName>
                            </author>
                        </analytic>
                        <monogr>
                            <title level="j" coords="14,427.69,721.99,42.55,7.17">Mach. Learn</title>
                            <imprint>
                                <biblScope unit="volume">45</biblScope>
                                <biblScope unit="issue">1</biblScope>
                                <biblScope unit="page" from="5" to="32"/>
                                <date type="published" when="2001">2001</date>
                            </imprint>
                        </monogr>
                    </biblStruct>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>