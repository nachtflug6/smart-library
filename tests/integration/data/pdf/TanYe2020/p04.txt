ple Sk = (tk, xk), where tk = [tk
.1, · · · , tk
.n, · · · , tk
.N] and
xk = [xk
.1, · · · , xk
.n, · · · , xk
.N], are shorter than the list of all
timestamps tk
all = [tk
1, tk
2, · · · , tT k
k] due to missing values.
For each variable, we represent actually observed records as
Xk
n and corresponding timestamps as Tk
n. The estimation of
the missing value at time t∗(t∗∈tk
all & t∗/∈Tk
n) is made
via the computation of the conditional distribution, which is
a Gaussian distribution with a mean function E[x∗] and a
covariance function Con[x∗]:
p(x∗|t∗, Xk
n, Tk
n) ∼N(u∗, δ2
∗)
(1)
E[x∗] = k(t∗)T (K(Tk
n, Tk
n) + δ2IN)−1Xk
n
(2)
Con[x∗] = k(t∗, t∗) −k(t∗)T (K(Tk
n, Tk
n) + δ2IN)−1k(t∗)
(3)
where K(Tk
n, Tk
n) is the covariance matrix between ob-
served records; k(t∗) is the covariance matrix between the
estimated value and observed records; IN is a unit matrix.
It should be noted that imputed values may not always
be reliable: 1) compared with actual records, imputed values
are relatively less reliable since they are inferred from actual
records; 2) different imputed data typically have different
degrees of reliability. When imputing the missing value at a
timestamp with intensive observations nearby, the estimated
value is reliable due to the increase of posterior knowledge.
Conversely, data inferred from sparse observations are less
reliable. GP naturally provides covariance functions to quan-
titatively describe unreliability of estimated data. Since ac-
tual records are relatively absolutely reliable compared with
imputed values, we set their unreliability scores to zero.
Thus, we get unreliability scores of different data:
u[x∗] =
0, for actually observed records
Con[x∗] > 0, for imputed values
(4)
For each sample, we represent its IMTS as three
data streams: an augmented input record, i.e., Xk
r
=
[xk
.1, · · · , xk
.n, · · · , xk
.N] = [xk
1, · · · , xk
t , · · · , xk
T k]T , which
is a unequally spaced MTS without missing values; an un-
reliability scores matrix Uk
r = [uk
.1, · · · , uk
.n, · · · , uk
.N] =
[uk
1, · · · , uk
t , · · · , uk
T k]T , which has the same shape with
Xk
r and quantitatively describes the unreliability degree of
each element in Xk
r; and a list of time intervals Δk
all =
[Δk
1, · · · , Δk
t , · · · , Δk
Tk], where Δt = tt −tt−1.
DATA-GRU takes input records xt, time intervals Δt and
unreliability scores ut as inputs, as shown in Fig.2. The time
intervals are directly incorporated into DATA-GRU to adjust
the hidden status in the previous memory cell. To ensure the
inﬂuence of previous status fades with the increase of the
time interval, we suggest to utilize a decay function to trans-
form it into weight. We tested several decay functions, e.g.,
wΔt=1/log(e+Δt), wΔt = e−Δt and wΔt=1/Δt, and found
that wΔt=1/log(e+Δt) is slightly better. Therefore, we use it
to transform time intervals into proper weights to adjust hid-
den state. The mathematical formulations for wΔt and hd
t−1
are as follows:
wΔt=1/log(e+Δt)
(5)
hd
t−1 = ht−1 ⊙wΔt
(6)
For convenience, we name the variant of GRU equipped
with the time-aware mechanism as T-GRU. Compared
with standard GRU, T-GRU can directly analyze unequally
spaced univariate or multivariate time series without the ne-
cessity of processing it into equally spaced data and thus can
preserve the informative varying intervals.
Dual-attention mechanism. Imputed records in aug-
mented data may not always reﬂect reality and the impu-
tation process could damage medical considerations behind
sampling characteristics of original EHR data, both affecting
risk prediction. To this end, a novel dual-attention structure
is further integrated into T-GRU to handle missing values by
jointly considering data quality and medical knowledge.
Unreliability-aware attention is proposed from the data-
quality view. Since the degrees of unreliability diverse
between actual records and imputed records, and also
vary among different imputed records, we propose an
unreliability-aware attention mechanism to adjust weights
assigned to different data to ensure high-quality data play
important roles to promote prediction performance while the
inﬂuence of low-quality data is limited. For convenience, un-
reliability score is converted into reliability score via ct =
1−ut. Since ct is only able to identify the quality of different
elements within each time series but is unable to identify im-
portant variables, we learn unreliability-aware weights from
ct using αu
t = sigmoid(W uct + bu) and utilize the learned
weights to adjust scores contributed by different elements in
time series of different variables. The expressions are given
below:
ct = 1 −ut
(7)
αu
t = sigmoid(W uct + bu)
(8)
xu
t = xt ⊙αu
t
(9)
The sampling characteristic of original EHR data pos-
sesses important medical considerations. We avoid dam-
aging informative varying intervals in IMTS by introduc-
ing the time-aware structure, which is a big step forward
compared with the typical methods of processing IMTS
into equally spaced. However, the imputation process may
still damage some medical information, i.e., missing val-
ues which are typically caused by changes in the symp-
toms of patients. To this end, from the medical-knowledge
view, we propose novel symptom-aware attention to fur-
ther supplement unreliability-aware attention. To exclude
the impact of imputed records, we ﬁlter out all the im-
puted values with an actual records pass ﬁlter (ARPF),
which only allows actually observed records to pass through,
namely cs
t 0/1 = FARP F (ct) = ⌊ct −0.5⌋for reliability
scores and xs
t 0/true = xt ⊙cs
t 0/1 for input records, such
that sampling characteristics of original EHR data are pre-
served. The ﬁltered data has severe irregularities and the
contained medical information is difﬁcult to extract by us-
ing standard machine learning methods, whose architec-
tures are designed for regular data. Therefore, we utilize the
aforementioned T-GRU to handle the time irregularity prob-
lem to extract deep symptom-aware input values xs
t deep =
TGRU(xs
t 0/true, wΔt) and deep symptom-aware attention
weights αs
t deep = TGRU(αs
t 0/1, wΔt). Then, αs
t deep are
933
