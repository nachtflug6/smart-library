version: "3.9"

services:
  grobid:
    image: grobid/grobid:0.8.2-full
    container_name: grobid
    ports:
      - "8070:8070"
      - "8071:8071"
    networks:
      - smartlib-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8070/api/isalive"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # Uncomment for GPU support (requires NVIDIA Container Toolkit installed)
    # runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - smartlib-net
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "
      ollama serve &
      sleep 4 &&
      ollama pull nomic-embed-text &&
      wait"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    # Uncomment for GPU support (requires NVIDIA Container Toolkit installed)
    # runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - capabilities: [gpu]

  devcontainer:
    build:
      context: .
      dockerfile: .devcontainer/Dockerfile
    container_name: smartlib_dev
    networks:
      - smartlib-net
    volumes:
      - ./:/workspace:cached
    depends_on:
      grobid:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8000:8000"  # API
      - "5173:5173"  # UI
    environment:
      - GROBID_HOST=grobid:8070
      - OLLAMA_HOST=ollama:11434
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

networks:
  smartlib-net:
    driver: bridge

volumes:
  ollama_data: